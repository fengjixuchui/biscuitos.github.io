---
layout: post
title:  "CACHE Mechanism[Updating]"
date:   2022-12-30 12:00:00 +0800
categories: [MMU]
excerpt: CACHE.
tags:
  - CACHE SYSTEM
  - Linux 6.0+
  - Hardware
  - BiscuitOS DOC 3.0+
---

![](/assets/PDB/BiscuitOS/kernel/IND00000L0.PNG)

![](/assets/PDB/RPI/RPI100100.png)

#### 目录

> - [CACHE 通用基础](#A)
>
>   - CACHE 硬件结构
>
>     - [CACHE 结构之直接映射](#A20)
>
>     - [CACHE 结构之全相联映射](#A21)
>
>     - [CACHE 结构之组相联映射](#A22)
>
>   - [CACHE Tag and Index](#A23)
>
>     - [CACHE 别名问题](#A232)
>
>     - [CACHE 歧义问题](#A231)
>
>     - [CACHE Index/Tag VIVT](#A233)
>
>     - [CACHE Index/Tag PIPT](#A234)
>
>     - [CACHE Index/Tag VIPT](#A235)
>
>     - [CACHE Index/Tag PIVT](#A236)
>
>   - [CACHE 读写策略: 回写/通写/写分配/写不分配](#A60)
>
>   - [CACHE 替换策略](#A70)
>
>   - [CACHE 一致性: MESI](#A80)
>
>   - [CACHE 多级缓存架构](#A90)
>
>     - [L1 CACHE](#A902)
>
>     - [L2 CACHE: MLC](#A903)
>
>     - [L3 CACHE: LLC](#A904)
>
> - Intel® X86 ™Processors 架构 CACHE 机制
>
>   - [Intel® Core-i7 an Xeon CACHE Architecture](#B)
>
>   - [Intel® Smart CACHE Technology](#B1)
>
>   - [Intel® Memory Type: UC/UC-/WC/WT/WB/WP](#B2)
>
>   - [Intel® CACHE Coherency Protocol: MESI](#B3)
>
>   - [Intel® Memory Type Range Registers(MTRRs) Technology](#B4)
>
>   - [Intel® Page Attribute Table(PAT) Technology](#B5)
>
>   - [Intel® CACHE Related Instruction](#B6)
>
>   - [Intel® Split_lock Check Technology]()
>
>   - [Intel® RDT Technology]()
>
>   - [Intel® DDIO Technology]()
>
>   - [Intel® CHA Architecture]()
>
>   - [Intel® CAT Technology]()
>
> - [ARM 架构 CACHE 机制(TODO)](#X)
>
> - Linux CACHE 机制
>
>   - [CACHE MODE on IOREMAP Mechanism](#C1)
>
>   - [CACHE MODE on Normal RAM](#C2)
>
>   - [CACHE MODE on MMIO](#C3)
>
>   - [Memory Type Coherent for MMIO/RAM](#C3)
>
>   - [Page CACHE CPA Mechanism]()
>
>   - [CPU CACHE-Info Mechanism]()
>
> - [CACHE 实践教程](#C)
>
> - [CACHE 使用教程](#D)
>
>   - [Uncache(UC) Memory on Kernel](#D1)
>
>   - [Uncache(UC) MMIO on Kernel](#D2)
>
>   - [WriteBack(WB) Memory on Kernel](#D3)
>
>   - [WriteBack(WB) MMIO on Kernel](#D4)
>
>   - [WriteThrough(WT) Memory on Kernel](#D5)
>
>   - [WriteThrough(WT) MMIO on Kernel](#D6)
>
>   - [UncacheD(UC-) Memory on Kernel](#D7)
>
>   - [UncacheD(UC-) MMIO on Kernel](#D8)
>
>   - [UncacheD(UC-) MMIO on Userspace](#D9)
>
>   - [WriteCombining(WC) Memory on Kernel](#DA)
>
>   - [WriteCombining(WC) MMIO on Kernel](#DB)
>
>   - [WC/WT/UC/UC-/WB Memory for Userspace Virtual Address](#DC)
>
>   - [More ...](#D)
>
> - [CACHE 应用场景](#E)
>
> - [CACHE 进阶研究](#F)
>
>   - [ATOMIC 机制研究]()
>
>   - [DMA 与 CACHE 一致性]()
>
>   - [伪共享研究]()
>
>   - [Spinlock 机制研究]()
>
>   - [CACHE 与 TLB 研究]()
>
>   - [CACHE 与 Combine Write Buffer]()
>
>   - [CACHE 与 Store Buffer]()
>
>   - [Intel RDT](#X)

######  🙂🙂🙂🙂🙂🙂 捐赠一下吧 🙂🙂🙂🙂🙂🙂

![BiscuitOS](/assets/PDB/BiscuitOS/kernel/HAB000036.jpg)

-------------------------------------------

<span id="A"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000Q.jpg)

#### CACHE 基础概念

![](/assets/PDB/HK/TH002421.png)

CACHE 是由一组称为**缓存行(CACHE Line)**固定大小的数据块组成，其长度称为 **CACHE Line Size**, 在有的架构中 CACHE Line Size 为 64 个字节。每个 CACHE Line 完全是在一个突发读操作周期中进行填充或更新, 即使处理器只访问存储器上的一个字节，CACHE 控制器也会将 CACHE Line Size 的数据块加载到 CACHE 里. 同理将内存中 CACHE Line Size 大小的数据块称为 **Data Block**.

![](/assets/PDB/HK/TH002422.png)

CACHE Line 是 CACHE 最基础的组成单位，将 N 个 CACHE Line 组成的集合称为 **CACHE Set**, 那么称 CACHE 是 N 路组相联(N-Way Set-associative Cache), 所以从硬件结构来看 CACHE 被划分成 N 个垂直面，每个垂直面上有多个 CACHE Line，对所有的 CACHE 垂直面进行一次横切的 CACHE Line 合集就是 CACHE Set. CACHE Line 由两部分组成: Tag 和 Offset 部分，其中 Tag 字段由于匹配具体的 Data Block，Tag 字段中还包含了 valid 标志位，该位置位说明 CACHE Line 的内容有效，反之 CACHE Line 的内容无效。CACHE Line 的 Offset 部分存储 Data Block 的内容.

* **CACHE Hit(命中)**: 指的是 CPU 要访问的地址正好缓存在 CACHE Line 中
* **CACHE Miss(缺失)**: 指 CPU 要访问的地址没有缓存在 CACHE Line 中.
* **CACHE 颠簸**: 指的是 CPU 访问多个地址导致某个 CACHE Line 来回更新和填充.

---------------------------------------

##### <span id="A20">直接映射</span>

![](/assets/PDB/HK/TH002418.png)

**直接映射(Direct-mapped)** 指的是内存中的 data block 按顺序映射到指定的 CACHE Line, 且每个 data block 只能映射到一个 CACHE Line 里，那么内存 data block 与 CACHE Line 一一对应，那么可以使用一个线性公式表示两者之间的关系，这个线性公式是求模公式，模数即为 CACHE Line 的数量，内存地址求模之后就可以知道其映射的 CACHE Line，那么模相同的内存 data block 会映射到同一个 CACHE Line 里. 例如上图的案例中，内存 data block 的长度和 CACHE LINE 的长度都是 64Bytes，内存 data block 0x0000000 只能被加载到 CACHE Line0, 0x00000008 只能加载到 CACHE Line1，以此类推，由于 0x00000030 的模数与 0x00000000 相同，那么其也映射到 CACHE Line0 上。相比其他的 CACHE 设计方案，直接映射有自己的简单的优点，但缺点也很明显:

* **优点**: 硬件设计简单、成本低
* **缺点**: 灵活性差，内存 data block 只能映射到固定的 CACHE Line 上，很容易与同一 CACHE Line 的 data block 冲突，从而引起**Cache 颠簸**. CACHE 的容量比较大才能显示直接映射的优势.

![](/assets/PDB/HK/TH002415.png)

直接映射硬件设计简单, CACHE 内部在一个平面内将所有的 CACHE LINE 按顺序依次排列组成，每一行就是一个 CACHE Line，每个 CACHE Line 包含了两部分，TAG 和 OFFSET，其中 TAG 部分还包含了一个 valid 位，Valid 位置位时表示 CACHE Line 维护的数据有效, 反之该位清零时表示 CACHE Line 维护的数据无效. 硬件处理过程如上图:

* **A**: CPU 生成需要**访问地址**，地址被划分成 3 个部分: Tag、Index 和 Offset
* **B**: CACHE 从**访问地址**的 Index 部分在 CACHE 中找到对应的 CACHE Line
* **C**: 将 CACHE Line 的 Tag 部分与**访问地址**的 Tag 部分进行比较，当匹配上且 CACHE Line Tag 的 valid 位置位，那么**命中(Cache hit)**; 反之如果 valid 位清零或者 Tag 字段不匹配，那么**缺失(Cache miss)**.
* **D**: 当命中之后，CACHE 根据**访问地址**的 Offset 字段从 CACHE Line 中获得最终的值.

----------------------------------------

##### <span id="A21">全相联映射</span>

![](/assets/PDB/HK/TH002419.png)

**全相联映射(Full-associative)** 指的是内存中的 data block 可以映射到任意 CACHE Line. 例如上图的案例中内存 0x00000000 可以映射到任一 CACHE Line 上，0x00000008 也可以映射到任一个 CACHE Line. 由于内存 data block 可以映射到任一 CACHE Line，因此 CACHE 需要花费更多的时间或者更多的资源去查找对应的 CACHE Line 中是否包含所需的数据.

* **优点**: 灵活性好，CACHE 只要要空闲的 CACHE Line 就可以加载内存 data block.
* **缺点**: 利用率不高，因为存在一个 m 位的标记，使 CACHE Line 中包含一些对存储无用的信息. 速度慢、硬件成本高，每次访问 CACHE 需要依次遍历，直到命中才能直到 data block 是否在 CACHE 中.

![](/assets/PDB/HK/TH002416.png)

全相联映射的硬件相对复杂，CACHE 内部在一个平面内将所有的 CACHE LINE 按顺序依次排列，每个 CACHE Line 都存在一个比较模块，用于比较 CACHE Line 的 Tag 部分，其中 TAG 部分还包含了一个 valid 位，Valid 位置位时表示 CACHE Line 维护的数据有效, 反之该位清零时表示 CACHE Line 维护的数据无效. 全>相联结果的 CACHE 有着最大的灵活性，因此缺失(Cache miss)率是最低的，但从硬件结构来看，由于有着大量的内容需要进行比较，它的延迟也是最大的，因此一般这种结构的 CACHE 都不会有很大的容量. 硬件处理过程如上图:

* **A**: CPU 生成需要**访问地址**，地址被划分成 2 个部分: Tag 和 Offset
* **B**: CACHE 从**访问地址**的 Tag 部分依次与 CACHE Line 的 Tag 部分进行比较
* **C**: 当匹配上且 CACHE Line Tag 的 valid 位置位，那么**命中(Cache hit)**; 反之如果 valid 位清零或者 Tag 字段不匹配，那么**缺失(Cache miss)**.
* **D**: 当命中之后，CACHE 根据**访问地址**的 Offset 字段从 CACHE Line 中获得最终的值.

-----------------------------------------

##### <span id="A22">组相联映射</span>

![](/assets/PDB/HK/TH002420.png)

**组相联映射(Set-associative)** 是直接映射和全映射的折中方案，将多个 CACHE Line 组成一个组称为 **CACHE Set**，同样将多个 Data block 组成一个组称为 **Data Set**，每个 Data Set 按直接映射的方式映射到指定的 CACHE Set. Data Set 内的任意 Data block 可以映射到 CACHE Set 中任意 CACHE Line. 对于组相联映射的 CACHE 来说，一个 Data block 可以被加载到 CACHE Set 的 N 个 CACHE Line，那么称这个 CACHE 是 N 路组相联的 CACHE(n-way set-associative Cache). 相比其他两种映射方式，组相联映射组内具有一定的灵活性，而且组内行数较少，比较的硬件电路比全相联方式简单，并且空间利用率比直接映射方式高.

![](/assets/PDB/HK/TH002422.png)

组相联映射的硬件是一个立体的结构，每个 CACHE Way 就是一个垂直平面，多个 CACHE Way 组成了一个立体的存储结构，那么一个 CACHE Set 就是在多个 CACHE Way 组成的立体结构上进行一次横切，那么同一个 Data Set 里面的 Data block 可以存储在横切之后 CACHE Set 里任意一个 CACHE Line 里. 硬件处理过程如上图:

* **A**: CPU 生成需要**访问地址**，地址被划分成 3 个部分: Tag、Index 和 Offset
* **B**: CACHE 从**访问地址**的 index 部分在 CACHE 中找到对应的 CACHE Set.
* **C**: 依次将 CACHE Set 里的 CACHE Line 的 Tag 部分与**访问地址**的 Tag 部分进行比较，如果匹配上且 Tag 的 valid 位置位，那么**命中(Cache hit)**; 反之如果 valid 位清零或者 Tag 字段没有匹配，那么**缺失(Cache miss)**.
* **D**: 当命中之后，CACHE 根据**访问地址**的 Offset 字段从 CACHE Line 中获得最终的值.

-----------------------------------

##### <span id="A23">CACHE Tag and Index</span>

![](/assets/PDB/HK/TH002432.png)

对于一段索引 CACHE 的内存地址(可以是虚拟机地址或者物理地址)，其分为以上三部分: Tag、Index 和 Offset. 三者组合可以在 CACHE 中定位唯一的 CACHE Line，其中 Index 字段用于定位 CACHE Set，Tag 用于在 CACHE Set 中定位到指定的 CACHE Line，Offset 用于在 CACHE Line 的数据域定位指定的数据.

![](/assets/PDB/HK/TH002433.png)

CACHE Line 内部结构如上图，其由 Tag、Flags 和数据域组成。Tag 部分用于在内存地址 Index 找到 CACHE Set 的情况下，与内存地址 Tag 相比较以此找到指定的 CACHE Line; Flags 字段包括 Valid 位，在数据 CACHE 中还包括 Dirty 位，Valid 位用于指明 CACHE Line 是否有效，Dirty 位则表明 CACHE Line 是否包含脏数据; Data 数据域包含了从 Data Block 取来的 CACHE Line Size 字节的数据.

![](/assets/PDB/HK/TH002434.png)

内存 Data Block 首次加载到 CACHE 时，先根据内存地址的 Index 字段在 CACHE 中找到对应的 CACHE Set，然后将 CACHE Set 中找到一个合适的 CACHE Line 将内存地址 Tag 字段存入 CACHE Line 的 Tag 字段，然后将内存 Data Block 的数据存储到 CACHE Line 的 Data 区域. 例如 Data Block 的地址为 0x00061000, 其 Index 为 1， 那么其被加载到 CACHE 时其会选择 CACHE Set 为 1，由于案例中 CACHE Set 只包含一个 CACHE Line，那么 Data Block 加载到 CACHE Line1 里，此时 CACHE 将 Data Block 的 Tag 字段 0x0006 存储到 CACHE Line1 的 Tag 字段，并将 Data Block 的数据加载到 CACHE Line1 的 Data 域. 以上便是一个最简单的 CACHE Load 内存的过程.

![](/assets/PDB/HK/TH002435.png)

在开启分页之后，CPU 直接使用的是虚拟地址，但索引 CACHE 的内存地址可以是物理地址，也可以是虚拟地址，存在这样的逻辑是因为 CACHE 的位置决定的。当 CACHE 位于 CPU 和 MMU 之间，那么称为**逻辑 CACHE**，可以使用虚拟地址的 Index 索引 CACHE Set，并称虚拟地址的 Index 为 **VI(Virtual Index)**; 当 CACHE 位于 MMU 和主存之间，那么称为 **物理 CACHE**, 可以使用物理地址的 Index 索引 CACHE Set，并称物理地址的 Index 为 **PI(Physical Index)**. 另外需要使用物理地址 Tag 确认 CACHE Line 的 Tag，那么物理地址 Tag 为 **PT(Physical Tag)**, 同理需要使用虚拟地址 Tag 确认 CACHE Line 的 Tag，那么虚拟地址 Tag 称为 **VT(Virtual Tag)**. 

------------------------------------

##### <span id="A231">CACHE 歧义(Ambiguity)</span>

![](/assets/PDB/HK/TH002436.png)

当 CACHE 控制器使用虚拟地址索引 CACHE Line，即使用 VIVP(Virtual Index Virtual Tag)，当两个进程相同的虚拟地址映射到不同的物理地址，例如上图中进程 0 的虚拟地址 0x1000000108 映射了物理地址 0x200020, 而进程 1 同样的虚拟地址 0x1000000108 映射了物理地址 0x300020. 由于 CACHE 控制器使用了虚拟地址 Index 所有 CACHE Set，那么两个虚拟地址的 Index 都是 0x01, 并且使用虚拟地址的 Tag 确认 CACHE Line 的 Tag，此时两个虚拟地址都有相同的 Tag，因此两个进程的虚拟地址在 CACHE 中找到了同一个 CACHE Line，换个角度就是**同一个 CACHE Line 映射不同的物理地址**, 称这种现象为 **CACHE 歧义(Ambiguity)**. 内核通过如下方法避免歧义:

* 进程切换时 flush cache，并使主存 Data Block 有效。针对 WriteBack 高速缓存，首先应使主存数据有效，保证已经修改数据 CACHE Line 已经写会主存，避免修改的数据丢失.
* 进程切换时 flush cache，并使 CACHE Line 无效，保证切换之后的进程不会错误命中切换之前进程的 CACHE Line.

##### <span id="A232">CACHE 别名(Alias)</span>

![](/assets/PDB/HK/TH002437.png)

当 CACHE 控制器使用虚拟地址索引 CACHE Line，即使用 VIVP(Virtual Index Virtual Tag). 当两个进程不同的虚拟地址映射到同一个物理地址上，例如上图进程 0 的虚拟地址 0x1000000108 和进程 1 的虚拟地址 0x8000000206 都映射到了物理地址 0x3000200，由于 CACHE 控制器使用 VIVP，那么此时就会出现两个虚拟地址会索引到不同 CACHE Set 的 CACHE Line 上，换个角度就是**一个物理地址被缓存到不同的 CACHE Line 里**，称这种现象为 **CACHE 别名(Alias)**. 别名问题会导致 CACHE 一致性问题(后面会讨论).

------------------------------------------

##### <span id="A233">VIVT</span>

![](/assets/PDB/HK/TH002438.png)

**VIVT(Virtual Index Virtual Tag)** 指的是**虚拟 CACHE 控制器**通过虚拟地址提供的 Tag 和 Index 索引 CACHE Line. VIVT 不需要经过 MMU 翻译，具有很小的延时，但会引起**别名**和**歧义**问题.

![](/assets/PDB/HK/TH002439.png)

**VIVT CACHE Hit**: 当 CACHE 控制器使用 VIVT 索引 CACHE Line:

* **(A)** CPU 产生的虚拟地址可以直接传递给 CACHE 控制器进行索引
* **(B)** CACHE 控制器从虚拟地址中提取 Index 字段，Index 可以选择指定的 CACHE Set，此时选中 CACHE Set1，其组内包含了两个 CACHE Line
* **(C)** CACHE 控制器从虚拟地址中提取 Tag 字段，与 CACHE Set1 的所有 CACHE Line 的 Tag 字段进行比较，一旦 Tag 匹配，并且 Valid 字段有效，那么找到指定的 CACHE Line.
* **(D)** CACHE 控制器从 CACHE Line 中区域 Data 区域，并从虚拟地址提取 Offset 字段，最终在 CACHE Line 的 Data 区域中读取指定的数据.

![](/assets/PDB/HK/TH002440.png)

**VIVT CACHE Miss**: 当 CACHE 控制器使用 VIVT 索引 CACHE Line:

* **(A)** CPU 产生的虚拟地址可以直接传递给 CACHE 控制器进行索引
* **(B)** CACHE 控制器从虚拟地址中提取 Index 字段，Index 可以选择指定的 CACHE Set，此时选中的 CACHE Set1，其组内包含了两个 CACHE Line.
* **(C1)** CACHE 控制器从虚拟地址中提取 Tag 字段，与 CACHE Set1 的第一个 CACHE Line 的 Tag 字段进行比较，发现不匹配
* **(C2)** CACHE 控制器继续与 CACHE Set1 剩下的 CACHE Line Tag 字段进行比较，发现还是不匹配，那么 CACHE 控制器认为发送 CACHE Miss
* **(C3)** CACHE Miss 之后 CACHE 控制器将虚拟地址传递给 TLB 或者 MMU 查询虚拟地址对应的物理内存
* **(C4)** 系统从 TLB 或者 MMU 中找到了虚拟地址对应的物理内存(可能也找不到)
* **(D)** CACHE 控制器根据替换算法将 CACHE Set1 中指定的 CACHE Line 刷出，然后将找到的物理内存 Data Block 加载到该 CACHE Line，并更新 Tag 和 Valid 字段
* **(E)** CACHE 控制器继续将 CACHE Line 的 Data 域取出，然后配合虚拟地址的 Offset 域找到最终的数据.

**VIVT 优缺点**: 通过上面对 VIVT Hit 和 Miss 场景的分析，VIVI 模式下 CPU 不需要将虚拟地址转换成物理地址就可以在 CACHE 中找到 CACHE Line，在一定程度上提升了 CACHE 的速度，因为访问虚拟地址转换成物理地址可能需要几十到几百个周期，只有在 CACHE Miss 的时候才会发送虚拟地址到物理地址的转换，硬件设计上更加简单. 但 VIVT 模式容易引入歧义和别名问题, 具体如下:

![](/assets/PDB/HK/TH002441.png)

在不同的进程地址空间里，虚拟地址虽然相同，但映射到不同的物理内存上，如果 CACHE 控制器使用虚拟地址的 Index 和 Tag 查找 CACHE Line，那么会出现**同一个 CACHE Line 会映射不同的物理地址**，这就是 **VIVT 的歧义问题**. 为了保证系统的正确工作，操作系统需要负责避免歧义出现，可以通过按需清除 CACHE Line，或者为每一个进程的地址空间添加标记(PCID/ASID)，当进程切换时刷掉指定的 CACHE Line, 不过这样做会导致进程调度回来时出现大量的 CACHE Miss，影响程序的性能.

![](/assets/PDB/HK/TH002442.png)

同样是不同进程不同的虚拟地址同时映射到同一个物理地址上，如果 CACHE 控制器使用虚拟地址的 Index 和 Tag 查找 CACHE Line，那么不同的虚拟地址可以找到不同的 CACHE Line，但找到的 CACHE Line 却缓存了同一个物理地址的 Data Block，也就**同一个物理地址会被映射到不同的 CACHE Line**，这就是 **VIVT 的别名问题**. 该问题会引起 CACHE 的一致性问题，例如更新了存在别名 CACHE Line 的数据，那么同别名的 CACHE Line 的数据没有被更新，导致 CACHE 数据一致性问题. 同歧义问题一样，可以通过 Flush CACHE 的方式避免别名问题，也可以针对共享的数据映射相同物理地址时采用 NOCACHE 的方式，但这样会损失 CACHE 带来的性能提升. 

-----------------------------------

##### <span id="A234">PIPT</span>

![](/assets/PDB/HK/TH002443.png)

**PIPT(Physical Index Physical Tag)** 指的是**物理 CACHE 控制器**通过物理地址提供的 Tag 和 Index 索引 CACHE Line. PIPT 需要将 CPU 产生的虚拟地址经过 TLB/MMU 翻译，获得物理地址之后才能在 CACHE 中查找 CACHE Line，因此 CACHE 的速度受限于 TLB/MMU 转换的效率. 由于物理地址的唯一性，那么 Physical Index 和 Physical Tag 也具有唯一性，那么不会引入 **CACHE 别名问题**和 **CACHE 歧义问题**.

![](/assets/PDB/HK/TH002444.png)

**PIPT CACHE Hit**: 当 CACHE 控制器使用 PIPT 索引 CACHE Line 发生 CACHE Hit:

* **(A)** CPU 产生的虚拟地址首先传递给 TLB/MMU 进行地址转换
* **(B)** TLB/MMU 将虚拟地址转换成物理地址, 并传递给 CACHE 控制器
* **(C)** CACHE 控制器从物理地址中提取 Index 字段，然后找到对应的 CACHE Set1，组内包含了两个 CACHE Line
* **(D1)** CACHE 控制器从物理地址中提取 Tag 字段，然后与 CACHE Set1 组内第一个 CACHE Line 的 Tag 字段进行比较，此时两个 Tag 字段并不匹配.
* **(D2)** CACHE 控制器继续与 CACHE Set1 组内最后一个 CACHE Line 的 Tag 字段进行比较，此时两个 Tag 匹配，并检查到 Valid 字段有效，那么 CACHE Hit
* **(E)** CACHE 控制器从匹配 CACHE Line 中提取其 Data 域，并从物理地址中提取 Offset 字段，最后从 Data 域中获得所需的数据.

![](/assets/PDB/HK/TH002445.png)

**PIPT CACHE Miss**: 当 CACHE 控制器使用 PIPT 索引 CACHE Line 发生 CACHE Miss:

* **(A)** CPU 产生的虚拟地址首先传递给 TLB/MMU 进行地址转换
* **(B)** TLB/MMU 将虚拟地址转换成物理地址，并传递给 CACHE 控制器
* **(C)** CACHE 控制器从物理地址中提取 Index 字段，然后找到对应的 CACHE Set1，组内包含了两个 CACHE Line
* **(D1)** CACHE 控制器从物理地址中提取 Tag 字段，然后与 CACHE Set1 组内第一个 CACHE Line 的 Tag 字段进行比较，此时两个 Tag 字段并不匹配.
* **(D2)** CACHE 控制器继续与 CACHE Set1 组内最后一个 CACHE Line 的 Tag 字段进行比较，此时两个 Tag 还是不匹配(或者就算 Tag 匹配但 Valid 域无效)，那么 CACHE Miss
* **(E)** CACHE 根据物理地址在主内存中找打对应的 Data Block
* **(F)** CACHE 根据一定的替换算法在 CACHE Set1 中指定的 CACHE Line 刷出，然后将主内存 Data Block 加载到 CACHE Line，并更新 Tag 和 Valid 字段
* **(G)** CACHE 控制器从新更新的 CACHE Line 中提取其 Data 域，并从物理地址中提取 Offset 字段，最后从 Data 域中获得所需的数据.

![](/assets/PDB/HK/TH002446.png)

在不同的进程地址空间里，虚拟地址相同，但映射到不同的物理内存上，如果 CACHE 控制器使用物理地址的 Index 和 Tag 查找 CACHE Line，那么不会出现**歧义问题**. 由于物理地址的唯一性，就算虚拟地址相同，但物理地址是不同的，因此可以**保证 Index 和 Tag 索引到的 CACHE Line 只映射一个的物理地址**, 从而避免了 CACHE 歧义问题.

![](/assets/PDB/HK/TH002447.png)

同样是不同进程不同的进程的空间虚拟地址映射到同一个物理地址，如果 CACHE 控制器使用物理地址的 Index 和 Tag 查找 CACHE Line，那么不会出现**别名问题**. 由于物理地址的唯一性，就算不同的虚拟地址映射到同一个物理地址，因此可以**保证物理地址只被加载到一个 CACHE Line 中**，从而避免了 CACHE 别名问题.

**PIPT 优缺点**: PIPT 带来的好处是很明显的，软件层面基本不需要任何维护就可以避免歧义和别名问题，但硬件设计上比 VIVT 复杂很多，因此硬件成本更高。最后对一个架构来说，CPU、TLB/MMU 和 CACHE 是三个不同的硬件模块，如果采用 PIPT 的话，CPU 发出虚拟地址经过 TLB/MMU 地址转译获得物理地址之后，CACHE 才能进行查询，这个串行操作直接损害性能, 因此没有 VIVT 高效.

-----------------------------------

##### <span id="A235">VIPT</span>

![](/assets/PDB/HK/TH002448.png)

**VIPT(Virtual Index Physical Tag)** 指的是**物理 CACHE 控制器**通过虚拟地址提供的 Index 和物理地址提供的 Tag 索引 CACHE Line. CPU 产生虚拟地址之后，可以同时将虚拟地址传递给 CACHE 控制器和 TLB/MMU 并发处理，这样索引 CACHE Set 和地址转译在时间上并发。歧义问题和别名问题在 VIPT 中是不存在的。

![](/assets/PDB/HK/TH002449.png)

**VIPT CACHE Hit**: 当 CACHE 控制器使用 VIPT 索引 CACHE Line 发生 CACHE Hit:

* **(A)** CPU 产生的虚拟地址同时传递给 CACHE 控制器和 TLB/MMU 组件
* **(B1)** 虚拟地址传送给 TLB/MMU 进行地址转译，以此获得物理地址
* **(B2)** 虚拟地址传送给 CACHE 控制器之后，提取虚拟地址的 Index 字段在 CACHE 中找到 CACHE Set1，组内包含两个 CACHE Line
* **(C)** TLB/MMU 地址转译完毕获得物理地址
* **(D1)** CACHE 控制器从物理地址中提取 Tag 字段与 CACHE Set1 第一个 CACHE Line 的 Tag 字段进行比对，结果不匹配
* **(D2)** CACHE 继续将物理地址提取的 Tag 字段与 CACHE Set1 的最后一个 CACHE Line 的 Tag 字段进行比对，匹配成功，并且此时 CACHE Line 的 Valid 字段有效，CACHE Hit.
* **(E)** CACHE 将匹配到的 CACHE Line Data 域取出，然后从物理地址或虚拟地址中获得 Offset 字段，最终从 Data 域中获得所需的数据.

![](/assets/PDB/HK/TH002450.png)

**VIPT CACHE Miss**: 当 CACHE 控制器使用 VIPT 索引 CACHE Line 发生 CACHE Miss:

* **(A)** CPU 产生的虚拟地址同时传递给 CACHE 控制器和 TLB/MMU 组件
* **(B1)** 虚拟地址传送给 TLB/MMU 进行地址转译，以此获得物理地址
* **(B2)** 虚拟地址传送给 CACHE 控制器之后，提取虚拟地址的 Index 字段在 CACHE 中找到 CACHE Set1，组内包含两个 CACHE Line
* **(C)** TLB/MMU 地址转译完毕获得物理地址
* **(D1)** CACHE 控制器从物理地址中提取 Tag 字段与 CACHE Set1 第一个 CACHE Line 的 Tag 字段进行比对，结果不匹配
* **(D2)** CACHE 继续将物理地址提取的 Tag 字段与 CACHE Set1 的最后一个 CACHE Line 的 Tag 字段进行比对，结果不匹配(或者匹配当 Valid 无效)，那么 CACHE Miss.
* **(E)** CACHE 控制器根据物理地址在主存中找到对应的 Data Block
* **(F)** CACHE 控制器在 CACHE Set1 根据替换算法将指定的 CACHE Line 刷出，然后将 Data Block 加载到 CACHE Line，并更新 Tag 和 valid 字段
* **(G)** CACHE 将新更新的 CACHE Line Data 域取出，然后从物理地址或虚拟地址中获得 Offset 字段，最终从 Data 域中获得所需的数据.

![](/assets/PDB/HK/TH002451.png)

在 VIPT 场景下，当两个进程的地址空间相同的虚拟地址映射到不同的物理地址，那么两个虚拟地址的 Index 是相同的，因此 CACHE 会定位到相同的 CACHE Set，但由于 CACHE Set 内包含多个 CACHE Line，由于物理地址的唯一性，不同的物理地址的 Tag 是不相同的，因此该场景下会对应两个不同的 CACHE Line，因此 VIPT 不存在**歧义问题**.

![](/assets/PDB/HK/TH002453.png)

在 VIPT 场景下，当不同进程的地址空间不相同的虚拟地址映射到同一个物理地址，那么两个虚拟地址的 Index 不相同，因此物理内存 Data block 可以加载到任意多个 CACHE Set。该场景下假设 CACHE 控制器通过虚拟地址 Index 找到 CACHE Set 之后，根据物理 Tag 发现该 CACHE Set 的所有 CACHE Line Tag 都不匹配，但物理内存的 Data block 已经加载到另外的 CACHE Set，CACHE 控制器现在无法获得其他 CACHE Set 的信息，因此引起了**别名问题**.

![](/assets/PDB/HK/TH002454.png)

**4KiB 别名问题**: 对于 VIPT 的别名问题，在 Linux 上是有解法的，例如在 8-Way 32KiB 组相联映射 CACHE 中 CACHE Line Size 为 64Bytes，一共包含 64 个 CACHE Set，因此需要虚拟地址提供 6bits 作为 Index。当映射 4KiB 物理页时，其包含 64 个 Data Block，将 8 个 Data Block 作为一组，其中物理地址 \[0:5\] 作为 Offset 字段，物理地址 \[7:11\] 寻址所有的 CACHE Set，实际只需要 3bit 就可以寻址所有 Data Block 组。由于映射 4KiB 页的低 12 位虚拟地址的内容和物理地址一致，那么虚拟地址 Index 6bit 与物理地址 \[7:11\] 字段内容是一致的，此时虚拟地址 Index 等效于物理地址 Index, 因此可以解决 VIPT 别名问题.

![](/assets/PDB/HK/TH002455.png)

**2MiB 别名问题**: 在 8-Way 32KiB 组相联映射 CACHE 中 CACHE Line Size 为 64Bytes，一共包含 64 个 CACHE Set，因此需要虚拟地址提供 6bits 作为 Index。当映射 2MiB 物理页时，其包含 32768 个 Data Block，将 8 个 Data Block 作为一组，一共 4096 Data block 组。其中物理地址 \[0:5\] 作为 Offset 字段，物理地址 \[7: 11\] 可以寻址所有的 CACHE Set, 实际需要 12bit 才能寻址所有的 Data Block 组，由于映射 2MiB 页的低 20 位虚拟地址的内容和物理地址一致，因此只要虚拟地址 6Bit 位于低 20 bit，那么虚拟地址 Index 就和物理 Index 一致，可以做到 2MiB 页内的物理地址只会加载到唯一的 CACHE Set 中，也可以解决 VIPT 的别名问题.

![](/assets/PDB/HK/TH002456.png)

**直接映射别名问题**: 在 8KiB 采用直接映射的 CACHE 中，CACHE Line Size 为 256 字节，其一共包含 32 个 CACHE Line，因此虚拟地址需要提供 5bit 寻址 CACHE Line，9 bit 用于 Offset 字段。在映射 4KiB 物理页的场景中，由于虚拟地址 \[0:8\] 区域用于 Offset 字段，虚拟地址 \[9:14\] 区域用于 Index 字段，由于 4KiB 映射物理页只有低 12 位虚拟地址和物理地址内容相同，因此此时虚拟地址 Index 不等效于物理地址 Index，因此会出现**别名问题**; 但同样的环境映射 2MiB 的物理页，由于低 20 位虚拟地址和物理地址内容一致，因此此时虚拟地址 Index 等效物理地址 Index, 此时不存在别名问题. 

**VIPT 别名问题总结**: 通过上面的案例分析，VIPT 要避免别名问题与 CACHE 的映射方式有关: 当使用组相联映射时，CACHE Set 的数量会影响别名问题，具体来说 CACHE Set 越大越容易引发别名问题; 当使用直接映射时，CACHE Line Size 会影响别名问题，当 CACHE Line Size 越大，Index 字段就会超过 4KiB/2MiB 低一致位，那么更容易引发一致性问题. 目前主流解决 VIPT 问题就是是 CACHE 采用组相联映射方式，CACHE 规模为 32KiB 8-Way(CACHE Line Size: 64B). 对于目前处理器 L1 CACHE 都是 VIPT，可以和 TLB/MMU 并发 CACHE 查询过程，但 VIPT 不是所有级 CACHE 的最佳选择，CACHE 越大需要的 Index 字段越大，那么越容易引发别名问题，因此 L2/L3 采用物理地址 Index 是最佳选择.

--------------------------------------------

##### <span id="A236">PIVT</span>

![](/assets/PDB/HK/TH002457.png)

**PIVT(Physical Index Virtual Tag)** 指的是**物理 CACHE 控制器**通过物理地址提供的 Index 和虚拟地址提供的 Tag 索引 CACHE Line. CPU 产生虚拟地址之后，PIVT 并不能像 VIPT 那样并发的在 CACHE 和 TLB/MMU 中工作，因为 CACHE 在查找 CACHE Line 时需要先定位 CACHE Set，再结合 Tag 字段才能定位 CACHE Line，但 PIVT 首先需要将虚拟地址传递给 TLB/MMU 进行地址转译，获得物理地址 Index 字段之后传递给 CACHE 才能定位 CACHE Set，最后根据虚拟地址的 Tag 定位 CACHE Line. PIVT 既包含了别名和歧义问题，而且性能也比 VIVT 或者 VIPT 方式慢很多.

![](/assets/PDB/HK/TH002458.png)

**PIVT CACHE Hit**: 当 CACHE 控制器使用 PIVT 索引 CACHE Line 发生 CACHE Hit:

* **(A)** CPU 产生虚拟地址
* **(B)** CPU 将虚拟地址传递给 TLB/MMU 进行地址转译
* **(C)** TLB/MMU 地址转译完毕，获得物理地址
* **(D)** 将物理地址的 Index 传递给 CACHE 控制器，匹配到 CACHE Set1，组内包含了两个 CACHE Line
* **(E1)** CACHE 控制器从虚拟地址提取 Tag 字段与 CACHE Set1 第一个 CACHE Line 的 Tag 字段进行比对，结果发现不匹配
* **(E2)** CACHE 控制器继续与 CACHE Set1 的最后一个 CACHE Line 的 Tag 字段进行比对，发现匹配且 CACHE Line 的 Valid 位有效，那么 CACHE Hit.
* **(F)** CACHE 将匹配到的 CACHE Line Data 域取出，然后从物理地址或虚拟地址中获得 Offset 字段，最终从 Data 域中获得所需的数
据.

![](/assets/PDB/HK/TH002459.png)

**PIVT CACHE Miss**: 当 CACHE 控制器使用 PIVT 索引 CACHE Line 发生 CACHE Miss:

* **(A)** CPU 产生虚拟地址
* **(B)** CPU 将虚拟地址传递给 TLB/MMU 进行地址转译
* **(C)** TLB/MMU 地址转译完毕，获得物理地址
* **(D)** 将物理地址的 Index 传递给 CACHE 控制器，匹配到 CACHE Set1，组内包含了两个 CACHE Line
* **(E1)** CACHE 控制器从虚拟地址提取 Tag 字段与 CACHE Set1 第一个 CACHE Line 的 Tag 字段进行比对，结果发现不匹配
* **(E2)** CACHE 控制器继续与 CACHE Set1 的最后一个 CACHE Line 的 Tag 字段进行比对，结果发现不匹配或者就算匹配，当 CACHE Line 的 Valid 标志位无效，因此 CACHE Miss.
* **(F)** CACHE 控制器利用物理地址在主存中找到对应的 Data Block
* **(G)** CACHE 控制器根据一定的替换算法将 CACHE Set1 中的某个 CACHE Line 刷出去，然后将找到的 Data Block 加载到该 CACHE Line 里
* **(H)** CACHE 将匹配到的 CACHE Line Data 域取出，然后从物理地址或虚拟地址中获得 Offset 字段，最终从 Data 域中获得所需的数
据.

![](/assets/PDB/HK/TH002460.png)

在不同的进程地址空间，虚拟地址虽然相同，但映射到不同的物理内存，如果 CACHE 控制器使用物理地址 Index 和虚拟地址 Tag 查找 CACHE Line，那么会出现**同一个 CACHE Line 映射不同的物理地址**，这就是 PIVT 的歧义问题. 为了保证系统的正确工作，操作系统需要负责避免歧义出现，可以通过按需清除 CACHE Line，或者为每一个进程的地址空间添加标记(PCID/ASID)，当进程切换时刷掉指定的 CACHE Line, 不过这样做会导致进程调度回来时出现大量的 CACHE Miss，影响程序的性能.

![](/assets/PDB/HK/TH002461.png)

同样是不同进程的地址空间不同的虚拟地址映射到同一个物理地址上，如果 CACHE 控制器使用物理地址的 Index 和虚拟地址 Tag 查找 CACHE Line，那么由于同一个物理地址，因此会定位到同一个 CACHE Set，但由于 Tag 不同，那么不同的虚拟地址 Tag 会定位到不同的 CACHE Line，此时**出现了同一个物理地址被映射到不同 CACHE Line**, 这就是 **PIVT 的别名问题**. 该问题会引起 CACHE 的一致性问题，例如更新了存在别名 CACHE Line 的数据，那么同别名的 CACHE Line 的数据没有被更新，导致 CACHE 数据一致性问题. 同歧义问题一样，可以通过 Flush CACHE 的方式避免别名问题，也可以针对共享的数据映射相同物理地址时采用 NOCACHE 的方式，但这样会损失 CACHE 带来的性能提升.

**PIVT 优缺点**: 通过上面对 PIVT 进行分析，发现 PIVT 具有 VIVT 的别名和歧义缺点，另外还没有 VIPT 并行特点，因此 PIVT 并没有任何优势可言.

###### VIVT/VIPT/PIPT/PIVT 总结

CACHE 对 CPU 的性能至关重要，目前主流处理器 L1 CACHE 采用 VIPT，CACHE 控制器通过虚拟地址 Index 取出一组 CACHE Line，同时并行进行 TLB/MMU 地址转译获得物理地址 Tag，最后从 CACHE Line 组中找到目标 CACHE Line，另外 Index 字段所需的位数在页表页偏移之内，那么虚拟地址 Index 是等效于物理地址 Index，那么这个时候不存在歧义和别名问题。但 VIPT 并不是所有级 CACHE 的首选，随着 CACHE 体积变大，Index 字段的长度会不断变大，当超过虚拟内存映射物理内存页内偏移区域之后，虚拟地址 Index 不再等效于物理地址 Index，便会增加歧义和别名问题的概率，因此 L2/L3 CACHE 选择 PIPT 的索引方式，这样可以避免歧义和别名问题. VIVT 软件维护成本太高，并会引入歧义和别名问题，需要在进程切换时 Flush 指定的 CACHE Line，软件管理难度大。现在主流使用 VIPT 和 PIPT。在多路组相联的 CACHE 中，CACHE Way 的大小等于 4KiB，一般硬件采用 VIPT 方式可以有效避免歧义和别名问题，等效于 PIPT; 当 CACHE Way 的大小大于 4KiB，一般采用 PIPT 方式，可以减轻操作系统的压力.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-----------------------------------

##### <span id="A60">CACHE 回写/通写</span>

![](/assets/PDB/HK/TH002423.png)

CACHE 写数据分为两种情况: 1. **将被改写的数据在 CACHE 中**. 2. **被改写的数据不在 CACHE 中**. 针对情况 1，CACHE 有两种策略来写数据:

* **回写(Write Back)**: 只改写 CACHE 中的 CACHE Line，不更新主存 Data Block. 优点是速度快，因为不用访问速度较慢的主存，缺点是只改写了 CACHE Line，CACHE Line 和主存 Data Block 数据不再一致，如果有别的核来访问主存中的 Data Block，那么它将读到错误的数据。另外在 CACHE Line 被替换出去的时候，数据应该被写入主存 Data Block，这就需要系统判断哪些 CACHE Line 被更新过，反应在电路上就需要增加一个 Dirty 位，当一个被标记为 Dirty 的 CACHE Line 被替换出去，其内容要被更新到主存.
* **通写(Write Through)**: 改写 CACHE 中的 CACHE Line 和主存 Data Block. 优点是时刻保持存储器数据一致，缺点是每次 store 指令都需要更新主存中的 Data Block，这个延时代价特别高.

针对第二种情况, 被改写的数据不在 CACHE 中，也有两种策略，一般情况下，**回写**和**写分配**组合，**通写**和**写不分配**组合:

* **写不分配**: 直接把数据写入主存 Data Block.
* **写分配**: 先把 Data block 放到 CACHE Line，然后回写或通写. 

###### 通写和写不分配

![](/assets/PDB/HK/TH002424.png)

上图是**通写**和**写不分配**的处理逻辑，当 CPU 执行了 Load/Store 指令需要读写数据(Load 读请求、Store 写请求)，可能出现一下几种情况:

* **Load CACHE Hit**: 此时 Load 需要读取的数据在 CACHE 中，那么直接从 CACHE Line 中读取 Load 所需的数据.
* **Load CACHE Miss**: 此时 Load 需要读取的数据不在 CACHE 中，那么首先从 CACHE 中找到一块 CACHE Line，然后从主存中将 Data Block 加载到 CACHE Line 中，最后从 CACHE Line 中读取 Load 所需的数据.
* **Store CACHE Hit**: 此时 Store 需要写入的数据在 CACHE 中，那么先将 Store 指令写入的数据写入到 CACHE Line 中，然后写入到主存的 Data Block.
* **Store CACHE Miss**: 此时 Store 需要写入的数据不在 CACHE 中，那么直接将 Store 指令写入的数据写入到主存的 Data Block 里.

###### 回写与写分配

![](/assets/PDB/HK/TH002425.png)

上图是**回写**和**写分配**的处理逻辑，当 CPU 执行 Load 指令需要读数据，可能出现以下几种情况:

* **Load CACHE Hit**: 此时 Load 需要读取的数据在 CACHE 中，那么直接将对应 CACHE Line 的数据返回给 Load 指令.
* **Load CACHE Miss 且 CACHE Line No Dirty**: 此时 Load 需要读取的数据不在 CACHE 中，CACHE 找了一块 No-Dirty 的 CACHE Line 加载主存中的 Data Block，并将 CACHE Line 标记为 No-Dirty, 最后将 CACHE Line 的数据返回给 Load 指令.
* **Load CACHE Miss 且 CACHE Line Dirty**: 此时 Load 需要读取的数据不在 CACHE 中，CACHE 找到一块 Dirty 的 CACHE Line，先将 CACHE Line 更新到主存，然后将主存中的 Data Block 加载到 CACHE Line 中，并标记 CACHE Line 为 No-Dirty，最后将 CACHE Line 的数据返回给 Load 指令.

当 CPU 执行 Store 指令需要写数据时，可能会出现一下几种情况:

* **Store CACHE Hit**: 此时 Store 需要写入的 Data Block 已经在 CACHE 中，那么直接将 Store 指令写如的数据更新到 CACHE Line 中，然后将 CACHE Line 标记为 Dirty.
* **Store CACHE Miss 且 CACHE Line No Dirty**: 此时 Store 需要写入的 Data Block 不在 CACHE 中，那么 CACHE 首先找到了一块 No-Dirty 的 CACHE Line，然后将主存中的 Data Block 加载到 CACHE Line，接着将 Store 指令需要的数据写入到 CACHE Line，最后将 CACHE Line 标记为 Dirty.
* **Store CACHE Miss 且 CACHE Line Dirty**: 此时 Store 需要写入的 Data Block 不在 CACHE 中，那么 CACHE 首先找到了一块 Dirty 的 CACHE Line，然后将 CACHE Line 的内容刷新到主存中，接着将主存中的 Data Block 加载到 CACHE Line 中，并将 Store 指令的数据写入到 CACHE Line 中，最后将 CACHE Line 标记为 Dirty.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

--------------------------------

##### <span id="A70">CACHE 替换策略</span>

![](/assets/PDB/HK/TH002426.png)

无论 CPU 执行 Store/Load 读或写数据时，一旦 CACHE 发生 CACHE Miss，那么需要替换某个 CACHE Line，然后将所需的 Data Block 加载到 CACHE Line。Load 读数据 CACHE Miss 时需要从主存中将 Data Block 加载到 CACHE 中，这个 Data Block 需要替换某个 CACHE Line，这时需要替换算法决定顶替谁; Store 写数据 CACHE Miss 时如果是**写分配**，那么需要将主存中的 Data Block 加载到 CACHE 中，因此 CACHE 也需要决定 Data Block 替换哪个 CACHE Line. CACHE 支持多种替换算法，包括 **FIFO(先进先出)**、**LRU(最近最少使用)** 和**随机替换**策略等.

###### LRU(最近最少使用) 策略

![](/assets/PDB/HK/TH002427.png)

**LRU 策略**的基本思想就是选择最近一段实践使用次数最少的 CACHE Line 进行替换. CACHE 控制器需要对一个 CACHE Set 中的每个 CACHE Line 的使用情况进行跟踪，可以通过每一个 CACHE Line 都设置**年龄位**, 但起始状态所有 CACHE Line 都没有使用过，因此只需记录已经被使用的 CACHE Line 的年龄，例如 2-Way 的 CACHE，其每个 CACHE Set 有两个 CACHE Line，但只需一个年龄位，其通过如下逻辑进行判断:

* 当两个 CACHE Line 一直没有使用，那么年龄位一直为 0，那么替换时直接选择 CACHE Line0 替换出去
* 当 CACHE Line1 被使用，年龄位依旧保持 0，那么替换时选择 CACHE Line0 替换出去
* 当 CACHE Line0 被使用，年龄位设置为 1，那么替换的时候选择 CACHE Line1 替换出去

通过案例可以知道，在 2-Way 的 CACHE Set 一个年龄位置位或者清零已经表示有一个 CACHE Line 年龄已经变大. 如果 CACHE Set 里包含的 CACHE Line 越来越多，那么N-Way CACHE Set 需要 **Log2(N)**

![](/assets/PDB/HK/TH002428.png)

在 8-Way CACHE 中，CACHE Set 采用了 3 个年龄位，每个年龄位被分成不同的年龄级层，Age-level-0 用于将 CACHE Set 中的 CACHE Line 分作两半，当 Age-level-0 置位，那么表示 CACHE Line0 到 CACHE Line3 没有被访问过，反之 Age-level-0 清零，那么 CACHE Line4 到 CACHE Line7 没有被访问过; Age-level-1 则描述 Age-level-0 识别出没有被访问过的 CACHE Line，当 Age-level-1 位清零，那么表示前半部分 CACHE Line 没有被访问过，反之 Age-level-1 位置位，那么表示后半部分的 CACHE Line 被访问过，Age-level-2 依次类推，最终会定位到最近最少被访问的 CACHE Line, 接下来以 CACHE Line4 为例子进行讲解:

![](/assets/PDB/HK/TH002429.png)

当 Age-level-0 置位，那么表示 CACHE Line4 到 CACHE Line7 没有被访问过，那么 Age-level-1 清零表示 CACHE Line4 到 CACHE Line 5 没有被访问过，最后 Age-level-2 清零表示 CACHE Line4 没有被访问过. 再如 CACHE Line0 为例进行讲解:

![](/assets/PDB/HK/TH002430.png)

当 Age-level-0 清零，那么表示 CACHE Line0 到 CACHE Line3 没有被访问过，那么 Age-level-1 清零表示 CACHE Line0 到 CACHE Line1 没有被访问过，最后 Age-level-0 清零，那么表示 CACHE Line0 没有被访问过.

![](/assets/PDB/HK/TH002431.png)

在多路 CACHE 中，需要多个年龄位进行维护，当一个 CACHE Line 被使用，那么它对应的年龄应该被设置为最大，其他 CACHE Line 的年龄按照之前的顺序排在它之后，这个过程类似于把单链表中的某个节点放到了链表表头，其余节点按之前的顺序连接在节点头之后。替换的时候总是替换年龄最小的那个 CACHE Line，也就是单链表表尾替换掉.

###### LFU(最不经常使用) 策略

LFU(Least Frequently Used) 策略将一段时间内被访问次数最少的 CACHE Line 替换出去，其原理是为每个 CACHE Line 设置一个计数器，从 0 开始计数，每访问以此对应的 CACHE Line 计数器加一。当需要替换时，将计数值最小的 CACHE Line 替换出去，同时将所有的计数器清零. 这种策略将计数周期限定在两次替换之间的时间间隔内，不能严格反应近期访问情况，新调入的块很容易被替换出去.

###### 随机替换策略

随机替换算法完全不管 CACHE 的情况，简单地根据一个随机数选择一块替换出去，随机替换算法在硬件上容易实现，且速度也比前面两种策略块。缺点就是降低了 CACHE 的命中率和 CACHE 工作效率.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

------------------------------

##### <span id="A80">CACHE 一致性</span>

![](/assets/PDB/HK/TH002462.png)

什么是 CACHE 一致性? 先看一个案例，之前的学习中可以知道 CPU-0 访问 Main Memory Data Block X 之后，其会被加载到 CPU-0 CACHE 的某个 CACHE Line，然后 CPU-0 就可以在 CACHE 中访问而不用去主存中访问数据. 在单核年代，CPU 对该数据的修改可以直接在 CACHE Line 中进行，然后配合适合的硬件同步机制再将 CACHE Line 中的数据同步到主存中; 但到了多核年代，CPU-0 CACHE 中缓存了主存 Data Block X, 然而 CPU-0 CACHE 同样也缓存 Data Block X, 那么 CPU-0 修改了 CACHE 中 Data Block X 的数据，同时 CPU-1 读取了 CACHE 中 Data Block X, 此时 Data Block X 在两个 CPU CACHE 的缓存的数据不一致，这种情况称为 **CACHE 一致性问题**.

![](/assets/PDB/HK/TH002463.png)

那么如何解决 CACHE 一致性问题呢? 这里通过一个例子进行讲解，当 CPU0 和 CPU1 的 CACHE Line 都缓存了主内存的 Data Block 的数据，且都为 0x1. 当 CPU0 对 Data block 执行写操作并写入 0x8, CPU0 更新了私有 CACHE Line 中的值，同时 CPU1 读取 Data Block 的值，此时 CPU1 发现 CACHE Line 命中，然后直接从私有的 CACHE Line 中读取 0x1. 从这里例子看到造成了 CPU0 和 CPU1 CACHE Line 数据不一致现象， 这样就会导致数据的观察者 (CPU/CPU/DMA) 看到数据不一致，因此维护 CACHE 一致性非常必要。维护 CACHE 一致性的关键就是需要跟踪每个 CACHE Line 的状态，并且根据读写操作和总线上相应的传输内容来更新 CACHE Line 在不同 CPU 上的 CACHE Hit 状态.

![](/assets/PDB/HK/TH002464.png)

维护 CACHE 一致性有软件和硬件两种方式，现在主流架构采用硬件维护. 在处理器中通过 CACHE 一致性协议实现，这些协议维护一个有限状态机, 根据存储器读写指令或总线上的传输内容，进行状态迁移或相应 CACHE Line 操作来维护 CACHE 一致性。CACHE 一致性协议主要分为两大类:

* 监听协议: 每个 CACHE Line 被监听或者监听其他 CACHE Line 的总线活动
* 目录协议: 全局统一管理 CACHE Line 状态

这里介绍主流的 **MESI 协议(Write-Once 总线监听协议)**, MESI 分别代表 Modify、Exclusive、Shared 和 Invalid. CACHE Line 的状态必须是其中的一种。前三种状态均是数据有效下的状态, CACHE Line 的 Flags 域包含了两个标志: **Dirty** 和 **Valid**，Dirty 置位代表该 CACHE Line 与主存 Data Block 内容不一致，Valid 置位则代表 CACHE Line 是有效的.

###### <span id="A801">MESI</span>

![](/assets/PDB/HK/TH002465.png)

* **M(Modify)**: CACHE Line 数据已经被修改，与主存中数据不一致，该数据只缓存在本地 CACHE Line 中，其他 CPU 没有缓存该副本.
* **E(Exclusive)**: CACHE Line 中的数据与主存一致，且该数据只在本地 CACHE Line 中，其他 CPU 没有缓存该副本.
* **S(Shared)**: CACHE Line 中的数据与主存一致，且多个 CACHE Line 都缓存该数据.
* **I(Invalid)**: 该 CACHE Line 没有缓存该数据.

**MESI** 在总线上的操作分为**本地读写**和**总线操作**. 当操作类型为**本地读写**时, CACHE Line 的状态指的是本地 CPU(Local CPU); 而当操作类型为**总线读写**时，CACHE Line 的状态指的是远端 CPU(Remote CPU):

* **本地读**: **本地 CPU** 读取 CACHE Line.
* **本地写**: **本地 CPU** 更新 CACHE Line.
* **总线读/远端读**: 总线监听一个来自远端 CPU 的读 CACHE 信号. 收到信号的 CPU 先检查 CACHE 是否存在该数据，然后广播应答
* **总线写/远端写**: 总线监听一个来自远端 CPU 的写 CACHE 信号. 收到信号的 CPU 先检查 CACHE 是否存在该数据，然后广播应答
* **总线更新**: 总线收到更新请求，请求其他 CPU 干活. 其他 CPU 收到请求后，若 CPU 有 CACHE 副本，则使其 CACHE Line 无效.
* **刷新**: 总线监听到刷新请求，收到请求的 CPU 将本地 CACHE Line 内容写会主内存
* **刷新到总线**: 收到该请求的 CPU 将本地 CACHE Line 发送到总线上，发起请求的 CPU 会获取该 CACHE Line 的内容.

![](/assets/PDB/HK/TH002466.png)

上图为 MESI 之间变换的状态图，**LRd** 表示本地读、**LWr** 表示本地写、**BusRd** 表示远端读或者监听到总线读请求、**BusWr** 表示远端写或监听到总线写请求、**FlushOpt** 表示把当前 CACHE Line 内容发到总线上、**Writeback** 表示将 CACHE Line 内容更新到内存. 接下来通过具体理解讲解每种状态之间变化过程:

> - [MESI 初始状态为 Invalid](#AA01)
>
> - [MESI 初始状态为 Modify](#AA02)
>
> - [MESI 初始状态为 Shared](#AA03)
>
> - [MESI 初始状态为 Exclusive](#AA04)

--------------------------------------------

###### <span id="AA01">初始状态为 Invalid</span>

![](/assets/PDB/HK/TH002467.png)

当本地 CACHE Line 的状态为 Invalid 时，其会触发本地 CPU 访问时 CACHE Miss，此时无论是本地读还是本地写，都会转换成总线读或者总线写信号，远端 CPU 的 CACHE 监听到总线信号之后会将其缓存的 CACHE Line 状态进行改变，具体改变可以从**读请求**和**写请求**进行分析:

###### 本地/远端读请求

![](/assets/PDB/HK/TH002473.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 为远端视角. 当 CPU0 发起**本地读请求 LRd** 或者 CPU1/CPU2 发起**远端读请求 RRd** 时，此时 CPU0 对应的 CACHE Line 为 Invalid，CPU1、CPU2 对应的 CACHE Line 的状态可能是 Invalid、Shared 或者 Exclusive, 那么所有 CACHE Line 的变化包含如下几种情况:

![](/assets/PDB/HK/TH002468.png)

**(1) 本地读全 Invalid**: 当 CPU0 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线读 BusRd**，CPU1 和 CPU2 监听到 BusRd 之后检查 CACHE 中是否包含副本，此时 CPU1 和 CPU2 的 CACHE Line 都为 Invalid，没有包含副本, 接着 CPU1 和 CPU2 向总线发送应答信号. CPU0 广播完所有的 CPU 之后发现总线上并没有数据，那么其从内存中读取数据到本地 CACHE Line，并将 CACHE Line 的状态切换到 **Exclusive**.

![](/assets/PDB/HK/TH002469.png)

**(2) 本地读远端 Shared**: 当 CPU0 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线读 BusRd**，CPU1 和 CPU2 监听到 BusRd 之后检查 CACHE 中是否包含副本，此时 CPU1 和 CPU2 检查到其缓存了副本，且 CACHE Line 的状态为 Shared，那么 CPU1/CPU2 向总线回复一个 **FlushOpt** 信号，并将 CACHE Line 的内容发送到总线上. CPU0 收到 FlushOpt 信号之后从总线上读取了数据并缓存到本地的 CACHE Line，并将 CACHE Line 的状态标记为 Shared.

![](/assets/PDB/HK/TH002470.png)

**(3) 本地读远端 Exclusive**: 当 CPU0 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线读 BusRd**，CPU1 和 CPU2 监听到 BusRd 之后检查 CACHE 中是否包含副本，此时 CPU1 的 CACHE Line 都为 Invalid，那么没有包含副本，那么 CPU1 直接向总线发送应答信号; CPU2 检查到其缓存了副本，且该 CACHE Line 的状态为 Exclusive，那么 CPU2 向总线回复一个 **FlushOpt** 信号，并将 CACHE Line 的内容发送到总线上, 并将 CACHE Line 的状态切换成 Shared. CPU0 收到 FlushOpt 信号之后从总线上读取了数据并缓存到本地的 CACHE Line，并将 CACHE Line 的状态标记为 Shared.

![](/assets/PDB/HK/TH002471.png)

**(4) 本地读远端 Modify**: 当 CPU0 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线读 BusRd**，CPU1 和 CPU2 监听到 BusRd 之后检查 CACHE 中是否包含副本，此时 CPU1 的 CACHE Line 都为 Invalid 没有包含副本，那么 CPU1 直接向总线发送应答信号; CPU2 检查到其缓存了副本，且该 CACHE Line 的状态为 Modify，那么 CPU2 向总线回复一个 **Writeback** 信号，并先将 CACHE Line 的内容更新到内存，然后发送到总线上, 并将 CACHE Line 的状态切换成 Shared. CPU0 收到 Writeback 信号之后从总线上读取了数据并缓存到本地的 CACHE Line，并将 CACHE Line 的状态标记为 Shared.

![](/assets/PDB/HK/TH002492.png)

**(5) 远端读本地 Invalid**: 当 CPU1 发起**远端读请求 RRd** 时，无论远端 CACHE Line 的状态如何，也无论是否产生**总线读请求 BusRd**，CPU0 监听到 BusRd 信号之后，检查其 CACHE 中并没有副本，然后直接应答总线，然后继续保持 Invalid.

###### 本地/远端写请求

![](/assets/PDB/HK/TH002472.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 为远端视角, 当 CPU0 发起**本地写请求 LRd** 或者 CPU1/CPU2 发起**远端写请求 RWr**，当本地 CACHE Line 为 Invalid 状态，远端 CACHE Line 可能是 Invalid、Modify、Exclusive 和 Shared，那么 CACHE Line 的变化包含以下几种情况:

![](/assets/PDB/HK/TH002474.png)

**(1) 本地写全 Invalid**: 当 CPU0 发起**本地写请求 LWr** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线写 BusWr**，CPU1 和 CPU2 监听到 BusWr 之后检查 CACHE 中是否包含副本，此时 CPU1 和 CPU2 的 CACHE Line 都为 Invalid，那么没有包含副本. 接着 CPU1 和 CPU2 向总线发送应答信号，并继续广播剩余的 CPU. CPU0 广播完所有的 CPU 之后发现总线上并没有数据，那么其从内存中读取数据到本地 CACHE Line，然后再修改 CACHE Line 中的数据，并将 CACHE Line 的状态切换到 **Modify**.

![](/assets/PDB/HK/TH002475.png)

**(2) 本地写远端 Shared**: 当 CPU0 发起**本地写请求 LWr** 时，发现本地 CACHE Line 的状态为无效，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线写 BusWr**，CPU1 和 CPU2 监听到 BusWr 之后检查 CACHE 中是否包含副本，此时 CPU1 和 CPU2 的 CACHE Line 都为 Shared，那么都有副本. 接着 CPU1 和 CPU2 向总线发送 FlushOpt 应答信号，并将副本的内容发送到总线，此时将 CACHE Line 状态都设置为 Invalid. CPU0 广播完所有的 CPU 之后发现总线上存在数据，那么其从总线上读取数据到本地 CACHE Line，然后修改 CACHE Line 中的数据，并将 CACHE Line 的状态切换到 **Modify**.

![](/assets/PDB/HK/TH002493.png)

**(3) 本地写远端 Exclusive**: 当 CPU0 发起**本地写请求 LWr** 时，发现本地 CACHE Line 的状态为无效，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线写 BusWr**，CPU1 和 CPU2 监听到 BusWr 之后检查 CACHE 中是否包含副本，此时 CPU1 没有包含对应的副本，那么直接应答总线. CPU2 中包含副本，且 CACHE Line 的状态为 Exclusive，接着 CPU2 向总线发送 FlushOpt 应答信号，并将副本的内容发送到总线，然后将 CACHE Line 状态都设置为 Invalid. CPU0 广播完所有的 CPU 之后发现总线上存在数据，那么其从总线上读取数据到本地 CACHE Line，然后修改 CACHE Line 中的数据，并将 CACHE Line 的状态切换到 **Modify**.

![](/assets/PDB/HK/TH002494.png)

**(4) 本地写远端 Modify**: 当 CPU0 发起**本地写请求 LWr** 时，发现本地 CACHE Line 的状态为无效，即 CACHE 中没有缓存对应的内容 CACHE Miss，那么在总线上产生一个 **总线写 BusWr**，CPU1 和 CPU2 监听到 BusWr 之后检查 CACHE 中是否包含副本，此时 CPU1 没有包含对应的副本，那么直接应答总线. CPU2 中包含副本，且 CACHE Line 的状态为 Modify，接着 CPU2 向总线发送 WriteBack 信号，并将副本的内容写入内存，再将副本发送到总线，然后将 CACHE Line 状态都设置为 Invalid. CPU0 广播完所有的 CPU 之后发现总线上存在数据，那么其从总线上读取数据到本地 CACHE Line，然后修改 CACHE Line 中的数据，并将 CACHE Line 的状态切换到 **Modify**.

![](/assets/PDB/HK/TH002495.png)

**(5) 远端写本地 Invalid**: 当 CPU1 发起**远端写请求 RWr** 时，无论远端 CACHE Line 的状态如何，也无论是否产生**总线读请求 BusWr**，CPU0 监听到 BusWr 信号之后，检查其 CACHE 中并没有副本，然后直接应答总线，然后继续保持 Invalid.

--------------------------------------------

###### <span id="AA02">初始状态为 Modify</span>

![](/assets/PDB/HK/TH002477.png)

当本地 CACHE Line 的状态是 Modify，那么说明本地 CPU 修改了 CACHE Line 的值，但没有刷新到内存里，是一份脏数据, 并且其他 CPU 没有缓存副本。. 本地或远端发起的读写请求都会概念 CACHE Line 的状态，具体变化如下场景:

###### 本地/远端读请求

![](/assets/PDB/HK/TH002478.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 为远端视角，由于 CPU0 的 CACHE Line 状态为 Modify，那么其他 CPU 没有副本。读请求会引起 CACHE Line 状态在 Modify 和 Shared 之间转换, 具体如下:

![](/assets/PDB/HK/TH002496.png)

**(1) 本地读请求**: 当 CPU0 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Modify，即 CACHE 中缓存对应内容 CACHE Hit, 并且 CACHE Line 中的数据是最新的，与内存中的数据不一致，那么 CPU0 直接从本地 CACHE Line 中读取数据，并保持 CACHE Line 状态为 Modify.

![](/assets/PDB/HK/TH002479.png)

**(2) 远端读本地 Modify**: 当 CPU2 发起**本地读请求 RRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应内容 CACHE Miss, 那么在总线上产生一个**总线读 BusRd**, CPU1 监听到 BusRd 之后由于没有对应的副本，那么直接应答总线; CPU0 监听到 BusRd 之后发现具有对应的 CACHE Line 副本，且此时 CPU0 CACHE Line 的状态为 Modify，那么其向总线发送一个 Writeback 信号，同时将 CACHE Line 的数据发送到总线，并且将数据也写会到内存，最后将 CPU0 CACHE Line 状态设置为 Shared; CPU2 收到总线 Writeback 信号之后从总线上获得数据，并存储在 CPU2 的 CACHE Line，最后将 CACHE Line 状态设置为 Shared.

###### 本地/总线写请求

![](/assets/PDB/HK/TH002480.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 为远端视角，由于 CPU0 的 CACHE Line 状态为 Modify，那么其他 CPU 没有副本。写请求会引起 CACHE Line 状态在 Invalid 和 Modify 之间转换, 具体场景如下:

![](/assets/PDB/HK/TH002497.png)

**(1) 本地写请求**: 当 CPU0 发起**本地写请求 LWr** 时，发现本地 CACHE Line 的状态为 Modify，即 CACHE 中缓存对应内容 CACHE Hit, 并且 CACHE Line 中的数据是最新的，与内存中的数据不一致，那么 CPU0 直接更新本地 CACHE Line 中数据，并保持 CACHE Line 状态为 Modify.

![](/assets/PDB/HK/TH002481.png)

**(2) 远端写本地 Modify**: 当 CPU2 发起**本地写请求 RRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应内容 CACHE Miss, 那么在总线上产生一个**总线写 BusWr**, CPU1 监听到 BusWr 之后由于没有对应的副本，那么直接应答总线; CPU0 监听到 BusWr 之后发现具有对应的 CACHE Line 副本，且此时 CPU0 CACHE Line 的状态为 Modify，那么其向总线发送一个 Writeback 信号，同时将 CACHE Line 的数据发送到总线，并且将数据也写会到内存，最后将 CPU0 CACHE Line 状态设置为 Invalid; CPU2 收到总线 Writeback 信号之后从总线上获得数据，并存储在 CPU2 的 CACHE Line，接着更新 CACHE Line 中的内容，最后将 CACHE Line 状态设置为 Modify.

--------------------------------------------

###### <span id="AA03">初始状态为 Shared</span>

![](/assets/PDB/HK/TH002482.png)

当本地 CACHE Line 的初始状态为 Shared，那么说明其他 CPU 也缓存了该副本，且 CACHE Line 的状态可能是 Invalid 或者 Shared. 此时无论是本地读写还是远端读写，都会改变 CACHE Line 的状态，具体改变可以从**读请求**和**写请求**场景进行分析:

###### 本地/远端读请求

![](/assets/PDB/HK/TH002498.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 远端视角. 当 CPU0 发起**本地读请求 LRd**或者 CPU1/CPU2 发起**远端读请求 RRd**，当本地 CACHE Line 的状态为 Shared 状态，远端 CACHE Line 的状态可能是 Shared 或 Invalid，那么 CACHE Line 的变化包括如下几种场景:

![](/assets/PDB/HK/TH002483.png)

**(1) 本地读请求**: 当 CPU0 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Shared，即 CACHE 中有缓存对应内容 CACHE Hit, 其他 CPU 也有相应的副本，且所有的副本与内存上的数据是一致的，因此本地 CACHE Line 的状态保持 Shared，不会向总线发起 BusRd 信号，而是直接从 CACHE Line 中读取数据.

![](/assets/PDB/HK/TH002484.png)

**(2) 远端写请求**: 当 CPU1 发起**远端读请求 RRd** 时，发现其 CACHE Line 的状态为 Invalid，即 CACHE 中有没有缓存对应内容 CACHE Miss, 那么在总线上产生一个**总线读请求 BusRd**，CPU0 监听到 BusRd 信号之后，发现其具有副本且 CACHE Line 状态为 Shared，那么直接应答一个 FlushOpt 信号，并将 CACHE Line 的内存发送到总线上，其他 CACHE Line 为 Shared 的也会做同样操作; CPU1 收到 FlushOpt 信号之后，从总线上读取数据到自己的 CACHE Line，并将 CACHE Line 信号更改为 Shared.

###### 本地/远端写请求

![](/assets/PDB/HK/TH002499.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 远端视角. 当 CPU0 发起**本地写请求 LWr**或者 CPU1/CPU2 发起**远端写请求 RWr**，当本地 CACHE Line 的状态为 Shared 状态，远端 CACHE Line 的状态可能是 Shared 或 Invalid，那么 CACHE Line 的变化包括如下几种场景:

![](/assets/PDB/HK/TH002485.png)

**(1) 本地写请求**: 当 CPU0 发起**本地写请求 LWr** 时，发现本地 CACHE Line 的状态为 Shared，即 CACHE 中有缓存对应内容 CACHE Hit, 其他 CPU 也有相应的副本，且所有的副本与内存上的数据是一致的，CPU0 向总线产生一个**总线写 BusWr**, 其他 CPU 监听到 BusWr 之后检查是否包含对应的副本，如果有则将对应的 CACHE Line 状态设置为 Invalid，并回应一个 FlushOpt 信号. CPU0 收到 FlushOpt 应答之后直接修改本地 CACHE Line 的内容，并将 CACHE Line 的状态修改为 Modify.

![](/assets/PDB/HK/TH002486.png)

**(2) 远端写请求**: 当 CPU1 发起**远端写请求 RWr** 时，发现其 CACHE Line 的状态为 Shared，即 CACHE 中有缓存对应内容 CACHE Hit, 其他 CPU 也有相应的副本，且所有的副本与内存上的数据是一致的，CPU1 向总线产生一个**总线写 BusWr**, CPU0 监听到 BusWr 之后检查包含对应的副本，CPU0 则将对应的 CACHE Line 状态设置为 Invalid，并回应一个 FlushOpt 信号. CPU2 监听到 BusWr 之后检查不包含对应的副本，直接应答总线. CPU1 收到 FlushOpt 应答之后直接修改其 CACHE Line 的内容，并将 CACHE Line 的状态修改为 Modify.

--------------------------------------------

###### <span id="AA04">初始状态为 Exclusive</span>

![](/assets/PDB/HK/TH002487.png)

当本地 CACHE Line 的状态为 Exclusive，那么说明其他 CPU 并没有该 CACHE Line 的副本，并且本地 CACHE Line 的内容和内存中的内存是一致的. 此时无论是本地读写还是远端读写，都会改变 CACHE Line 的状态，具体改变可以从**读请求**和**写请求**场景进行分析:

###### 本地/远端读请求

![](/assets/PDB/HK/TH002500.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 远端视角. 当 CPU0 发起**本地读请求 LRd**或者 CPU1/CPU2 发起**远端读请求 RRd**，当本地 CACHE Line 的状态为 Exclusive 状态，远端 CACHE Line 的状态是 Invalid，那么 CACHE Line 的变化包括如下几种场景:

![](/assets/PDB/HK/TH002488.png)

**(1) 本地读请求**: 当 CPU0 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Exclusive，即 CACHE 中有缓存对应内容 CACHE Hit, 其他 CPU 没有有相应的副本，且 CACHE Line 与内存上的数据是一致的，因此本地 CACHE Line 的状态保持 Exclusive，不会向总线发起 BusRd 信号，而是直接从 CACHE Line 中读取数据.

![](/assets/PDB/HK/TH002489.png)

**(2) 远端读请求**: 当 CPU1 发起**本地读请求 LRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应内容 CACHE Miss, 那么产生一个**总线读请求 BusRd**. CPU0 监听到 BusRd 信号之后，检查其 CACHE 中存在副本，那么向总线发送 FlushOpt 信号并将 CACHE Line 内容发送到总线上，然后将其 CACHE Line 的状态切换成 Shared; CPU2 收到 BusRd 信号之后，检查其没有对应的副本，那么直接应答总线; CPU1 收到 FlushOpt 信号之后从总线上读取内容到本地的 CACHE Line，然后将 CACHE Line 状态设置为 Shared.

###### 本地/远端写请求

![](/assets/PDB/HK/TH002501.png)

假设系统有 3 个 CPU，分别是 CPU0、CPU1 和 CPU2，并且 CPU0 为本地视角，CPU1 和 CPU2 远端视角. 当 CPU0 发起**本地写请求 LWr**或者 CPU1/CPU2 发起**远端写请求 RWr**，当本地 CACHE Line 的状态为 Exclusive 状态，远端 CACHE Line 的状态是 Invalid，那么 CACHE Line 的变化包括如下几种场景:

![](/assets/PDB/HK/TH002490.png)

**(1) 本地写请求**: 当 CPU0 发起**本地写请求 LRd** 时，发现本地 CACHE Line 的状态为 Exclusive，即 CACHE 中有缓存对应内容 CACHE Hit, 其他 CPU 没有有相应的副本，且 CACHE Line 与内存上的数据是一致的，因此直接修改 CACHE Line 的内容，并将 CACHE Line 的状态切换为 Modify.

![](/assets/PDB/HK/TH002491.png)

**(2) 远端写请求**: 当 CPU1 发起**本地写请求 LRd** 时，发现本地 CACHE Line 的状态为 Invalid，即 CACHE 中没有缓存对应内容 CACHE Miss, 那么产生一个**总线写请求 BusWr**. CPU0 监听到 BusWr 信号之后，检查其 CACHE 中存在副本，那么向总线发送 FlushOpt 信号并将 CACHE Line 内容发送到总线上，然后将其 CACHE Line 的状态切换成 Invalid; CPU2 收到 BusWr 信号之后，检查其没有对应的副本，那么直接应答总线; CPU1 收到 FlushOpt 信号之后从总线上读取内容到本地的 CACHE Line，然后修改 CACHE Line 内容，并将 CACHE Line 状态设置为 Modify.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

------------------------------

##### <span id="A90">多级 CACHE 架构</span>

![](/assets/PDB/HK/TH002408.png)

随着多核技术的不断普及以及摩尔定律的失效，CPU 的频率已经不是限制性能的主要因素，内存访问延时成为了系统性能的瓶颈，各大厂商在多核架构下通过增加 CACHE 的级数和 CACHE 容量，以此来加速对内存的访问。例如在 Intel 架构下，一个物理 Core 包含了两个逻辑核，两个逻辑核共用 L1 和 L2 CACHE，也就是一个物理核只有一个 L1 和 L2 CACHE。一个 CPU Socket 上面可能有一个或多个物理核，这些物理核共用 L3 CACHE，L3 CACHE 也被称为 **LLC(Last Level CACHE)**，L2 CACHE 则被称为 **MLC(Middle LeveL CACHE)**. L1 CACHE 又分为 Data CACHE 和 Instruct CACHE. 在有的架构中可能存在 4 级 CACHE，本文重点描述 3 级 CACHE 的架构.

![](/assets/PDB/HK/TH002407.png)

上表是 X86 架构下各级 CACHE 的 latency，CPU 访问每一级 CACHE 的延时不相同，L1 CACHE 的延时基本和指令执行的周期一致，MLC 的延时为 7ns，访问主存的时延是 100ns. 为什么采用多级 CACHE 架构就能显著提升系统性能呢? 这里通过一个例子进行讲解:

* 假设把 CPU 的一个时钟周期看做 1s，系统访问数据比作在图书管理里面查资料
* 从 L1 CACHE 读取信息就好比拿起桌上一张草稿纸(3s), 草稿纸上没有则从书包里找.
* 从 L2 CACHE 读取信息就好比是从书包里找书(14s). 书包里如果没有则去书架上找.
* 从 L3 Cache 读取信息好比从身边的书架上取出一本书(60s). 3 楼书架没有则去 1 楼书架找.
* 从主存中读取数据好比从 3 楼去 1 楼书架上查找(6 分钟). 1 楼没有则联系开发商补书.
* 从磁盘中读取数据好比联系开发商重新补印书本(长达一年零三个月).

通过上面的例子就知道多级 CACHE 对系统性能提升的重要性了吧，可以看到在性能极佳的场景下，CPU 访问的数据都在 L1 CACHE 中，系统基本没有延时的访问数据。而性能最差的场景则是所有访问的数据都在磁盘上, 系统大部分时间浪费在等待磁盘访问上，这简直是系统的噩耗.

###### 多级 CACHE 架构发展

最早出现缓存的时候其实是单级缓存(L1 CACHE), 但随着技术的不断演进，多级缓存提供了更好的低平均请求延迟 Latency(平均所有命中和未命中). 另外目前处理器采用多核架构，所有核心都共享连接相对高延迟的 DRAM，因此多级是必不可少的。另外一个原因是多级缓存设计可以分开设计功率，而功率/热量是现代 CPU 设计中最重要的限制因素之一. 目前的 CPU 处理器都有 3 级 CACHE，但是三者存在明显差异。L1 CACHE 注重速度，L2 CACHE 要在 L1 CACHE Miss 之后才发挥作用，更注重节能和容量，L3 CACHE 则更追求容量和共享。但查询一个地址时，L1、L2 和 L3 的行为是不同的:

* L1 CACHE 会把多个 CACHE Line 的 Tag 和数据全部取出，然后再比较 Tag 看哪个命中或者都没命中，如果命中直接使用数据.
* L2 CACHE 虽然也是 N 路组相联，但比较时先只取 Tag，当找到命中的 CACHE Line 之后再去把对应的数据取出来.
* L3 CACHE 位于核外，多个物理核共享，因此还需要额外考虑一致性等.

###### <span id="A902">L1 CACHE</span>

![](/assets/PDB/HK/TH002502.png)

L1 CACHE 与 CPU Core 紧密耦合，CPU 每次访问内存时都会访问 L1 CACHE，需要非常快速地返回数据，通常延时在 0.5ns，基本和时钟周期一致，因此 L1 需要非常快(低延时和高吞吐), 也意味着其命中率有限. 为了提高带宽，需要大量的读写端口来支持 L1 CACHE 的高速访问，端口数量是一个非常重要的设计点，会消耗芯片面积，这是因为端口会向 CACHE 添加线路(铜线). 同时高吞吐量也意味着能够在每个周期处理多个读取和写入(多个端口)，使得功率暴增.

L1 CACHE 又分配 L1 Data CACHE(L1D) 和 L1 Instruction CACHE(L1I), 其在 CPU 内部的布局如上图。L1I 可以放置在物理上接近代码获取逻辑，而 L1D 可以放置在物理上靠近加载/存储单元. 这样的设计当时钟周期仅持续 1/3 纳秒时(2.5GHz, 为 0.4ns)，光速传输线延迟也称为大问题，所以布线也很重要(例如 Intel 最新处理器硅片上有 13+ 层铜，类似 PCB 13 层板). 

**L1 CACHE 容量**: L1 CACHE 的特点就是快，如果 L1 变得更大，它将增加 L1 访问延迟从而降低性能, 因此 L1 CACHE 的大小几乎没有争议。目前主流处理器的 L1 CACHE 大小为 64KiB。即使决定增加 L1 CACHE，L2 CACHE 也会同步跟进增加，以此加速 L1 CACHE Miss，但这样带来了成本和功耗的暴增.

###### <span id="A903">L2 CACHE: MLC</span>

![](/assets/PDB/HK/TH002502.png)

系统中如果不存在 L2 CACHE，那么 L1 CACHE Miss 之后直接从主存中存取数据, 这样直接导致很多访问进入内存，不但意味着需要更多的内存带宽，也意味着性能严重降级. 所以 L2 CACHE 是为了缓解内存带宽压力，保持 L2 CACHE 一定大小，使其具备一定的带宽能力是非常必须的。如果架构中没有 L3 CACHE，那么 L2 CACHE 通常作为带宽过滤器，减少内存带宽使用. 如果架构中包含 L3 CACHE，那么 L2 CACHE 可以有效减少对片上互联和对 L3 的访问.

![](/assets/PDB/HK/TH002408.png)

L2 CACHE 没有像 L1 CACHE 分为 L1 Data CACHE 和 L1 Instruction CACHE，而是无差别对待指令和数据，因此称 L2 CACHE 为 L2 Unitied CACHE. L2 CACHE 的位置如上图，其位于 CPU CORE 内部，被多个逻辑核共享，且每个物理核只有一个 L2 CACHE. 虽然 L1 区分指令和数据对速度有很多的帮助，当统一的 L2 CACHE 还是最佳的选择，因为有些任务的代码量比较小，但其数据量远大于代码量，L2 CACHE的统一以适应不同的任务负载是有意义，而不是静态地划分为代码和数据.

###### <span id="A904">L3 CACHE: LLC</span>

![](/assets/PDB/HK/TH002502.png)

主流的架构中都包含了 L3 CACHE，并且 L3 CACHE 作为最后一级 CACHE，简称为 LLC. L3 CACHE 不在物理核内部，其位于一个 CPU Socket 内部，被多个物理核共享. L3 CACHE 通常使用 ECC，可以完成 ECC 在较大的块上以降低开销. 相对 L2 CACHE，L3 CACHE 速度更慢，慢速可以以较低的电压/时钟速度运行，减少热量. 甚至可以为每个存储单元使用不同的晶体管布置，以使存储器更优化功率而不是高速存储器.

###### Inclusive 与 Exclusive

在 《CACHE 一致性章节》描述了 MESI 如何维护 CACHE 一致性的，其中 CACHE Line 具有 Exclusive 状态，其表示只有该 CPU 缓存了 CACHE Line，并且 CACHE Line 中的内存与主存中的一致.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

![](/assets/PDB/BiscuitOS/kernel/IND00000I.jpg)

#### Intel® X86 架构 CACHE 机制

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

---------------------------------------

##### <span id="B">Intel® Core-i7 and Xeon CACHE</span>

![](/assets/PDB/HK/TH002516.png)

Intel 64 架构和 IA-32 架构的 CACHE 系统由Data CACHE Unit(L1)、Instruction CACHE、Data TLB、Instruction TLB、L2 CACHE、L3 CACHE、Store Buffer 以及 Trace CACHE 等硬件组成，不同系列的处理器 CACHE 架构可能有所差异，大体上可以分为 Xeon 系列和 Core 系列，**Figure 11-1** 展示 Xeon 系列处理器 CACHE 基本组成架构，**Figure 11-2** 则展示了 Core i7 处理器的 CACHE 基本架构。两种图的差异不仅体现在硬件组织方式，还体现在同种硬件的大小，其与具体的系列有关系，那么接下来对每种硬件进行介绍:

![](/assets/PDB/HK/TH002517.png)

**Trace CACHE** Trace CACHE 是一种加快指令提取的技术，使用处理器能以更短时间读取更多的指令，能有效的提高处理器性能. **L1 Instruct CACHE** 片上 L1 指令 CACHE，靠近代码获取逻辑，专门用于缓存指令，加速指令的读取. **L1 Data CACHE** 片上 L1 数据 CACHE，靠近加载/存储单元，专门用于缓存数据，加速数据的读取. **L2 Unitied CACHE** 片上的 L2 CACHE，不区分指令和数据，缓解内存带宽压力，通常作为带宽过滤器. **L3 Unified CACHE** 被同一个 Socket 上的物理核共享，不缺分指令和数据，缓解内存带宽压力. **Instruction TLB(4-KByte Pages)** 用于缓存 L1 Instruction CACHE 中虚拟地址到 4KiB 物理页的映射关系, 加速地址翻译.

**Data TLB(4-KByte Pages)** 用于缓存 L1 Data CACHE 中虚拟地址到 4KiB 物理页的映射关系, 加速地址翻译. **Instruction TLB(Large Pages)** 用于缓存 L1 Instruction CACHE 中虚拟地址到巨型物理页(2M/1G/512G HugePage)的映射关系，加速地址翻译. **Data TLB(Large Pages)** 用于缓存 L1 Data CACHE 中虚拟地址到巨型物理页(2M/1G/512G HugePage)的映射关系, 加速地址翻译. **Second-level Unified TLB(4-KByte Pages)** 用于缓存 L2 Unified CACHE 中虚拟地址到物理页的映射关系，加速地址翻译. **Store Buffer** 可先将 CPU 写数据缓存到 Store Buffer，然后给其他 CPU 发送消息，然后处理其他事，待其他 CPU 发送应答消息之后，再将数据从 Store Buffer 写入到 CACHE Line. **Write Combining(WC) Buffer** 当 L1 CACHE Write Miss，WC Buffer 为了减少 Write Miss 带来的性能开销，可以把多个对同一 CACHE Line Store 操作的数据放到 WC Buffer，等到需要读取时再一次性写入，以此减少写入的次数和总线的压力.

![](/assets/PDB/HK/TH002519.png)

**Table 11-1** 描述了各硬件在不同系列的 Intel 处理器的大小信息.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-----------------------------------------

##### <span id="B1">Intel® Smart CACHE Technology</span>

![](/assets/PDB/HK/TH002518.png)

在 Intel 12 代 Core CPU 上使用了 Smart CACHE 技术，其共享了最后一级缓存 LLC. 由于共享了最后一级 CACHE，那么 LLC 具有了 non-inclusive 特点，LLC 被所有的物理核共享(因为只有一个 Socket). 上图为 12 代 CPU 的 Hybrid CACHE 架构, CPU 被换分为 P-Core 和 E-Core，P-Core 用于处理较重的任务，而 E-Core 则是低功耗且处理相对轻松的后台任务.

**L1 CACHE**: P-Core 的 L1 CACHE 被分为数据 CACHE (DFU) 和指令 CACHE (IFU), L1 CACHE 包含了 48KiB 的 DFU 和 32KiB 的 IFU, L1 CACHE 是一个 12-Way 的组相联 CACHE 架构, P-Core L1 CACHE 不被其他物理核共享. E-Core 的 L1 CACHE 同样被分为数据 CACHE(DFU) 和指令 CACHE(IFU), L1 CACHE 包含了 64KiB DFU 和 32KiB IFU，E-Core 的 L1 CACHE 是一个 8-Way 的组相联 CACHE 架构, E-Core L1 CACHE 不被其他物理核共享.

**L2 CACHE**: L2 CACHE 不区分数据和指令，其也被称为 MLC. P-Core L2 CACHE 大小为 1.25MiB, 10-Way non-inclusive 组相联 CACHE 结构，P-Core 不与其他物理核共享 L2 CACHE. E-Core 由 4 个物理核共享一个 L2 CACHE，其 16-Way non-inclusive 组相联的 2MiB CACHE.

**L3 CACHE**: L2 CACHE 不区分数据和指令，其被称为 LLC. LLC 被所有 P-Core 共享，且每个 P-Core 物理核最大包含 3MiB 的 12-Way 组相联 CACHE. LLC 也被所有 E-Core 共享，但 4 个 E-Core 包含 3MiB 的 12-Way 组相联 CACHE.

> [From《2.4.2 12 IA Cores L1 and L2 Caches - 12th Generation Intel® Core ™Processors》](https://www.intel.com/content/www/us/en/newsroom/resources/press-kit-12th-gen-core-processors.html)

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

--------------------------------

##### <span id="B2">Intel® Memory Type</span>

![](/assets/PDB/HK/TH002520.png)

Intel X86 架构可以将所有的物理内存缓存到 L1/L2/L3 CACHE, 并且架构支持按独立页或者独立区域的粒度设置物理内存的缓存方式，在 Intel 架构下将物理内存缓存方式称为 **memory type**. **Table 11-2** 定义了 Intel 64 和 Intel IA-32 架构都支持的 **memory type**, 包括了 UC/UC-/WC/WT/WB/WP, 那么接下来对每种缓存类型进行详细的分析.

> - [Strong Uncacheable (UC)](#B21)
>
> - [Uncacheable (UC-)](#B26)
>
> - [Write Combining (WC)](#B22)
>
> - [Write-Through (WT)](#B23)
>
> - [Write-Back (WB)](#B24)
>
> - [Write-Protected (WP)](#B25)
>
> - [UC/UC-/WC/WT/WB/WP 横向对比](#B27)
>
> - [Memory Type 推荐搭配](#B28)
>
> - [Uncached Memory 编程](#B29)

----------------------------------

###### <span id="B21">Strong Uncacheable (UC)</span>

**Strong Uncacheable (UC)**: 强非缓存类型，该类型的物理内存不使用任何缓存机制. 系统对该类型的物理内存的读写请求会直接发送到总线上，另外硬件上不会对该类型物理内存的访问和对应页表遍历预测功能，同样也不会对分支进行预测. UC 缓存类型通常用于映射外设的 MMIO，因为外设不具有监听(snoop)总线的能力，为了保持设备看物理区域与 CPU 看到的一致，因此将该物理区域设置为 UC 之后，CPU 对该区域的读写请求直接访问到物理地址上，不会缓存在 CACHE 里，因此可以保持设备和 CPU 看到数据的一致性. 不建议将普通的物理内存的缓存类型设置为 UC，这样会 CPU 每次都对物理内存进行访问，这会大大影响系统性能. 接下来通过一个实践案例介绍内核如何将一段 MMIO 区域的缓存类型设置为 UC, 其在 BiscuitOS 上的部署逻辑如下:

###### <span id="B21A">内核空间虚拟地址映射 UC MMIO</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] UNCACHE(UC): Mapping UC MMIO on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHE-MMIO-KERNEL-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHE-MMIO-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHE-MMIO-KERNEL)

![](/assets/PDB/HK/TH002503.png)

源码通过一个内核模块进行展示，模块有两部分组成，第一部分是 20-35 行，模块向系统资源总线注册了一段 MMIO，这段 MMIO 起始物理地址是 BROILER_MMIO_BASE, 长度为 BROILER_MMIO_LEN, 注册完毕之后可以在系统启动之后，通过 '/proc/iomem' 接口查看到该段信息; 第二部分是 38-46 行，模块在 38 行调用 ioremap_uc() 函数将 BROILER_MMIO_BASE 开始且长度为 BROILER_MMIO_LEN 的 MMIO 区域，映射为 Uncached 的 **memory type**, 映射完毕之后函数在 46 行对 MMIO 进行访问，此时并不会触发缺页，因为 ioremap_uc() 函数已为 MMIO 分配了对应的虚拟地址并建立了页表，因此模块可以直接访问 MMIO. 最后在 BiscuitOS 上实践模块，由于需要一段真实的 MMIO 才能运行模块，因此可使用 Broiler 模拟一段 MMIO，其起始物理地址为 0xF0000000(Broiler 已经模拟好硬件)，开发者直接在源码目录执行 **make broiler** 即可:

![](/assets/PDB/HK/TH002504.png)

BiscuitOS 运行之后，加载 BiscuitOS-CACHE-UNCACHE-MMIO-KERNEL-default.ko 模块之后，从打印的内核信息可以看出模块已经映射了 MMIO，并且向 MMIO 写入 0x88520 之后可以读出正确的数据，最后查看 '/sys/kernel/debug/x86/pat_memtype_list' 节点查看系统 MMIO **memory_type** 信息，可以看到模块映射的 MMIO \[0xF0000000, 0xF0001000] 为 uncached, 另外查看 '/proc/iomem' 节点查看物理地址空间信息，可以看到 \[0xF0000000, 0xF0001000) 为 Broiler MMIO 区域, 实践符合预期. 接下来一个实践案例用于介绍如何将用户空间虚拟地址映射成 UC 的 MMIO，BiscuitOS 部署逻辑如下:

###### 用户空间虚拟地址映射 UC MMIO

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] UNCACHE(UC): Mapping UC MMIO on Userspace  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHE-MMIO-USERSPACE-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHE-MMIO-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHE-MMIO-USERSPACE)

![](/assets/PDB/HK/TH002527.png)

实践案例由两个部分组成，main.c 为内核模块部分，负责映射 MMIO 的底层逻辑。模块首先在 22-26 行定义了 MMIO 区域的信息，然后在 56-57 行通过调用 request_resource() 函数将 MMIO 区域添加到系统物理地址空间树里。模块通过注册一个 MISC 驱动，向用户空间提供 "/dev/BiscuitOS-MMIO" 接口，用户空间程序打开该接口，并调用 mmap() 映射 MMIO 时会调用到模块的 BiscuitOS_mmap() 函数，该函数首先在 32 行将虚拟地址对应的页表属性里的 \_PAGE_PAT、\_PAGE_PCD 和 \_PAGE_PWT 标志清除，然后在 34 行调用 cachemode2protval() 函数，结合 \_PAGE_CACHE_MODE_UC 将相关的 PAT 页表属性赋值到 vm_page_prot 成员里，接下来调用 io_remap_pfn_range() 函数进行虚拟地址到 MMIO 的映射工作。映射完毕之后用户空间虚拟地址就可以以 UC 方式访问 MMIO.

![](/assets/PDB/HK/TH002528.png)

实践案例的另外一部分位于 app.c 内，其为用户空间程序，用于映射虚拟地址到 MMIO 并进行访问. 程序首先在 29 行通过 open() 函数打开 "/dev/BiscuitOS-MMIO" 节点，然后在 36-46 通过 mmap() 函数将进程地址空间的虚拟地址映射到 MMIO，接下来在 49-51 行对映射之后对 MMIO 进行访问，最后在 56-57 行释放相应的资源. 源码分析完毕之后在 BiscuitOS 进行实践，由于 MMIO 绑定在具体的硬件上，此时可以使用 Broiler 进行模块，直接使用 "make broiler" 命令进行实践:

![](/assets/PDB/HK/TH002529.png) 

BiscuitOS Broiler 运行之后加载 BiscuitOS-CACHE-UNCACHE-MMIO-USERSPACE-default.ko, 接着运行用户态程序 APP，这里以后台方式运行，因此查看其他有用信息，运行之后可以看到正常访问 MMIO。接下来查看 /sys/kernel/debug/x86/pat_memtype_list 节点，可以看到 \[0xF0000000, 0xF0001000) 区域映射为 uncached，最后查看 /proc/iomem 系统物理地址空间，可以看到 Broiler MMIO 对应的区域正好是 \[0xF0000000, 0xF0001000)。实践结果符合预期，那么接下来对普通物理内存映射为 UC 场景进行实践: 

###### 用户空间虚拟内存映射 UC 物理内存

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] UNCACHE(UC): Mapping UC Memory on Userspace  --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHE-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHE-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHE-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHE-MEM-USERSPACE)

![](/assets/PDB/HK/TH002521.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 UC. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_UC, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 UC 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，其提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002522.png)

BiscuitOS 运行之后, 执行应用程序 BiscuitOS-CACHE-USER-PAGE-U-default，此时系统提示了程序预期将 \[0x10000000-0x10000fff] 为 uncached，但是系统还是将对应的 **memory type** 设置为 write-back, 这个与预期不符合. 查看内核模块源码的 3 行提示需要将 \[0x10000000, 0x10200000) 进行预留，那么在 CMDLINE(CMDLINE 位于 RunBiscuitOS.sh 文件中) 中添加预留字段后再次实践:

![](/assets/PDB/HK/TH002524.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息了，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 UC. 实践符合预期，那么接下来实践内核空间虚拟地址映射 UC 物理内存:

###### <span id="B21D">内核空间虚拟内存映射 UC 物理内存</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] UNCACHE(UC): Mapping UC Memory on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHE-MEM-KERNEL-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHE-MEM-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHE-MEM-KERNEL)

![](/assets/PDB/HK/TH002530.png)

实践案例主要目的是在内核空间申请一段内存之后，将对应的虚拟地址以 UC 的方式映射到物理内存，并进行访问。案例在 21-25 行通过 \_\_get_free_page() 函数分配一个物理页并获得对应的虚拟地址，然后在 28 行调用 set_memory_ucx() 函数进行 UC 方式的映射，案例接着在 30-33 行对映射之后的虚拟地址进行访问。最后在 36-37 行回收设置，值得注意的是当将内核空间虚拟地址设置为非 WB 之后，在回收时要主动设置为 WB。另外由于内核不支持 UC 映射物理内存方式，因此需要对内核进行修改，参考如下补丁:

![](/assets/PDB/HK/TH002531.png)

PATCH 主要做了两个事情，首先对 set_memory_ucx() 函数进行实现，然后在头文件中导出相应的函数定义，最后在 reserve_ram_page_type() 函数解除内核对 \_PAGE_CACHE_MODE_UC 不支持的逻辑。接下来在 BiscuitOS 上进行实践:

![](/assets/PDB/HK/TH002532.png)

BiscuitOS 运行之后查看 dmesg 可以看到映射过程，由于没有合入 reserve_ram_page_type() 的 patch，内核会提示异常，那么开发者可以自行合入补丁之后，该异常会消失，系统就可以将内核虚拟地址以 UC 方式映射到物理内存上. 另外通过前面的分析，如果将物理内存设置为 UC 之后，其性能将会受到极大影响，那么接下来通过一个实践案例对比 UC 和 WB(默认为 WB) 物理内存之间性能差异，案例在 BiscuitOS 的部署逻辑为:

###### UC Vs. WB 物理内存性能比对
{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Performance: WB Vs. UC on Normal Memory  --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-PERFORMANCE-WB-UC-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-PERFORMANCE-WB-VS-UC-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-PERFORMANCE-WB-VS-UC-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-PERFORMANCE-WB-VS-UC)

![](/assets/PDB/HK/TH002525.png)

基于上一个实践案例进行改进，实践案例提供了 memory_test_costtime() 函数，其逻辑是对同一个地址循环读写 9000000 次，并计算所消耗的时间，并将时间按 ns 进行打印。其他的代码逻辑和上一个分析一致，并且其也对应一个内核模块。那么接下来直接在 BiscuitOS 上运行程序查看结果(运行前在 CMDLINE 中添加预留内存字段):

![](/assets/PDB/HK/TH002526.png)

BiscuitOS 运行之后，分别运行 BiscuitOS-CACHE-PERFORMANCE-WB-VS-UC-default 5 次，查看每次 WB 和 UC 之间的差异，可以看到差异最大的一组 WB 仅仅消耗 6117ns, 而 UC 消耗达到 19907ns. 通过实践可以初步得出结论不同的 **memory type** 之间会存在性能差异，并且 WB 在读写测试中明显优于 UC 内存, 因此建议不要把普通物理内存映射为 UC!

----------------------------------

###### <span id="B26">Uncacheable (UC-)</span>

**Uncacheable (UC-)**: 非缓存类型，该类型的物理内存不使用任何缓存机制. 系统对该类型的物理内存的读写请求会直接发送到总线上，另外硬件上不会对该类型物理内存的访问和对应页表遍历预测功能，同样也不会对分支进行预测, 与 UC 不同的是 UC- 会被 WC 给覆盖. UC- 缓存类型通常用于映射外设的 MMIO，因为外设不具有监听(snoop)总线的能力，为了保持设备看物理区域与 CPU 看到的一致，因此将该物理区域设置为 UC- 之后，CPU 对该区域的读写请求直接访问到物理地址上，不会缓存在 CACHE 里，因此可以保持设备和 CPU 看到数据的一致性. 不建议将普通的物理内存的缓存类型设置为 UC-，这样会 CPU 每次都对物理内存进行访问，这会大大影响系统性能. 接下来通过一个实践案例介绍内核如何将一段 MMIO 区域的缓存类型设置为 UC-, 其在 BiscuitOS 上的部署逻辑如下:

###### <span id="B26A">内核空间虚拟地址映射 UC- MMIO</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] UNCACHED(UC-): Mapping UC- MMIO on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHED-MMIO-KERNEL-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHED-MMIO-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHED-MMIO-KERNEL)

![](/assets/PDB/HK/TH002563.png)

源码通过一个内核模块进行展示，模块有两部分组成，第一部分是 20-35 行，模块向系统资源总线注册了一段 MMIO，这段 MMIO 起始物理地址是 BROILER_MMIO_BASE, 长度为 BROILER_MMIO_LEN, 注册完毕之后可以在系统启动之后，通过 '/proc/iomem' 接口查看到该段信息; 第二部分是 38-46 行，模块在 38 行调用 ioremap() 函数将 BROILER_MMIO_BASE 开始且长度为 BROILER_MMIO_LEN 的 MMIO 区域，映射为 UC- 的 **memory type**, 映射完毕之后函数在 46 行对 MMIO 进行访问，此时并不会触发缺页，因为 ioremap() 函数已为 MMIO 分配了对应的虚拟地址并建立了页表，因此模块可以直接访问 MMIO. 最后在 BiscuitOS 上实践模块，由于需要一段真实的 MMIO 才能运行模块，因此可使用 Broiler 模拟一段 MMIO，其起始物理地址为 0xF0000000(Broiler 已经模拟好硬件)，开发者直接在源码目录执行 **make broiler** 即可:

![](/assets/PDB/HK/TH002564.png)

BiscuitOS 运行之后，加载 BiscuitOS-CACHE-UNCACHED-MMIO-KERNEL-default.ko 模块之后，从打印的内核信息可以看出模块已经映射了 MMIO，并且向 MMIO 写入 0x88520 之后可以读出正确的数据，最后查看 '/sys/kernel/debug/x86/pat_memtype_list' 节点查看系统 MMIO **memory_type** 信息，可以看到模块映射的 MMIO \[0xF0000000, 0xF0001000] 为 uncached-minus, 另外查看 '/proc/iomem' 节点查看物理地址空间信息，可以看到 \[0xF0000000, 0xF0001000) 为 Broiler MMIO 区域, 实践符合预期. 接下来一个实践案例用于介绍如何将用户空间虚拟地址映射成 UC- 的 MMIO，BiscuitOS 部署逻辑如下:

###### 用户空间虚拟地址映射 UC- MMIO

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] UNCACHED(UC-): Mapping UC- MMIO on Userspace  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHED-MMIO-USERSPACE-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHED-MMIO-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHED-MMIO-USERSPACE)

![](/assets/PDB/HK/TH002565.png)

实践案例由两个部分组成，main.c 为内核模块部分，负责映射 MMIO 的底层逻辑。模块首先在 22-26 行定义了 MMIO 区域的信息，然后在 56-57 行通过调用 request_resource() 函数将 MMIO 区域添加到系统物理地址空间树里。接下来基于 Linux 提供的 "/dev/mem" 接口进行映射 MMIO.

![](/assets/PDB/HK/TH002566.png)

实践案例的另外一部分位于 app.c 内，其为用户空间程序，用于映射虚拟地址到 MMIO 并进行访问. 程序首先在 29 行通过 open() 函数打开 "/dev/mem" 节点，然后在 36-46 通过 mmap() 函数将进程地址空间的虚拟地址映射到 MMIO，接下来在 49-51 行对映射之后对 MMIO 进行访问，最后在 56-57 行释放相应的资源. 源码分析完毕之后在 BiscuitOS 进行实践，由于 MMIO 绑定在具体的硬件上，此时可以使用 Broiler 进行模块，直接使用 "make broiler" 命令进行实践:

![](/assets/PDB/HK/TH002567.png) 

BiscuitOS Broiler 运行之后加载 BiscuitOS-CACHE-UNCACHED-MMIO-USERSPACE-default.ko, 接着运行用户态程序 APP，这里以后台方式运行，因此查看其他有用信息，运行之后可以看到正常访问 MMIO。接下来查看 /sys/kernel/debug/x86/pat_memtype_list 节点，可以看到 \[0xF0000000, 0xF0001000) 区域映射为 uncached-minus，最后查看 /proc/iomem 系统物理地址空间，可以看到 Broiler MMIO 对应的区域正好是 \[0xF0000000, 0xF0001000)。实践结果符合预期，那么接下来对普通物理内存映射为 UC- 场景进行实践: 

###### 用户空间虚拟内存映射 UC- 物理内存

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] UNCACHED(UC-): Mapping UC- Memory on Userspace --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHED-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHED-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHED-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHED-MEM-USERSPACE)

![](/assets/PDB/HK/TH002568.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 UC-. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_UC_MINUS, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 UC 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，其提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002569.png)

BiscuitOS 运行之后, 执行应用程序 BiscuitOS-CACHE-USER-PAGE-U-default，此时系统提示了程序预期将 \[0x10000000-0x10000fff] 为 uncached-minus，但是系统还是将对应的 **memory type** 设置为 write-back, 这个与预期不符合. 查看内核模块源码的 3 行提示需要将 \[0x10000000, 0x10200000) 进行预留，那么在 CMDLINE(CMDLINE 位于 RunBiscuitOS.sh 文件中) 中添加预留字段后再次实践:

![](/assets/PDB/HK/TH002570.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息了，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 UC-. 实践符合预期，那么接下来实践内核空间虚拟地址映射 UC- 物理内存:

###### <span id="B26D">内核空间虚拟内存映射 UC- 物理内存</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] UNCACHED(UC-): Mapping UC- Memory on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-UNCACHED-MEM-KERNEL-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHED-MEM-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-UNCACHED-MEM-KERNEL)

![](/assets/PDB/HK/TH002571.png)

实践案例主要目的是在内核空间申请一段内存之后，将对应的虚拟地址以 UC- 的方式映射到物理内存，并进行访问。案例在 21-25 行通过 \_\_get_free_page() 函数分配一个物理页并获得对应的虚拟地址，然后在 28 行调用 set_memory_uc() 函数进行 UC- 方式的映射，案例接着在 30-33 行对映射之后的虚拟地址进行访问。最后在 36-37 行回收设置，值得注意的是当将内核空间虚拟地址设置为非 WB 之后，在回收时要主动设置为 WB. 接下来在 BiscuitOS 上进行实践:

![](/assets/PDB/HK/TH002572.png)

BiscuitOS 运行之后，加载 BiscuitOS-CACHE-UNCACHED-MEM-KERNEL-default.ko 模块，可以看到系统可以使用 UC- 的虚拟内存，并打印字符串 “Hello BiscuitOS”, 符合预期.

----------------------------------

###### <span id="B22">Write Combining (WC)</span>

**Write Combining (WC)**: 写合并类型，该类型与 UC 类似不使用任何缓存机制，并且不保证 CACHE 一致性协议。WC 允许读预测，但写请求可能会被延后并合并缓存到 **Write Combining Buffer(WC)** 里，以此减少对总线拥堵。如果 WC Buffer 没有满，那么写请求很可能被延时并合并到 WC buffer，直到下一个串行事件发生之后，写请求才会被真正写入内存. 这类型应用的场景包括 Video Frame Buffer，其特点就是对写的顺序不是很重要，只要最后更新数据的时候都能在图形设备上显示. 具体可以参考如下章节对 WriteCombining Buffer 进一步了解, 那么接下来通过一组实践案例介绍内核和用户空间如何使用 WC 类型的内存.

> [Intel® WriteCombining Buffer]()

###### <span id="B22A">内核空间虚拟地址映射 WC MMIO</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] WriteCombining(WC): Mapping WC MMIO on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MMIO-KERNEL-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHE-WC-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WC-MMIO-KERNEL)

![](/assets/PDB/HK/TH002533.png)

源码通过一个内核模块进行展示，模块有两部分组成，第一部分是 20-35 行，模块向系统资源总线注册了一段 MMIO，这段 MMIO 起始物理地址是 BROILER_MMIO_BASE, 长度为 BROILER_MMIO_LEN, 注册完毕之后可以在系统启动之后，通过 '/proc/iomem' 接口查看到该段信息; 第二部分是 38-46 行，模块在 38 行调用 ioremap_wc() 函数将 BROILER_MMIO_BASE 开始且长度为 BROILER_MMIO_LEN 的 MMIO 区域，映射为 WriteCombining 的 **memory type**, 映射完毕之后函数在 46 行对 MMIO 进行访问，此时并不会触发缺页，因为 ioremap_wc() 函数已为 MMIO 分配了对应的虚拟地址并建立了页表，因此模块可以直接访问 MMIO. 最后在 BiscuitOS 上实践模块，由于需要一段真实的 MMIO 才能运行模块，因此可使用 Broiler 模拟一段 MMIO，其起始物理地址为 0xF0000000(Broiler 已经模拟好硬件)，开发者直接在源码目录执行 **make broiler** 即可:

![](/assets/PDB/HK/TH002534.png)

BiscuitOS 运行之后，加载 BiscuitOS-CACHE-WC-MMIO-KERNEL-default.ko 模块之后，从打印的内核信息可以看出模块已经映射了 MMIO，并且向 MMIO 写入 0x88520 之后可以读出正确的数据，最后查看 '/sys/kernel/debug/x86/pat_memtype_list' 节点查看系统 MMIO **memory_type** 信息，可以看到模块映射的 MMIO \[0xF0000000, 0xF0001000] 为 write-combining, 另外查看 '/proc/iomem' 节点查看物理地址空间信息，可以看到 \[0xF0000000, 0xF0001000) 为 Broiler MMIO 区域, 实践符合预期. 接下来一个实践案例用于介绍如何将用户空间虚拟地址映射成 WC 的 MMIO，BiscuitOS 部署逻辑如下:

###### 用户空间虚拟地址映射 WC MMIO

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] WriteCombining(WC): Mapping WC MMIO on Userspace  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MMIO-USERSPACE-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-WC-MMIO-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WC-MMIO-USERSPACE)

![](/assets/PDB/HK/TH002535.png)

实践案例由两个部分组成，main.c 为内核模块部分，负责映射 MMIO 的底层逻辑。模块首先在 22-26 行定义了 MMIO 区域的信息，然后在 56-57 行通过调用 request_resource() 函数将 MMIO 区域添加到系统物理地址空间树里。模块通过注册一个 MISC 驱动，向用户空间提供 "/dev/BiscuitOS-MMIO" 接口，用户空间程序打开该接口，并调用 mmap() 映射 MMIO 时会调用到模块的 BiscuitOS_mmap() 函数，该函数首先在 32 行将虚拟地址对应的页表属性里的 \_PAGE_PAT、\_PAGE_PCD 和 \_PAGE_PWT 标志清除，然后在 34 行调用 pgport_writecombine() 函数，将 WC 属性赋值到 vm_page_prot 成员，接下来调用 io_remap_pfn_range() 函数进行虚拟地址到 MMIO 的映射工作。映射完毕之后用户空间虚拟地址就可以以 WC 方式访问 MMIO.

![](/assets/PDB/HK/TH002536.png)

实践案例的另外一部分位于 app.c 内，其为用户空间程序，用于映射虚拟地址到 MMIO 并进行访问. 程序首先在 29 行通过 open() 函数打开 "/dev/BiscuitOS-MMIO" 节点，然后在 36-46 通过 mmap() 函数将进程地址空间的虚拟地址映射到 MMIO，接下来在 49-51 行对映射之后对 MMIO 进行访问，最后在 56-57 行释放相应的资源. 源码分析完毕之后在 BiscuitOS 进行实践，由于 MMIO 绑定在具体的硬件上，此时可以使用 Broiler 进行模块，直接使用 "make broiler" 命令进行实践:

![](/assets/PDB/HK/TH002537.png) 

BiscuitOS Broiler 运行之后加载 BiscuitOS-CACHE-WC-MMIO-USERSPACE-default.ko, 接着运行用户态程序 APP，这里以后台方式运行，因此查看其他有用信息，运行之后可以看到正常访问 MMIO。接下来查看 /sys/kernel/debug/x86/pat_memtype_list 节点，可以看到 \[0xF0000000, 0xF0001000) 区域映射为 uncached-minus，最后查看 /proc/iomem 系统物理地址空间，可以看到 Broiler MMIO 对应的区域正好是 \[0xF0000000, 0xF0001000)。实践结果不符合预期，为什么用户空间无法映射 WC MMIO 呢？ 具体分析请看. 那么接下来对普通物理内存映射为 WC 场景进行实践: 

###### 用户空间虚拟内存映射 WC 物理内存

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteCombining(WC): Mapping WC Memory on Userspace  --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WC-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WC-MEM-USERSPACE)

![](/assets/PDB/HK/TH002538.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 WC. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_WC, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 WC 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，其提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002539.png)

BiscuitOS 运行之后, 执行应用程序 BiscuitOS-CACHE-WC-MEM-USERSPACE-default，此时系统提示了程序预期将 \[0x10000000-0x10000fff] 为 write-combining，但是系统还是将对应的 **memory type** 设置为 write-back, 这个与预期不符合. 查看内核模块源码的 3 行提示需要将 \[0x10000000, 0x10200000) 进行预留，那么在 CMDLINE(CMDLINE 位于 RunBiscuitOS.sh 文件中) 中添加预留字段后再次实践:

![](/assets/PDB/HK/TH002540.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息了，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 WC. 实践符合预期，那么接下来实践内核空间虚拟地址映射 WC 物理内存:

###### <span id="B22D">内核空间虚拟内存映射 WC 物理内存</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteCombining(WC): Mapping WC Memory on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MEM-KERNEL-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WC-MEM-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WC-MEM-KERNEL)

![](/assets/PDB/HK/TH002541.png)

实践案例主要目的是在内核空间申请一段内存之后，将对应的虚拟地址以 WC 的方式映射到物理内存，并进行访问。案例在 21-25 行通过 \_\_get_free_page() 函数分配一个物理页并获得对应的虚拟地址，然后在 28 行调用 set_memory_wc() 函数进行 WC 方式的映射，案例接着在 30-33 行对映射之后的虚拟地址进行访问。最后在 36-37 行回收设置，值得注意的是当将内核空间虚拟地址设置为非 WB 之后，在回收时要主动设置为 WB, 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002542.png)

BiscuitOS 运行之后，加载 BiscuitOS-CACHE-WC-MEM-KERNEL-default.ko 模块，可以看到系统可以使用 WC 的虚拟内存，并打印字符串 "Hello BiscuitOS", 符合预期.

----------------------------------

###### <span id="B23">Write-Through (WT)</span>

**Write Through (WT)**: 通写类型，该类型会缓存读写请求。当 Read CACHE Hit 时直接从 CACHE Line 中读取数据，当 Read CACHE Miss 时先进行 CACHE Fill，然后再从 CACHE Line 中读取数据; 当 Write CACHE Hit 时，即更新 CACHE Line 的数据，也更新 Memory 中的数据, 当 Write CACHE Miss 时不会 CACHE Fills，而是直接更新 Memory 中的数据. 当 WT 类型时预测读是允许的，并且 Write Combining 也是允许的。WT 类型通常应用于 Frame Buffer 或者会访问内存的设备，这类设备会访问总线，但没有监听总线的能力. WT 强制遵循 CACHE 一致性原理. 那么接下来通过一组实践案例介绍内核和用户空间如何使用 WT 类型的内存:

###### <span id="B23A">内核空间虚拟地址映射 WT MMIO</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] WriteThrough(WT): Mapping WT MMIO on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WT-MMIO-KERNEL-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-UNCACHE-WT-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WT-MMIO-KERNEL)

![](/assets/PDB/HK/TH002543.png)

源码通过一个内核模块进行展示，模块有两部分组成，第一部分是 20-35 行，模块向系统资源总线注册了一段 MMIO，这段 MMIO 起始物理地址是 BROILER_MMIO_BASE, 长度为 BROILER_MMIO_LEN, 注册完毕之后可以在系统启动之后，通过 '/proc/iomem' 接口查看到该段信息; 第二部分是 38-46 行，模块在 38 行调用 ioremap_wt() 函数将 BROILER_MMIO_BASE 开始且长度为 BROILER_MMIO_LEN 的 MMIO 区域，映射为 Write-Through 的 **memory type**, 映射完毕之后函数在 46 行对 MMIO 进行访问，此时并不会触发缺页，因为 ioremap_wt() 函数已为 MMIO 分配了对应的虚拟地址并建立了页表，因此模块可以直接访问 MMIO. 最后在 BiscuitOS 上实践模块，由于需要一段真实的 MMIO 才能运行模块，因此可使用 Broiler 模拟一段 MMIO，其起始物理地址为 0xF0000000(Broiler 已经模拟好硬件)，开发者直接在源码目录执行 **make broiler** 即可:

![](/assets/PDB/HK/TH002544.png)

BiscuitOS 运行之后，加载 BiscuitOS-CACHE-WT-MMIO-KERNEL-default.ko 模块之后，从打印的内核信息可以看出模块已经映射了 MMIO，并且向 MMIO 写入 0x88520 之后可以读出正确的数据，最后查看 '/sys/kernel/debug/x86/pat_memtype_list' 节点查看系统 MMIO **memory_type** 信息，可以看到模块映射的 MMIO \[0xF0000000, 0xF0001000] 为 write-through, 另外查看 '/proc/iomem' 节点查看物理地址空间信息，可以看到 \[0xF0000000, 0xF0001000) 为 Broiler MMIO 区域, 实践符合预期. 接下来一个实践案例用于介绍如何将用户空间虚拟地址映射成 WT 的 MMIO，BiscuitOS 部署逻辑如下:

###### 用户空间虚拟地址映射 WT MMIO

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] WriteThrough(WT): Mapping WT MMIO on Userspace  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WT-MMIO-USERSPACE-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-WT-MMIO-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WT-MMIO-USERSPACE)

![](/assets/PDB/HK/TH002545.png)

实践案例由两个部分组成，main.c 为内核模块部分，负责映射 MMIO 的底层逻辑。模块首先在 22-26 行定义了 MMIO 区域的信息，然后在 56-57 行通过调用 request_resource() 函数将 MMIO 区域添加到系统物理地址空间树里。模块通过注册一个 MISC 驱动，向用户空间提供 "/dev/BiscuitOS-MMIO" 接口，用户空间程序打开该接口，并调用 mmap() 映射 MMIO 时会调用到模块的 BiscuitOS_mmap() 函数，该函数首先在 32 行将虚拟地址对应的页表属性里的 \_PAGE_PAT、\_PAGE_PCD 和 \_PAGE_PWT 标志清除，然后在 34 行调用 pgport_writethrough() 函数，将 WT 属性赋值到 vm_page_prot 成员，接下来调用 io_remap_pfn_range() 函数进行虚拟地址到 MMIO 的映射工作。映射完毕之后用户空间虚拟地址就可以以 WT 方式访问 MMIO.

![](/assets/PDB/HK/TH002546.png)

实践案例的另外一部分位于 app.c 内，其为用户空间程序，用于映射虚拟地址到 MMIO 并进行访问. 程序首先在 29 行通过 open() 函数打开 "/dev/BiscuitOS-MMIO" 节点，然后在 36-46 通过 mmap() 函数将进程地址空间的虚拟地址映射到 MMIO，接下来在 49-51 行对映射之后对 MMIO 进行访问，最后在 56-57 行释放相应的资源. 源码分析完毕之后在 BiscuitOS 进行实践，由于 MMIO 绑定在具体的硬件上，此时可以使用 Broiler 进行模块，直接使用 "make broiler" 命令进行实践:

![](/assets/PDB/HK/TH002547.png) 

BiscuitOS Broiler 运行之后加载 BiscuitOS-CACHE-WT-MMIO-USERSPACE-default.ko, 接着运行用户态程序 APP，这里以后台方式运行，因此查看其他有用信息，运行之后可以看到正常访问 MMIO。接下来查看 /sys/kernel/debug/x86/pat_memtype_list 节点，可以看到 \[0xF0000000, 0xF0001000) 区域映射为 write-through，最后查看 /proc/iomem 系统物理地址空间，可以看到 Broiler MMIO 对应的区域正好是 \[0xF0000000, 0xF0001000)。实践结果符合预期. 那么接下来对普通物理内存映射为 WC 场景进行实践: 

###### 用户空间虚拟内存映射 WT 物理内存

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteThrough(WT): Mapping WT Memory on Userspace --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WT-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WT-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WT-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WT-MEM-USERSPACE)

![](/assets/PDB/HK/TH002548.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 WT. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_WT, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 WT 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，其提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002549.png)

BiscuitOS 运行之后, 执行应用程序 BiscuitOS-CACHE-WT-MEM-USERSPACE-default，此时系统提示了程序预期将 \[0x10000000-0x10000fff] 为 write-through，但是系统还是将对应的 **memory type** 设置为 write-back, 这个与预期不符合. 查看内核模块源码的 3 行提示需要将 \[0x10000000, 0x10200000) 进行预留，那么在 CMDLINE(CMDLINE 位于 RunBiscuitOS.sh 文件中) 中添加预留字段后再次实践:

![](/assets/PDB/HK/TH002550.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息了，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 WT. 实践符合预期，那么接下来实践内核空间虚拟地址映射 WT 物理内存:

###### <span id="B23D">内核空间虚拟内存映射 WT 物理内存</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteThrough(WT): Mapping WT Memory on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WT-MEM-KERNEL-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WT-MEM-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WT-MEM-KERNEL)

![](/assets/PDB/HK/TH002551.png)

实践案例主要目的是在内核空间申请一段内存之后，将对应的虚拟地址以 WT 的方式映射到物理内存，并进行访问。案例在 21-25 行通过 \_\_get_free_page() 函数分配一个物理页并获得对应的虚拟地址，然后在 28 行调用 \_set_memory_wt() 函数进行 WT 方式的映射，案例接着在 30-33 行对映射之后的虚拟地址进行访问。最后在 36-37 行回收设置，值得注意的是当将内核空间虚拟地址设置为非 WB 之后，在回收时要主动设置为 WB, 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002552.png)

BiscuitOS 系统启动过程中，可以看到 dmesg 打印字符串 "Hello BiscuitOS", 以及相应的虚拟地址，符合预期.

----------------------------------

###### <span id="B24">Write-Back (WB)</span>

**Write Back (WB)**: 回写类型，该类型会缓存读写请求。当 Read CACHE Hit 时直接从 CACHE Line 中读取数据，当 Read CACHE Miss 时先进行 CACHE Fill，然后再从 CACHE Line 中读取数据; 当 Write CACHE Hit 时，尽可能只更新 CACHE Line 的数据, 当 Write CACHE Miss 时会 CACHE Fills，并尽可能只更新 CACHE Line 中的数据. 当 WB 类型时预测读是允许的，并且 Write Combining 也是允许的。WB 类型内存由于写请求尽可能在 CACHE Line 中进行，很大程度上缓解了总线拥堵的问题. 与 WT 类型不同的是当写请求时不是立即写入到内存，而是尽可能的只写如到 CACHE Line，直到 Write-Back 信号到来时才把缓存中已经修改的数据更新到内存。Write-Back 操作可能发生在 CACHE 已经满的请求下，需要新分配一个 CACHE Line，那么被淘汰的 CACHE Line 就会执行 Write-Back 操作. 另外 CACHE 一致性协议针对其他核对 M 状态的 CACHE Line 进行读时也会触发 Write-Back 操作. WB 可以提供最好的性能，但其要求采用该类型的设备具有监听(snoop)内存访问，以此维护内存和 CACHE 的一致性. 那么接下来通过一组实践案例介绍内核和用户空间如何使用 WB 类型的内存:

###### 用户空间虚拟地址映射 WB MMIO

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] WriteBack(WB): Mapping WB MMIO on Userspace  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WB-MMIO-USERSPACE-default
# 部署源码
make download
# 在 BiscuitOS/Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-WB-MMIO-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WB-MMIO-USERSPACE)

![](/assets/PDB/HK/TH002553.png)

实践案例由两个部分组成，main.c 为内核模块部分，负责映射 MMIO 的底层逻辑。模块首先在 22-26 行定义了 MMIO 区域的信息，然后在 56-57 行通过调用 request_resource() 函数将 MMIO 区域添加到系统物理地址空间树里。模块通过注册一个 MISC 驱动，向用户空间提供 "/dev/BiscuitOS-MMIO" 接口，用户空间程序打开该接口，并调用 mmap() 映射 MMIO 时会调用到模块的 BiscuitOS_mmap() 函数，该函数首先在 32 行将虚拟地址对应的页表属性里的 \_PAGE_PAT、\_PAGE_PCD 和 \_PAGE_PWT 标志清除，然后在 34 行调用 cachemode2protval() 函数，将 WB 属性赋值到 vm_page_prot 成员，接下来调用 io_remap_pfn_range() 函数进行虚拟地址到 MMIO 的映射工作。映射完毕之后用户空间虚拟地址就可以以 WB 方式访问 MMIO.

![](/assets/PDB/HK/TH002554.png)

实践案例的另外一部分位于 app.c 内，其为用户空间程序，用于映射虚拟地址到 MMIO 并进行访问. 程序首先在 29 行通过 open() 函数打开 "/dev/BiscuitOS-MMIO" 节点，然后在 36-46 通过 mmap() 函数将进程地址空间的虚拟地址映射到 MMIO，接下来在 49-51 行对映射之后对 MMIO 进行访问，最后在 56-57 行释放相应的资源. 源码分析完毕之后在 BiscuitOS 进行实践，由于 MMIO 绑定在具体的硬件上，此时可以使用 Broiler 进行模块，直接使用 "make broiler" 命令进行实践:

![](/assets/PDB/HK/TH002555.png) 

BiscuitOS Broiler 运行之后加载 BiscuitOS-CACHE-WB-MMIO-USERSPACE-default.ko, 接着运行用户态程序 APP，这里以后台方式运行，因此查看其他有用信息，运行之后可以看到正常访问 MMIO。接下来查看 /sys/kernel/debug/x86/pat_memtype_list 节点，可以看到 \[0xF0000000, 0xF0001000) 区域映射为 uncached-minus，最后查看 /proc/iomem 系统物理地址空间，可以看到 Broiler MMIO 对应的区域正好是 \[0xF0000000, 0xF0001000)。实践结果不符合预期，具体原因见. 那么接下来对普通物理内存映射为 WB 场景进行实践: 

###### 用户空间虚拟内存映射 WB 物理内存

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteBack(WB): Mapping WB Memory on Userspace --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WB-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WB-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WB-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WB-MEM-USERSPACE)

![](/assets/PDB/HK/TH002556.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 WB. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_WB, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 WB 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，其提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002557.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 WB. 实践符合预期，那么接下来实践内核空间虚拟地址映射 WB 物理内存:

###### <span id="B24D">内核空间虚拟内存映射 WB 物理内存</span>

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteBack(WB): Mapping WB Memory on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WB-MEM-KERNEL-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WB-MEM-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WB-MEM-KERNEL)

![](/assets/PDB/HK/TH002558.png)

实践案例主要目的是在内核空间申请一段内存之后，将对应的虚拟地址以 WB 的方式映射到物理内存，并进行访问。案例在 21-25 行通过 \_\_get_free_page() 函数分配一个物理页并获得对应的虚拟地址，然后在 28 行调用 set_memory_wb() 函数进行 WB 方式的映射，案例接着在 30-33 行对映射之后的虚拟地址进行访问。最后在 36-37 行回收设置, 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002559.png)

BiscuitOS 运行之后，加载 BiscuitOS-CACHE-WB-MEM-KERNEL-default.ko 模块，可以看到系统可以使用 WB 的虚拟内存，并打印字符串 “Hello BiscuitOS”, 符合预期.

----------------------------------

###### <span id="B25">Write-Protected (WP)</span>

**Write Protected (WP)**: 写保护类型，该类型读请求从 CACHE Line 中获得数据，当 Read CACHE Miss 时触发 CACHE Fill. 但写请求会广播到总线，让其他 CPU 缓存副本的 CACHE Line 在总线上的数据全部无效，这个场景优点类似于所有 CPU 对一个 Shared 状态的 CACHE Line 进行写操作时，每个 CPU 都有将 CACHE Line 设置为 Modify 的可能，但此时采用 WP 的 CPU 会让其他 CPU 的 Modify 都无效，只有自己的 CACHE Line 可以设置为 Modify。另外 WP 时分支读是允许的, 那么接下来通过一组实践案例介绍内核和用户空间如何使用 WP 类型的内存:

###### 用户空间虚拟内存映射 WP 物理内存

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteProtected(WP): Mapping WP Memory on Userspace --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WP-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WP-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WP-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WP-MEM-USERSPACE)

![](/assets/PDB/HK/TH002560.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 WP. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_WP, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 WP 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，其提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002561.png)

BiscuitOS 运行之后, 执行应用程序 BiscuitOS-CACHE-WP-MEM-USERSPACE-default，此时系统提示了程序预期将 \[0x10000000-0x10000fff] 为 write-protected，但是系统还是将对应的 memory type 设置为 write-back, 这个与预期不符合. 查看内核模块源码的 3 行提示需要将 \[0x10000000, 0x10200000) 进行预留，那么在 CMDLINE(CMDLINE 位于 RunBiscuitOS.sh 文件中) 中添加预留字段后再次实践:

![](/assets/PDB/HK/TH002562.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 WP. 实践符合预期，那么接下来实践内核空间虚拟地址映射 WP 物理内存:

----------------------------------

###### <span id="B27">UC/UC-/WC/WT/WB/WP 横向对比</span>

![](/assets/PDB/HK/TH002573.png)

本节对不同的 memory type 进行横向对比，在上表中对于 Read/Write 请求是否缓存，UC、UC-、WC 全部不缓存，WT、WB 全部缓存，WP 只对读请求进行缓存; 只有 WB 支持 Writeback 不一定访问内存，其余类型 Write 都会访问内存; UC、UC- 不支持分支读预测功能，其余都支持分支读预测功能;

![](/assets/PDB/HK/TH002574.png)

上图展示的是当 Read/Write Hit/Miss 时不同 memory type 的动作: WB Read/Write hit 仅仅访问 CACHE Line，而 Read/Write Miss 时都会先触发 CACHE Fill，然后在访问缓存; UC Read/Write 无视 Hit/Miss，直接访问内存; WT Read Hit 只访问 CACHE Line，Write Hit 是不仅访问 CACHE Line，还会更新内存，Read Miss 是先触发 CACHE Fill 再访问 CACHE Line，Write Miss 时仅仅访问内存; WP Read Hit 直接访问 CACHE Line，Read Miss 触发 CACHE Fille 再访问 CACHE Line，Write Hit 时直接访问内存并让其他 CACHE Line 副本 Invalid，Write Miss 直接访问内存.

---------------------------------

###### <span id="B28">Memory Type 适用场景</span>

不同的 Memory Type 适用与不同的场景，本节用于分析系统推荐的 memory type 场景, 总结如下:

* 普通物理内存(不包括 Frame Buffer) memory type 均采用 WB.
* 具有 IO Agent 且能维护 CACHE 一致性的外设，其能 DMA 的物理内存 memory type 采用 WB.
* MMIO 采用 UC/UC-
* Dual-ported Memory 可以采用 UC/UC-/WT/WC
* Frame Buffer 可以采用 UC/UC-/WT/WC. 其特点就是大量的写而很少的读操作.

###### <span id="B29">Uncacahed Memory 编程</span>

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

----------------------------------

##### <span id="B3">Intel® CACHE Coherency Protocol</span>

![](/assets/PDB/HK/TH002575.png)

在 Intel 64 和 IA-32 架构里，L1 Data CACHE、L2 和 L3 Unified CACHE 支持 MESI(Modify、Exclusive、Shared、Invalid) CACHE Line 状态，以此维护 CACHE 一致性. L1 Data CACHE、L2 和 L3 Unified CACHE 的每个 CACHE Line 具有两个 MESI 状态，每个 CACHE Line 可以标记为上图中的四种状态。同理在 X86 架构下 MESI 状态对软件依旧透明. 在 L1 Instruction CACHE 因为是只读的，因此 CACHE Line 只支持了 SI(Shared、Invalid) 状态。在多核架构里，IA-32 和 Intel 64 架构有能力监听(Snoop) 其他处理器访问内存和各自内部 CACHE. 处理器利用监听能力，保证了其内部的 CACHE 与内存和其他处理其内部 CACHE 的一致性.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="B4"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000T.jpg)

##### Intel® MTRRs Technology

![](/assets/PDB/HK/TH002576.png)

**MTRRs** The memory type range registers: Intel® 在 P6 之后向系统提供了 MTRRs 技术，MTRRs 运行为 RAM、ROM、Frame-Buffer 内存和 MMIO 对应的物理区域设置不同的 **Memory Type**. MTRRs 机制通过提供一系列的 MSR 寄存器用于指定物理区域范围和 Memory Type，上表描述了 MTRR 映射物理区域的范围，可以分为三类:

* **固定物理区域 MTRRs(Fixed MTRRs)**, MTRRs 提供了多个 MSR 寄存器，这些寄存器针对固定的物理区域可以设置指定的 Memory Type
* **可变物理区域 MTRRs(Variable MTRRs)**, MTRRs 提供了多对 MSR 寄存器，每一对寄存器可以设置物理区域和 Memory Type
* **默认 MTRR(Default MTRRs)**, MTRRs 提供了一个 MSR 用于设置默认的 Memory Type，针对不在前两种覆盖范围的物理区域，均采用默认的 Memory Type.

![](/assets/PDB/HK/TH002577.png)

上表描述了 MTRRs 可以配置的 Memory Type 种类，以及每种类型的编码. MTRRs 支持 UC、WC、WT、WP 和 WB 五种 Memory Type，当在 MTRRs 寄存器中配置了 Reserved 之后会引起 **general-protection exception (#GP)**. 当系统 Reset 之后，硬件会 Disable 所有的 Fixed 和 variable MTRRs，并将所有物理区域设置为 Uncached，因此系统初始化需要为指定的物理区域设置指定的 Memory Type, 典型的做法是 BIOS 负责 MTRRs 的初始化，然后操作系统或软件再结合 PAT 设置最终的 Memory Type。PAT(Page Attribute Table) 技术可以提供页级的 Memory Type 设置能力，PAT 与 MTRRs 组合形成最终的 Memory Type，因此 MTRRs 机制只是设置物理区域的 Memory Type，但不能决定最终的 Memory Type。

![](/assets/PDB/HK/TH002578.png)

**MTRR Memory type** 指明通过 MTRRs 机制对某段物理区域设置的 Memory Type，**PAT Entry Value** 通过页表的 PAT、PWT、PAT 属性进行设置的 Memory Type，两者结合形成 **Effective Memory Type**, 其为最终生效的 Memory Type.

###### MTRRs Feature Identification

![](/assets/PDB/HK/TH002579.png)

系统是否启用 MTRRs 机制可以通过查看 CPUID 指令(EAX=01H), 当指令返回时可以查看 EDX 寄存器，**Bit12 MTRR** 指明是否启用 MTRRs 机制，如果该标志位置位，那么 MTRRcap MSR 寄存器记录了 MTRRs 所支持的 Memory Type，以及多少个可变 MTRRs 寄存器等信息; 反之该标志位清零，那么系统不支持 MTRRs 机制.

![](/assets/PDB/HK/TH002580.png)

当系统支持 MTRRs 机制之后，可以从 IA32_MTRRCAP 寄存器获得更多 MTRRs 机制信息. 上图为该寄存器的位图: WC 标志位指明系统是否支持 Write-Combining Memory Type，当置位时系统支持 WC Memory Type, 反之不支持 WC Memory Type; FIX 标志位用于指明 MTRRs 是否支持固定物理区域 MTRR，如果支持那么 MTRRs 提供了一些列的 MSR 寄存器 (IA32_MTRR_FIX64K_00000 - IA32_MTRR_FIX4K_0F8000)，这些寄存器可以设置固定物理区域的 Memory Type; SMRR 标志位指明是否支持 System-Management Range Register 机制; VCNT 字段指明系统支持可变 MTRRs 寄存器的数量. 

![](/assets/PDB/HK/TH002581.png)

对于系统物理区域既不属于**固定物理区域 MTRRs** 又不属于**可变物理区域 MTRRs** 管理的区域，系统使用**默认 MTRRs** 方式设置这些物理区域的 Memory Type。MTRRs 提供了 IA32_MTRR_DEF_TYPE 寄存器用于配置默认的 Memory Type，上图为该 MSR 寄存器的位图. E 标志位用于控制是否使能 MTRR 机制，当标志位置位，那么系统使能 MTRR 机制，反之当标志位清零，那么系统关闭 MTRR 机制; FE 标志位用于控制开启或关闭**固定物理区域 MTRRs**, 当标志位置位，那么系统启用**固定物理区域 MTRRs**，反之关闭**固定物理区域 MTRRs**; Type 字段用于指明默认的 Memory Type，Intel 推荐对不存在物理内存的物理区域的 Memory Type 设置为 UC，那么可以将 Default Memory Type 设置为 UC.

![](/assets/PDB/HK/TH002582.png)

通过上面的分析，系统需要通过检查 CPUID、IA32_MTRRCAP、IA32_MTRR_DEF_TYPE 寄存器相互配合，才能获得系统支持 MTRRs 机制的情况。如果 CPUID 就检测不支持 MTRR，那么剩余两个寄存器就没必要检查.

###### 固定物理区域 MTRRs

![](/assets/PDB/HK/TH002583.png)

当系统使能 **FIXED-RANGE MTRRs** 之后，系统提供了一系列的寄存器用于配置固定物理区域的 Memory Type，例如上图的 IA32_MTRR_FIX64L_00000 寄存器，其用于设置 \[0x00000, 0x80000) 区域的 Memory Type，并且将寄存器划分为 8 个区域，每个区域的长度为 64KiB，并且 BIt0 到 BIT7 表示第一个 64KiB 区域，也就是 \[0x00000, 0x10000) 物理区域，以此类推，操作系统或者 BIOS 可以向每个字段写入指定的 Memory Type，Memory Type 的编码如下表:

![](/assets/PDB/HK/TH002577.png)

向字段写入不支持的 Memory Type 会引起 **general-protection exception (#GP)**. IA32_MTRR_FIX16K_80000 至 IA32_MTRR_FIX16K_A0000 寄存器集按 16KiB 粒度设置了 \[0x80000, 0xC0000) 物理区域 Memory Type. IA32_MTRR_FIX4K_C0000 至 IA32_MTRR_FIX4K_F8000 寄存器集按 4KiB 粒度设置了 \[0xC0000, 0x100000) 物理区域的 Memory Type.

###### 可变物理区域 MTRRs

![](/assets/PDB/HK/TH002584.png)

当系统置位 IA32_MTRR_DEF_TYPE 寄存器的 E 标志位，那么系统使能 Variable-RANGE MTRRs，另外在 IA32_MTRRCAP 寄存器的 VCNT 字段指令了系统所支持 Variable-RANGE MTRRs 寄存器的数量. 每个 Variable-RANGE MTRRs 寄存器由两个寄存器配对组成，分别是上图的 IA32_MTRR_PHYSBASEn 和 IA32_MTRR_PHYSMASKn 寄存器。IA32_MTRR_PHYSBASEn 寄存器的 PhysBase 字段用于指明物理区域的起始地址，该地址按 PAGE_SIZE 进行对其，Type 字段用于设置该区域的 Memory Type. IA32_MTRR_PHYSMASKn 寄存器的 PhysMask 字段用于设置物理区域掩码，与 PhysBase 字段值一起计算出可变物理区域的长度，V 字段指明了设置是否有效. 那么接下来通过一个实践案例讲解如何使用 MTRR 设置 Memory Type:

###### MTRR 设置 Memory Type

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteCombining(WC): Mapping WC Memory on Userspace  --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WC-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WC-MEM-USERSPACE)

![](/assets/PDB/HK/TH002538.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 WC. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_WC, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 WC 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，模块在 60 行调用 mtrr_add() 函数设置 \[MTRR_MEM_BASE, MTRR_MEM_BASE + MTRR_MEM_SIZE) 区域 Memory Type 为 WB. 模块提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002539.png)

BiscuitOS 运行之后, 执行应用程序 BiscuitOS-CACHE-WC-MEM-USERSPACE-default，此时系统提示了程序预期将 \[0x10000000-0x10000fff] 为 write-combining，但是系统还是将对应的 **memory type** 设置为 write-back, 这个与预期不符合. 查看内核模块源码的 3 行提示需要将 \[0x10000000, 0x10200000) 进行预留，那么在 CMDLINE(CMDLINE 位于 RunBiscuitOS.sh 文件中) 中添加预留字段后再次实践:

![](/assets/PDB/HK/TH002587.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息了，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 WC. 接着查看 "/proc/mtrr" 节点，可以看到 0x10000000 起始的 2MiB 区域都是 WB，实践符合预期. 通过实践可以看出内核通过 mtrr_add() 函数可以实现对指定物理区域设置 Memory Type，接下来查看其函数调用逻辑:

![](/assets/PDB/HK/TH002585.png)

mtrr_add() 函数的调用逻辑如上，核心步骤是通过 generic_get_free_region() 函数获得一个空闲的 Variable-RANGE MTRRs 寄存器组，然后调用 generic_set_mtrr() 函数设置相应的 MTRRs 寄存器，其函数逻辑如下:

![](/assets/PDB/HK/TH002586.png)

generic_set_mtrr() 函数的核心步骤在 50-56 行，可以看出函数是如何构造 IA32_MTRR_PHYSBASEn 寄存器的 PhysBase 字段和 IA32_MTRR_PHYSMASKn 寄存器的 PhysMask 字段，设置完毕之后同个mtrr_wrmsr() 函数写入相应的寄存器内。在设置 MTRR 寄存器过程中，分别调用 prepare_set() 函数和 post_set() 函数保证设置生效.

###### Interface with /proc/mtrr

![](/assets/PDB/HK/TH002588.png)

Linux 提供了 /proc/mtrr 接口可以以 ASCII 的方式对指定物理区域的 MTRRs 进行读取和写入，该接口一般给系统管理员使用. 可以通过写入一段字符串实现实现对某段物理内存的 MTRR 设置，也可以通过 cat 该节点获得当前系统设置的 MTRR 信息，如上图，regN 表示系统第 N+1 个物理区域, base 字段表示物理区域的起始地址，size 字段表示物理区域的长度，接下来的区域表示 Memory Type，最后 count 字段表示物理区域的数量.

![](/assets/PDB/HK/TH002589.png)

通过 echo 命令向 /proc/mtrr 接口写入一段字符串，可以动态设置某段物理区域的 Memory Type，接口支持写入的参数包括 base、size、type 字段，其中 base 表示物理区域的起始物理地址，size 表示物理区域的长度、type 表示需要设置 Memory Type 的种类，接口目前支持的 Memory Type 包括: write-combining、write-back、write-through、uncachable.

![](/assets/PDB/HK/TH002590.png)

"/proc/mtrr" 接口也支持移除某段物理区域的 MTRRs 设置，可以通过向接口写入字符串 "disable=N", 其中 N 表示物理区域 regN 的标号 N，例如上图向移除 reg01, 那么此时 N 为 1，于是可以向接口写入 "disable=1" 即可移除 reg01 的 MTRRs 设置.

###### IOCTL with MTRRs

用户空间除了可以直接使用 echo 和 cat 命令对 MTRRs 设置进行读取和写入，也可以在代码里通过 IOCTL 实现对物理区域 MTRRs 的读取和写入. 其原理 /proc/mtrr 节点向用户空间提供了 ioctl 接口，因此代码里可以实现 MTRRs 的读取写入，接下来通过案例讲解 ioctl 的使用, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] MTRRs: Read MTRRs via IOCTL on Userspace  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-MTRR-IOCTL-READ-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-MTRR-IOCTL-READ-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-MTRR-IOCTL-READ)

![](/assets/PDB/HK/TH002591.png)

源码很精简，程序首先在 64 行通过 open() 函数打开 "/proc/mtrr" 节点，然后在 74 行通过 ioctl() 函数结合 MTRRIOC_GET_ENTRY 获得一个有效的 Entry，然后使用 for 循环遍历每个 Entry。struct mtrr_gentry 数据结构里维护的 regnum、base、size 和 type 成员，详细描述了 MTRRs 维护的一段物理区域，程序在 82 行结合 mtrr_strings[] 字符数组和 type 成员，最终打印了每段物理区域的 Memory Type 信息。接下来在 BiscuitOS 上实际运行案例:

![](/assets/PDB/HK/TH002592.png)

BiscuitOS 启动之后，运行 BiscuitOS-CACHE-MTRR-IOCTL-READ-default 应用程序，可以看到其打印了 MTRRs 所有 Region，无论 Region 是否真的维护物理区域，统统打印，可以看到 Region0 的起始地址为 0x80000000, 长度为 0x80000000, 该区域的 Memory Type 是 uncachable. 同时使用 cat 查看 "/proc/mtrr" 节点，打印的信息一致，实践符合预期. 接下来通过案例讲解 ioctl 实现 mtrrs 写入功能，其在 BiscuitOS 上部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] MTRRs: Write MTRRs via IOCTL on Userspace  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-MTRR-IOCTL-WRITE-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-MTRR-IOCTL-WRITE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-MTRR-IOCTL-WRITE)

![](/assets/PDB/HK/TH002593.png)

源码很精简，程序首先在 69-75 行从传入 argv 参数中获得 base、size 和 type 信息，并存储在 struct mtrr_sentry 数据结构里，因此程序运行的时候必须包含以上三个参数，否则程序直接异常退出. 程序在 72-80 行会对传入的 type 信息进行检查，查看是否符合 mtrr_strings[] 定义的可用类型。程序接着在 82 行调用 open() 函数打开 "/proc/mtrr" 节点，打开成功之后在 92 行调用 ioctl() 函数，并传入 MTRRIOC_ADD_ENTRY 参数，因此将新增的 Entry 传入到系统 MTRRs 子系统，如果 ioctl() 函数执行成功，那么说明新增物理区域的 Memory Type 设置成功，那么只要节点没有关闭，就可以通过 "/proc/mtrr" 节点查看新增的区域，因此在 99 行为了调试进行无限睡眠，那么接下来在 BiscuitOS 上实际运行案例:

![](/assets/PDB/HK/TH002594.png)

BiscuitOS 启动之后，首先查看 "/proc/mtrr" 节点，以此获得当前系统维护的 MTRRs 物理区域，接着运行 BiscuitOS-CACHE-MTRR-IOCTL-WRITE-default，并传入三个参数，第一个参数表示物理区域的起始物理地址，第二个参数表示物理区域的长度，最后一个参数表示需要设置的 Memory Type，为了调试效果，此时压后台运行。程序运行之后查看 "/proc/mtrr" 可以看到新增的物理区域 \[0x10000000, 0x10001000) 的 Memory Type 已经设置为 write-protect, 符合预期.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="B5"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000H.jpg)

##### Intel® PAT Technology

**PAT (Page Attribute Table)** 技术是拓展了 IA-32 页表属性，可以基于页表设置线性地址映射物理内存的 Memory Type. 对比 MTRRs 机制，MTTRs 可以看成是基于物理区域设置物理内存的 Memory Type，而 PAT 是基于线性地址(等同于虚拟地址)设置映射物理内存的 Memory Type。PAT 需要配合 MTRRs 才能使用, PAT 可以设置 Memory Type，MTRRs 也可以设置 Memory Type，只有两种机制设置的 Memory Type 组合才能形成最终生效的 Memory Type.

![](/assets/PDB/HK/TH001463.png)

在 Intel 架构下，支持 3-/4-/5-level 级页表，可以映射虚拟地址(线性地址)到物理地址，PAT 按照线性地址映物理地址的粒度设置 Memory Type，那么 PAT 支持的线性地址粒度可以是 4KiB、2MiB、4MiB(i386 Only)、1Gig 以及 512Gig.

![](/assets/PDB/HK/TH002596.png)

例如 PAT 按 4KiB 线性地址设置映射物理内存的 Memory Type，那么在 PTE 页表中存在 PWT、PCD、PAT 三个标志位，系统或者软件可以通过设置 3 个标志位，使映射的物理内存采用不同的 Memory Type. 假设 PAT 按 2MiB 线性地址设置映射物理内存的 Memory Type，那么 PDE 页表中的 PWT、PCD、PAT 三个标志位起作用, 其他粒度依次类推.

![](/assets/PDB/HK/TH002597.png)

页表中的 PAT、PCD、PWD 排列组合可以形成不同的 PAT Entry, 每个 PAT Entry 对应不同的编码值. 页表中设置好三个标志位之后，PAT 机制会根据 PAT Entry 找到对应的编码.

![](/assets/PDB/HK/TH002595.png)

PAT 采用 IA32_PAT MSR 寄存器为了所有的 PAT Entry，上图为该寄存器的位图，每 8 bit 一组，从低到高依次为 Entry 编号，页表中的 PAT、PCD、PWT 组合选择的 PAT Entry X 即该寄存器里的 PAx. 系统在初始化过程中，会为每个 PAx 填入 Memory Type 对应的编码，也就是说 PAx 里可以灵活的填入系统所规划的 Memory Type.

![](/assets/PDB/HK/TH002598.png)

IA32_PAT MSR 寄存器每组 PAx 所支持的 Memory Type 编码如上图，例如系统软件想让 PA0 里的 Memory Type 为 WT，那么 PA0 里的值为 04H，另外页表向将虚拟地址映射的物理内存 Memory Type 设置为 WT，那么页表的 PAT、PCD、PWT 组合选择 PAT0 即可, 这样极大的提高了 PAT 设置的灵活性.

![](/assets/PDB/HK/TH002578.png)

当通过页表的 PAT、PWT、PCD 三个标志位选择了线性地址映射物理内存的 Memory Type 之后，该 Memory Type 还不是最终生效的 Memory Type，硬件上会找到线性地址映射物理内存对应的 MTRRs 设置，从而获得 MTRRs 对应的 Memory Type，此时硬件将 MTRRs 和 PAT 获得的 Memory Type 按上表结合生成最终的 Memory Type. 接下来通过一个实践案例了解 PAT 机制的使用方法, 其在 BiscuitOS 上部署逻辑如下:

###### PAT 设置 Memory Type

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteCombining(WC): Mapping WC Memory on Userspace  --->
          -*- CACHE User-Page for Kernel Stub (Basic)  --->

# 进入源码目录
# Userspace: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MEM-USERSPACE-default/
# Kernel: BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default/
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WC-MEM-USERSPACE-default/
# 部署源码
make prepare
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WC-MEM-USERSPACE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WC-MEM-USERSPACE)

![](/assets/PDB/HK/TH002538.png)

源码分成两部分，其中一部分为用户空间程序(上图所示), 用户空间代码的主要功能是映射一段虚拟内存，并将其缓存类型设置为 WC. 函数首先在 34 行将 pcm 变量设置为 \_PAGE_CACHE_MODE_WC, 然后在 39 行打开 "/dev/BiscuitOS-CACHE" 节点，并基于该节点映射长度为 PAGE_SIZE 的虚拟内存，此时在函数 50 行将 pcm 传入到 mmap() 函数，映射完毕后函数在 58-59 行使用 WC 的内存，使用完毕在 61-62 行解除映射并关闭文件。

![](/assets/PDB/HK/TH002523.png)

> [BiscuitOS-CACHE-USER-PAGE-Kernel-Stub-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-USER-PAGE-Kernel-Stub)

源码的另外一部分位于内核空间，其主要功能是进行实际的映射任务。模块通过一个 MISC 驱动进行实现，模块在 60 行调用 mtrr_add() 函数设置 \[MTRR_MEM_BASE, MTRR_MEM_BASE + MTRR_MEM_SIZE) 区域 Memory Type 为 WB. 模块提供了 mmap 接口 BiscuitOS_mmap(), 当用户空间基于 "/dev/BiscuitOS-CACHE" 节点调用 mmap() 函数时，BiscuitOS_mmap() 函数就会别调用。模块首先在 60 行调用 mtrr_add() 函数将 \[MTRR_MEM_BASE, MTRR_MEM_SIZE + MTRR_MEM_BASE) 区域的 MTRR 设置为 WB. 模块接着在 33 行在用户空间调用 mmap() 函数时从 vma 的 vm_pgoff 成员中获得 PAGE CACHE MODE 信息，接着在 36 行将 vma_page_prot 成员中移除 \_PAGE_PCD、\_PAGE_PWT 和 \_PAGE_PAT 属性，并在 39 行调用 cachemode2protval() 函数将 PAGE CACHE MODE 转换成对应的页表属性，并重新存储到 vma 的 vm_page_prot, 以此作为用于空间设置的 **memory type** 页表属性，最后模块在 41 行调用 remap_pfn_range() 函数为虚拟内存建立页表并映射到 MTRR_MEM_BASE 对应的物理内存上. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002539.png)

BiscuitOS 运行之后, 执行应用程序 BiscuitOS-CACHE-WC-MEM-USERSPACE-default，此时系统提示了程序预期将 \[0x10000000-0x10000fff] 为 write-combining，但是系统还是将对应的 **memory type** 设置为 write-back, 这个与预期不符合. 查看内核模块源码的 3 行提示需要将 \[0x10000000, 0x10200000) 进行预留，那么在 CMDLINE(CMDLINE 位于 RunBiscuitOS.sh 文件中) 中添加预留字段后再次实践:

![](/assets/PDB/HK/TH002587.png)

BiscuitOS 在次运行之后，运行应用程序，可以看到系统没有再提示修改信息了，那么用户进程已经成功将一段物理内存的 **memory type** 设置为 WC. 接着查看 "/proc/mtrr" 节点，可以看到 0x10000000 起始的 2MiB 区域都是 WB，实践符合预期. 通过上面的实践，PAT 核心设置发生在内核模块调用了 cachemode2protval() 函数，该函数的逻辑实现了页表 PAT、PCD、PWT 三个标志位依据 Memory Type 反向选择 PAT Entry，这是因为系统在初始化过程中，已经将三个标志位与 PAT Entry，以及 PAT Entry 内 Memory Type 的关系固化在 cachemode2protval() 函数逻辑了，开发者仅仅传入所需的 Memory Type 即可.

![](/assets/PDB/HK/TH002599.png)

cachemode2protval() 函数的逻辑如上，目前支持的 PAT ENTRY MODE 有 \_PAGE_CA-CHE_MODE_WB、\_PAGE_CACHE_MODE_WC、\_PAGE_CACHE_MODE_UC_MINUS、\_PAGE_CACHE_MODE_UC、\_PAGE_CACHE_MODE_WT 和 \_PAGE_CACHE_MODE_WP, 只需向函数传入这些值，函数就可以获得页表 PAT、PCD、PWT 三个标志位的集合, 程序接下来就是将三个标志位写入最终的页表即可。

###### PAT RB-TREEs/Page CACHE Mode

PAT 可以按线性地址粒度设置映射物理内存的 Memory Type，PAT 如何避免不同虚拟地址映射到同一物理内存时采用不同的 Memory Type? 针对 PAT 机制，内核将物理地址空间划分成了 MMIO 和普通内存进行不同的管理。对于 MMIO，PAT 采用了一颗区间树(特殊功能的红黑树)，将物理区域维护在区间树里，每个区间都有各自的 Memory Type。当新增一段 MMIO 的 Memory Type，PAT 机制首先在区间树中查找是否有重叠的区域，如果有重叠的区域，那么继续检查是否存在 Memory Type 不一致的情况，如果出现则报错，如果相同则调整区间的范围. 如果没有重叠的区域，那么在区间树中新增加一个区域; 对于普通内存，PAT 首先查看物理内存是否有对应的 struct page, 如果存在那么从 struct page 的 flags 字段中截取 memorytype 字段，该字段就是当前物理内存对应的 Memory Type，然后与 PAT 需要设置的 Memory Type 进行比对，如果相同则不做处理，反之如果不相同，那么系统将报错. 

![](/assets/PDB/HK/TH002547.png)

PAT 机制向用户空间提供了 "/sys/kernel/debug/x86/pat_memtype_list" 接口，该接口可以查看 MMIO 物理区域 Memory Type 的情况，例如在上图的案例里，运行 BiscuitOS_CACHE_WT_MMIO_USERSPACE_default 之后程序将映射 MMIO 的 Memory Type 为 WT(write-through), 可以查看该接口，看到 MMIO \[0xF0000000, 0xF0001000) 区域的 Memory Type 为 write-through.

![](/assets/PDB/HK/TH002549.png)

对于物理内存，当将物理内存设置为原先默认的 Memory Type 不同时，系统都会进行提示，例如上图的将物理内存的 Memory Type 有 WB 设置为 WT，内核就会弹出提示，并阻止了本次改动.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="B6"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000I.jpg)

##### Intel® CACHE Related Instruction

Intel® 64/IA-32 架构提供了一些列的指令管理 L1、L2、L3 CACHE. **INVD** 和 **WBINVD** 只有内核才可用使用，其作用与 L1、L2、L3 或者所有 CACHE. **PREFETCHh**、**CLFLUSH** 和 **CLFLUSHOP** 指令提供粒度的 CACHE 控制，用户空间进程和内核都可以使用。**MOVNTI、MOVNTQ、MOVNTDQ、MOVNTPS、MOVNTPD** 无时间跟踪的指令同样提供了更多粒度的 CACHE 控制，用户空间进程和内核同样都可以使用. 那么接下来分别对这些指令进行详细讲解:

> - [INVD Instruction](#B61)
>
> - [WBINVD Instruction](#B62)
>
> - [WBNOINVD Instruction](#B63)
>
> - [CLFLUSH Instruction](#B64)
>
> - [CLFLUSHOPT Instruction](#B65)
>
> - [CLWB Instruction](#B66)
>
> - [PREFETCHh Instruction](#B67)
>
> - [PREFETCHW Instruction](#B68)
>
> - [MOVNTI Instruction](#B69)

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

###### <span id="B61">INVD Instruction</span>

![](/assets/PDB/HK/TH002600.png)

**INVD** 指令将所有的 CACHE 标记为无效，但不会触发 Write-Back 操作，因此 Modify 状态的 CACHE Line 数据并不会更新到内存，导致数据未同步的数据丢失. **INVD** 指令只能在内核空间运行，该指令应用的场景是 CACHE 用于 CACHE Line 里面缓存了临时内存的内容，系统需要 Invalid CACHE Line 的内容而不是 Write-back 操作. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): INVD - Invalidate Internal CACHE  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-INVD-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-INVD-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-INVD)

![](/assets/PDB/HK/TH002601.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 20 行通过 \_\_get_free_page() 函数分配一个物理页，并将物理页对应的虚拟地址存储到 addr 变量里，接着在 27 行使用这段内存。模块在 30 行使用内嵌汇编的方式调用了 INVD 指令，并在 33 行再次使用这段内存，最后在 36 行调用 free_page() 函数释放这段内存. 调用 INVD 指令之后在次访问内存会比不调用 INVD 指令快. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002602.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-INVD-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-------------------------------------------

###### <span id="B62">WBINVD Instruction</span>

![](/assets/PDB/HK/TH002603.png)

**WBINVD** 指令先将所有 Modify 状态的 CACHE Line 更新到内存，然后将所有的 CACHE LINE 标记为无效. 由于 Write-Back 操作的存在，该指令会导致系统总线阻塞. 指令的执行无需等待 Write-back 和 Invalid 操作完成，硬件上会应答操作的完成. WBINVD 指令同样只运行在内核空间. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): WBINVD - WriteBack and Invalidate CACHE  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-WBINVD-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-WBINVD-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-WBINVD)

![](/assets/PDB/HK/TH002604.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 20 行通过 \_\_get_free_page() 函数分配一个物理页，并将物理页对应的虚拟地址存储到 addr 变量里，接着在 27 行使用这段内存。模块在 30 行调用 wbinvd_on_all_cpus() 函数，函数底层调用 WBINVD 指令，并在 34 行再次使用这段内存，最后在 37 行调用 free_page() 函数释放这段内存. 调用 WBINVD 指令会将已经修改的 CACHE Line 刷新到内存里. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002605.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-WBINVD-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-------------------------------------------

###### <span id="B63">WBNOINVD Instruction</span>

![](/assets/PDB/HK/TH002606.png)

**WBNOINVD** 指令将所有 Modify 状态的 CACHE Line 更新到内存，但不会使所有的 CACHE LINE 无效. 指令执行之后软件无需等待所有 Write-Back 操作完成，其他 CACHE Line 完成 Write-Back 操作之后会自动进行应答，以此硬件确认指令执行完成. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): WBNOINVD - WriteBack and Don't Invalidate CACHE  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-WBNOINVD-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-WBNOINVD-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-WBNOINVD)

![](/assets/PDB/HK/TH002608.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 20 行通过 \_\_get_free_page() 函数分配一个物理页，并将物理页对应的虚拟地址存储到 addr 变量里，接着在 27 行使用这段内存。模块在 30 行通过内嵌汇编的方式调用了 WBNOINVD 指令，并在 34 行再次使用这段内存，最后在 37 行调用 free_page() 函数释放这段内存. 调用 WBNOINVD 指令会将已经修改的 CACHE Line 刷新到内存里. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002609.png)

BiscuitOS 启动之后，加载 BiscuitOS_CACHE_INSTRUCT_WBNOINVD_default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-------------------------------------------

###### <span id="B64">CLFLUSH Instruction</span>

![](/assets/PDB/HK/TH002610.png)

**CLFLUSH** 指令基于虚拟地址, 如果 CACHE Line 的状态是 Modify，那么先执行 Write-Back 将 CACHE Line 内容刷新到内存里，然后将包含内容的 CACHE Line 全部置为 Invalid. **CLFLUSH** 指令一般用于准备读取新数据的场景. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): CLFLUSH - Flush CACHE Line  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-CLFLUSH-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-CLFLUSH-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-CLFLUSH)

![](/assets/PDB/HK/TH002611.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 25 行调用 alloc_page() 分配一个物理页，然后在 32 行调用 kmap_local_page() 函数将物理页临时映射到内核空间，并在 40 行使用内存，接着在 45-46 行按 CACHE Line Size 的粒度循环调用 clflush() 函数将虚拟地址对应的 CACHE Line 进行刷新. 最后模块在 49 行调用 kunmap_local() 函数解除了物理页的临时映射，并调用 \_\_free_page() 函数回收物理页. clflush() 函数最终调用到 CLFLUSH 指令并将修改的内容更新到内存，这在临时映射的场景有效果. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002612.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-CLFLUSH-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-----------------------------------------

###### <span id="B65">CLFLUSHOPT Instruction</span>

![](/assets/PDB/HK/TH002613.png)

**CLFLUSHOPT** 指令基于虚拟地址进行 CACHE Line 刷新, 如果 CACHE Line 的状态是 Modify，那么先执行 Write-Back 将 CACHE Line 内容刷新到内存里，然后将包含内容的 CACHE Line 全部置为 Invalid. **CLFLUSHOPT** 指令一般用于准备读取新数据的场景. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): CLFLUSHOPT - Flush CACHE Line Optimized  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-CLFLUSHOPT-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-CLFLUSHOPT-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-CLFLUSHOPT)

![](/assets/PDB/HK/TH002614.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 25 行调用 alloc_page() 分配一个物理页，然后在 32 行调用 kmap_local_page() 函数将物理页临时映射到内核空间，并在 40 行使用内存，接着在 45-46 行按 CACHE Line Size 的粒度循环调用 clflushopt() 函数将虚拟地址对应的 CACHE Line 进行刷新. 最后模块在 49 行调用 kunmap_local() 函数解除了物理页的临时映射，并调用 \_\_free_page() 函数回收物理页. clflushopt() 函数最终调用到 CLFLUSHOPT 指令并将修改的内容更新到内存，这在临时映射的场景有效果. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002615.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-CLFLUSHOPT-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-----------------------------------------

###### <span id="B66">CLWB Instruction</span>

![](/assets/PDB/HK/TH002616.png)

**CLWB** 指令基于虚拟地址进行 WriteBack 操作，将数据从 CACHE Line 中刷新到内存里, **WBNOINVD** 或者 **WBINVD** 指令都有 WriteBack 操作，但是针对所有的 CACHE，而 **CLWB** 可以按 CACHE Line 的粒度进行 WriteBack 操作. 该指令用于同步外设同步数据的场景。接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): CLWB - WriteBack CACHE Line  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-CLWB-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-CLWB-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-CLWB)

![](/assets/PDB/HK/TH002617.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 25 行调用 alloc_page() 分配一个物理页，然后在 32 行调用 kmap_local_page() 函数将物理页临时映射到内核空间，并在 40 行使用内存，接着在 45-46 行按 CACHE Line Size 的粒度循环调用 clwb() 函数将虚拟地址对应的 CACHE Line 进行 WriteBack 操作. 最后模块在 49 行调用 kunmap_local() 函数解除了物理页的临时映射，并调用 \_\_free_page() 函数回收物理页. clwb() 函数最终调用到 CLWB 指令并将修改的内容更新到内存，这在临时映射的场景有效果. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002618.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-CLWB-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-----------------------------------------

###### <span id="B67">PREFETCHh Instruction</span>

![](/assets/PDB/HK/TH002619.png)

**PREFETCHh** 指令用于将内存中的 Data Block 加载到 CACHE Line 中，有预取的作用，指令可以按不用的要求进行预期. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): PREFETCH - Prefetch Data into CACHE  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-PREFETCH-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-PREFETCH-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-PREFETCH)

![](/assets/PDB/HK/TH002620.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 25 行调用 \_\_get_free_page() 分配一个物理页并获得物理也对应的虚拟地址，然后在 29 行调用 prefetch() 函数将物理页预先加载到 CACHE Line 中，接着在 31-32 行使用内存，最后在 35 行调用 free_page() 函数释放物理页. prefetch() 函数最终会调用到 PREFETCH 指令, 该指令适用于需要加速读的场景. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002621.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-PREFETCH-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-----------------------------------------

###### <span id="B68">PREFETCHW Instruction</span>

![](/assets/PDB/HK/TH002622.png)

**PREFETCHW** 指令用于将内存中的 Data Block 加载到 L1/L2 CACHE Line 中，是该 CACHE Line 的其他数据无效. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): PREFETCHW - Prefetch Data into CACHE in Anticipation of a Write  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-PREFETCHW-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-PREFETCHW-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-PREFETCHW)

![](/assets/PDB/HK/TH002623.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 23 行调用 \_\_get_free_page() 分配一个物理页并获得物理也对应的虚拟地址，然后在 30 行调用 prefetchw() 函数将物理页预先加载到 CACHE Line 中，接着在 32-33 行使用内存，最后在 36 行调用 free_page() 函数释放物理页. prefetchw() 函数最终会调用到 PREFETCHW 指令, 该指令适用于需要加速读的场景. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002624.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-PREFETCHW-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

-----------------------------------------

###### <span id="B69">MOVNTI Instruction</span>

![](/assets/PDB/HK/TH002625.png)

**MOVNTI** 指令用于非时间限定最小化使用 CACHE 的方式使 CPU 写 DoubleWord 数据到内存，其实现基于 WriteCombining Memory Type，CPU 在写内存时可以尽可能推迟写操作. 接下来通过一个实践案例讲解如何使用该指令, 其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] CACHE Instruction(X86): MOVNTI - Store Doubleword Using No-Temporal Hint  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-INSTRUCT-MOVNTI-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-INSTRUCT-MOVNTI-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-INSTRUCT-MOVNTI)

![](/assets/PDB/HK/TH002626.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 23 行调用 \_\_get_free_page() 分配一个物理页并获得物理也对应的虚拟地址，然后在 28 行调用 kmalloc() 函数分配一段内存，接下来向物理页写入字符串。模块在 38 行调用 \_\_memory_flushcache() 函数在物理页和 mem 对应的内存之间拷贝数据，此时是向 mem 写入动作，拷贝完毕之后打印 mem 的内容，最后模块在 43-44 行调用 kfree() 和 free_page() 函数回收内存. 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002627.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-INSTRUCT-MOVNTI-default.ko 模块，可以看到内核成功打印了字符串，符合预期.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

#### Linux CACHE 机制

-------------------------------------------

<span id="C1"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000R.jpg)

##### CACHE on IOREMAP

![](/assets/PDB/HK/TH002628.png)

在 Linux 中物理地址地址空间主要由物理内存和 MMIO，其中 MMIO 是外设的 IO 映射到地址空间形成的，可以通过 "/proc/iomem" 节点查看系统物理地址空间。在 X86 架构中，外设的 IO 既可以映射到物理地址空间，然后像普通内存一样进行访问，同样外设也可以将 IO 映射到 IO 空间，然后使用 IN/OUT 系列指令进行访问，在 X86 IO 空间和物理地址空间是两个独立的空间. 外设可以根据自己的需求在硬件设计的时候规划 IO 映射到哪个空间. 例如 PCI/PCIe 设备可以将其 BAR 空间映射到 IO 空间形成 IO 端口，也可以映射到物理地址空间形成 MMIO. 

![](/assets/PDB/HK/TH002629.png)

当外设 IO 以 MMIO 方式映射到物理地址空间，用户进程或者内核线程需要访问 MMIO 时，需要将各自空间的虚拟地址通过页表映射到 MMIO 上，映射完毕之后可以通过访问虚拟地址进而访问 MMIO. Linux 为用户空间或者内核空间提供不同的机制来完成映射，其中对于内核空间，Linux 提供了 IOREMAP 机制，该机制可以映射一段 MMIO 并返回建立映射的虚拟地址, 接下来通过一个实践案例先认识一下 IOREMAP，其在 BiscuitOS 上的部署逻辑如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] IOREMAP: CACHE MODE with IOREMAP Mechanism  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-IOREMAP-default/
# 部署源码
make download
# 在 Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-IOREMAP-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-IOREMAP)

![](/assets/PDB/HK/TH002630.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 20-25 行描述了 MMIO 资源信息，然后在 33-35 行通过调用 request_resource() 函数将该 MMIO 资源注册到系统物理空间资源树，接着模块在 37 行调用 ioremap() 函数将 BROILER_MMIO_BASE 起始长度为 BROILER_MMIO_LEN 的 MMIO 区域映射到内核空间，并将映射之后的虚拟地址存储在 mmio 变量里，接下来就是在 43-50 行对 MMIO 进行访问。由于需要外设才能完成实践，因此可以使用 Broiler 进行实践，直接执行命令 "make broiler"，其在 Broiler 上的实践如下:

![](/assets/PDB/HK/TH002631.png)

BiscuitOS 启动之后，加载 BiscuitOS-CACHE-IOREMAP-default.ko 模块，可以看到系统可以访问 MMIO，可以看到对应的虚拟地址和物理地址，并看到 MMIO 写入然后读出的值 0x88520. 接着查看 pat_memtype_list 节点，可以看到 \[0xF0000000, 0xF0001000) 区域映射为 uncached-minus， 这里的 uncached-minus 指的是什么呢? 通过对前面的学习，可以知道这是映射 MMIO 采用的 Memory Type，也就是 CACHE Mode，此时映射 MMIO 采用了 UC 的模式，即映射时不采用任何 CACHE，CPU 直接对 MMIO 进行访问, 那么 MMIO 的 CACHE MODE 如何规划的？另外 IOREMAP 机制是否可以通过其他 CACHE MODE? 接下来本节进一步分析这些问题.

![](/assets/PDB/HK/TH002632.png)

当内核空间将虚拟地址通过页表映射到 MMIO 区域，那么 CPU 可以通过访问虚拟地址就可以访问到 MMIO，与普通内存一样，依据映射的 Memory Type 不同，CPU 访问虚拟地址时不一定能访问到真实的 MMIO，因为 CPU 和物理地址空间之间还存在 CACHE。假设映射 MMIO 时 Memory Type 采用了 Writeback，当 CPU 向 MMIO 写操作时，所写的数据不是立即写入到 MMIO 的，而是写入到 CACHE 里，只有当 CPU 对 MMIO 发起读或者 CACHE Line 收到 Writeback 信号，那么 CACHE 中的数据才会被真正写到 MMIO 中，如果所写的数据一直在 CACHE 中，此时设备对 MMIO 中的数据进行读时，此时会出现 Device 看到的时旧数据，而最新的数据还在 CACHE 里，因此对应 MMIO 的映射需要选择正确的 Memory Type.

![](/assets/PDB/HK/TH002634.png)

如果设备有 Snoop 总线的能力，即可以检测到 CPU 对内存发起的读写请求，那么当系统将内核空间虚拟地址映射 MMIO 的 Memory Type 设置为 Write Back 时，当 CPU 对 MMIO 进行写操作时，所写的数据会被缓存在 CACHE 里而不被写到 MMIO，此时如果设备需要读取 MMIO 中的值，由于其具有 snoop 能力，那么其可以知道 CACHE 中缓存了数据，此时向总线上发起 Write-back 和 Invalid 信息，那么设备就可以从 MMIO 中读取最新的值，另外由于 Invalid 信息，当 CPU 再次读对应的数据时，其会从 MMIO 中加载最新的数据.

![](/assets/PDB/HK/TH002633.png)

如果设备没有 Snoop 总线的能力，那么设备无法感知 CPU 什么时候访问了 MMIO，但此时可以将内核空间虚拟地址映射 MMIO 的 Memory Type 设置为 Write-Through, 那么 CPU 对 MMIO 发起写操作的时候，所写的数据缓存在 CACHE 的同时也会被写如到 MMIO 里，那么设备一定可以读取到 MMIO 最新值，但会存在一个问题，当设备更新了 MMIO 的数据，之后 CPU 发起 MMIO 读请求时，由于 CACHE Line 中存有数据并 CACHE hit，CPU 并不会到 MMIO 上读取真正的数据，因此造成数据差异。但在有的场景可以采用 Write-through Memory Type，CPU 进行大量的写操作，读请求的概率特别小，当读之前可以先 Invalid CACHE Line，以此获得最新的数据.

![](/assets/PDB/HK/TH002635.png)

如果设备没有 Snoop 总线的能力，那么设备无法感知 CPU 什么时候访问了 MMIO，那么最好的方案就是将内核空间虚拟地址映射 MMIO 的 Memory Type 设置为 Uncached 或者 Strong Uncached，那么 CPU 访问 MMIO 不会被 CACHE 缓存，CPU 直接写数据到 MMIO 和直接从 MMIO 读数据，那么外设看到的 MMIO 数据与 CPU 看到的一致.

![](/assets/PDB/HK/TH002636.png)

通过上面的分析，内核空间虚拟地址映射 MMIO 的 Memory Type 可以根据场景不同选择 WB、WC、WT 和 UC/UC-, IOREMAP 机制作为映射 MMIO 的通用接口，其也提供多种满足不同的 Memory Type 的接口，例如映射 WB 类型 MMIO 的 ioremap_cache() 函数, 整理了 IOREMAP 机制映射不同 Memory Type 的实践案例，具体见:

> - [IOREMAP with Strong Uncacheable (UC)](#B21A)
>
> - [IOREMAP with Uncacheable (UC-)](#B26A)
>
> - [IOREMAP with Write Combining (WC)](#B22A)
>
> - [IOREMAP with Write-Through (WT)](#B23A)

###### IOREMAP 映射过程

![](/assets/PDB/HK/TH002637.png)

ioremap() 函数的主题流程如上图，其他类似函数的流程差异在于 enum page_cache_mode 的选择，例如选择映射 WB 的 MMIO，那么这里会选择 \_PAGE_CACHE_MODE_WB. 接下来的逻辑都是一致的，IOREMAP 机制会通过调用 memtype_reserve() 函数将 MMIO 区域的 Memory Type 维护在一颗区间树里面，通过函数 pat_x_mtrr_type() 或者有效的 Memory Type，然后调用 memtype_check_insert() 函数检查是否与其他 MMIO 区域重叠，如果重叠，那么采用重叠的 Memory Type，最后将 MMIO 区域插入到区间树 memtype_rbroot。IOREMAP 机制根据 Memory Type 选择不同的 PAT 页表属性，并调用 get_vm_area_caller() 从 VMALLOC 区域中获得一段可用的虚拟内存，接下来调用 memtype_kernel_map_sync() 函数同步 Memory Type，但对 MMIO 不进行同步，最后调用 ioremap_page_range() 函数建立虚拟地址到 MMIO 的映射，并采用指定的 Memory Type. 接下来通过一个案例讲解 IOREMAP 如何修改 Memory Type，其在 BiscuitOS 中的部署如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support BiscuitOS Hardware Emulate
      [*] CACHE  --->
          [*] IOREMAP: Change CACHE Mode with IOREMAP Mechanism  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-IOREMAP-CHANGE-MEMTYPE-default/
# 部署源码
make download
# 在 Broiler 中实践
make broiler
{% endhighlight %}

> [BiscuitOS-CACHE-IOREMAP-CHANGE-MEMTYPE-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-IOREMAP-CHANGE-MEMTYPE)

![](/assets/PDB/HK/TH002638.png)

源码很精简，通过一个内核模块进行讲解，模块首先在 20-25 行描述了 MMIO 资源信息，然后在 33-35 行通过调用 request_resource() 函数将该 MMIO 资源注册到系统物理空间资源树，接着模块在 35 行调用 ioremap_wt() 函数将 BROILER_MMIO_BASE 起始长度为 BROILER_MMIO_LEN 的 MMIO 区域映射到内核空间，并且 Memory Type 为 WT，并将映射之后的虚拟地址存储在 mmio 变量里，接下来就是在 42 行调用 ioremap_uc() 函数改变 MMIO Memory Type 为 UC。由于需要外设才能完成实践，因此可以使用 Broiler 进行实践，直接执行命令 "make broiler"，其在 Broiler 上的实践如下:

![](/assets/PDB/HK/TH002639.png)

当 BiscuitOS 运行之后，加载 BiscuitOS-CACHE-IOREMAP-CHANGE-MEMTYPE-default.ko 模块，接下来查看 pat_memtype_list 节点，以此查看系统 MMIO 区域采用的 Memory Type，此时看到 \[0xF0000000, 0xF0004000) 区域 Memory Type 为 write-through, 但 \[0xF0000000, 0xF0001000) 区域 Memory Type 为 write-through, 此时 IOREMAP 并没有修改 Memory Type 成功，从另外一个层面思考，如果一个物理地址被多个虚拟地址映射，如果每个都映射不同的 Memory Type，这样会引起混乱，因此 IOREMAP 在这种场景下的做法如下:

![](/assets/PDB/HK/TH002640.png)

IOREMAP 在 85 行调用 interval_iter_first() 函数在 memtype_rbroot 区间树内查找包含 \[start, end) 的区间，当找到之后获取该区间的 Memory Type，该场景下新的 Memory Type 和当前 Memory Type 不相等，且新的 Memory Type 不为空，那么 IOREMAP 的做法是将新修改 MMIO 区域的 Memory Type 覆盖成当前 Memory Type，以此保证不同虚拟地址映射同一个 MMIO 时 Memory Type 是一致的. IOREMAP 在 95 行调用 interval_iter_next() 函数将新增加的区域插入到区间树内. 最后就看到实践过程中看到的新增的区域为独立区域，但 Memory Type 与当前一致.

![](/assets/PDB/HK/TH002641.png)

当 IOREMAP 机制解除虚拟地址对 MMIO 的映射时，需要将映射 Memory Type 一同解除，其代码流程如上图，IOREMAP 首先调用 find_vma_area() 函数找到对应的 vm_struct 数据结构，然后获得对应的物理地址，接下来通过调用 memtype_free() 函数移除 MMIO 区域的 Memory Type，最后调用 remove_vm_area() 函数是否虚拟内存. 

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

--------------------------------------

<span id="C2"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000E.jpg)

##### CACHE on RAM

![](/assets/PDB/HK/TH002394.png)

在 Linux 中，物理地址空间主要由 RAM 和外设映射的 MMIO 组成，Linux 将可以管理的 RAM 称为系统物理内存，物理内存的大小直接影响系统的效率和稳定性。RAM 通常由内存条组成，内存条根据硬件设计将内部的存储空间映射到物理地址空间，因此会在 Linux 看到内存条是连续的空间，但在物理地址空间却被换分成不连续的多块区域.

![](/assets/PDB/HK/TH002642.png)

用户空间的进程或者内核线程想要访问物理内存，那么需要将其虚拟地址映射与物理内存建立页表，页表建立完毕之后才能访问到物理内存。当虚拟地址映射到物理内存之后，其可以使用不同的 Memory Type 满足不同的 CACHE Mode，不同的 CACHE Mode 会对程序执行效率有很大的影响，开发者因根据场景的需求选择正确的 CACHE Mode，本节用于讲解虚拟内存映射物理内存时如何配置所需的 Memory Type. 首先通过一个实践案例介绍 **Linux 如何对内核空间虚拟地址映射的物理内存配置 CACHE Mode**, 其在 BiscuitOS 配置如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] WriteThrough(WT): Mapping WT Memory on Kernel  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-WT-MEM-KERNEL-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-WT-MEM-KERNEL-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-WT-MEM-KERNEL)

![](/assets/PDB/HK/TH002551.png)

实践案例主要目的是在内核空间申请一段内存之后，将对应的虚拟地址以 WT 的方式映射到物理内存，并进行访问。案例在 21-25 行通过 \_\_get_free_page() 函数分配一个物理页并获得对应的虚拟地址，然后在 28 行调用 \_set_memory_wt() 函数进行 WT 方式的映射，案例接着在 30-33 行对映射之后的虚拟地址进行访问。最后在 36-37 行回收设置，值得注意的是当将内核空间虚拟地址设置为非 WB 之后，在回收时要主动设置为 WB, 接下来在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002552.png)

BiscuitOS 系统启动过程中，可以看到 dmesg 打印字符串 "Hello BiscuitOS", 以及相应的虚拟地址。通过这个案例可以看到 Linux 如何为内核空间虚拟地址设置 CACHE Mode，核心函数是 \_set_memory_wt() 函数。Linux 提供了一些列的函数用于内核空间虚拟地址设置不同的 CACHE Mode.

![](/assets/PDB/HK/TH002643.png)

Linux 提供了 set_memory_X() 系列函数为内核虚拟地址映射物理地址时，提供丰富接口实现 CACHE Mode 的选择，例如 UC、WT、WC、WB 等，更多详细的实践案例可以参考如下:

> - [RAM with Strong Uncacheable (UC)](#B21D)
>
> - [RAM with Uncacheable (UC-)](#B26D)
>
> - [RAM with Write Combining (WC)](#B22D)
>
> - [RAM with Write-Through (WT)](#B23D)
>
> - [RAM with Write-Back (WB)](#B24D)

###### 内核虚拟地址映射过程

![](/assets/PDB/HK/TH002644.png)

内核虚拟地址设置 Memory Type 有两种方式，第一种是在建立线性映射的时候，内核统配置为 WB; 第二种是通过 set_memory_X() 系列函数修改已经建立映射的内存，这里重点介绍第二种，上图是该系列的通用流程，核心分作三部分，第一部分是将 Memory Type 记录到 struct page，第二部分是修改页表，第三部是刷新虚拟内存对应的 CACHE Line. memory_reserve() 函数根据不同的 enum page_cache_mode 计算出页表属性，然后在 reserve_ram_pages_type() 函数找到物理地址对应的 struct page, 然后将 Memory Type 信息存储在 struct page 的 flags 成员里，具体可以从 get_page_memtype()/set_page_memtype() 函数得知具体的逻辑. 接下来通过调用 \_\_change_page_attr_set_clr() 函数修改了对应页表的属性，最后调用 cpa_flush() 函数，其最终会调用 clflushopt() 函数，其在 X86 架构里会调用 CLFLUSHOPT 指令针对虚拟地址刷新 CACHE Line.

![](/assets/PDB/HK/TH002645.png)

set_page_memtype()/get_page_memtype() 函数可以看到 RAM 的 Memory Type 存储在 struct page 的 flags 成员里. 从 124-129 行定了不同 Memory Type 在 struct page flags 成员的位置，例如 UC- 位于 PG_uncached 位置, 因此可以利用 PageUncache() 函数判断一个物理页对应的 CACHE Mode 是否为 UC-. 另外从宏的定义来看，目前 Linux 可以动态修改 RAM CACHE Mode 只包括 WB、WT、WC 和 UC- 四种类型，其他类型都无法动态修改.

![](/assets/PDB/HK/TH002646.png)

\_\_change_page_attr() 函数在 set_memory_X() 函数流程里的主要目的就是修改页表，函数根据页表的粒度，如果是 4KiB 页，那么直接在 526-557 的分支直接修改 PTE 页表的属性，譬如 552 行直接调用 set_pte_atomic() 函数直接原子修改页表. 另外如果页表的粒度是大页，那么在 563 行调用 should_split_large_page() 函数判断是否对大页进行页表修改，如果是调用 split_large_page() 函数，如果只是修改了大页中的一个页或几个页，那么需要将大页拆分成多个小页，然后将需要修改的物理页更新其 CACHE Mode.

![](/assets/PDB/HK/TH002647.png)

修改 CACHE MODE 最后一步就是刷新已经修改物理页对应的 CACHE Line 和 TLB，函数在 375 行进行判断之后可以对单个 CPU 刷 TLB，也可以对所有 CPU 刷 TLB，刷完 TLB 之后继续刷 CACHE Line，由于虚拟地址都是内核空间虚拟地址，且都是线性映射区的虚拟地址，因此只会出现一个虚拟地址对应一个物理地址的情况，因此直接找到对应的页表，如果页表存在，那么调用 clflush_cache_range_opt() 函数将虚拟地址对应的 CACHE Line 全部刷掉. 通过上面的分析，set_memory_X() 系列提供的接口具有限制性功能:

* 只能针对内核线性映射区的虚拟地址，其他 VMALLOC 等区域的虚拟地址不能使用
* 接口只能动态修改 WB、WC、WT 和 UC- 四种类型的 CACHE MODE
* RAM 的 CACHE Mode 存储在 struct page 的 flags 里

###### CACHE Mode on 用户空间虚拟地址

![](/assets/PDB/HK/TH001461.png)

用户空间进程的地址空间被换分成很多个区域，每个区域完成不同的任务，其中 MMAP 区域可以将用户空间虚拟内存映射到物理地址空间上，因此在映射过程中也可以配置 CACHE MODE。在前面分析过，内核空间只有线性映射区的物理内存可以修改 CACHE MODE，这是要确保同一个物理内存只有一种 CACHE MODE，因此在用户空间同样也要保证被映射的物理内存只有一种 CACHE Mode. 接下来通过一个实践案例介绍 **Linux 如何对用户空间虚拟地址映射的物理内存配置 CACHE Mode**，其在 BiscuitOS 配置如下:

{% highlight bash %}
cd BiscuitOS
make menuconfig

  [*] Package  --->
      [*] BiscuitOS Architecture Specify Setup  --->
          [*] Support Host CPU Feature Passthrough
      [*] CACHE  --->
          [*] RAM: CACHE MODE with RAM (Recommend)  --->

# 进入源码目录
cd BiscuitOS/output/linux-X.Y.Z-ARCH/package/BiscuitOS-CACHE-MEMORY-default/
# 部署源码
make download
# 在 BiscuitOS 中实践
make build
{% endhighlight %}

> [BiscuitOS-CACHE-MEMORY-default Source Code on Gitee](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/CACHE-MECH/BiscuitOS-CACHE-MEMORY)

![](/assets/PDB/HK/TH002648.png)

案例由两部分组成，一部分位于内核模块组成，另外一部分由用户空间进程组成。上图为内核模块部分，模块提供了 MSIC 字符设备框架，并向用户空间提供了 mmap 接口，用户空间打开 '/dev/BiscuitOS-CACHE-MEM' 之后，调用 mmap() 函数就能调用到 BiscuitOS_mmap() 函数。在 BiscuitOS_mmap() 函数里，模块在 36 行首先移除配置的页表属性的 PAT 标志位，然后从 vma 的 vm_pgoff 中解析出 enum page_cache_mode 信息，并传入 cachemode2protval() 获得 CACHE Mode 对应的页表 PAT 标志集合，最后在 41 行调用 remap_pfn_range() 函数建立页表映射. 最后值得注意的是 3 行注释部分提示需要将 \[0x10000000, 0x10200000) 区域作为系统预留内存加入到 CMDLINE 里.

![](/assets/PDB/HK/TH002649.png)

案例的另外一部分是由用户空间进行组成的，程序首先在 34 行配置了 CACHE MODE，目前支持的 CACHE MODE 有 WC、WT、WB、WP、UC 和 UC-, 程序接着在 39 行通过 open() 函数打开 '/dev/BiscuitOS-CACHE-MEM' 节点，接着在 46-54 行通过 mmap() 函数映射一段物理内存，并将 CACHE Mode 通过 pgoff 参数传入到内核。映射完毕之后在 57-58 行使用内存，最后在 60-61 行解除映射和关闭节点. 接下来就是在 BiscuitOS 上实践该案例:

![](/assets/PDB/HK/TH002650.png)

BiscuitOS 系统启动之后，加载 BiscuitOS-CACHE-MEMORY-default.ko, 接着运行 APP 应用程序，此时可以看到系统提示 **x86/PAT: APP:135 map pfn RAM range req uncached for [mem 0x10000000-0x10000fff], got write-back**, 意思就是 APP 向将映射的 CACHE Mode 设置为 Uncached，当系统不允许并将 CACHE Mode 保持为 WB. 这是为什么? 看到这里想起还忘记做一件事，那么就是在 CMDLINE 里添加一段系统预留内存，那么在 RunBiscuitOS.sh 文件中进行添加，然后再次实践:

![](/assets/PDB/HK/TH002651.png)

BiscuitOS 再次启动，查看 '/proc/cmdline' 节点确认已经添加预留字段，接下来重复之前动作，可以看到 APP 可以使用内存，并没有出现之前的提示，因此可以确认 APP 映射的物理内存 CACHE Mode 修改有效。那么思考一个问题，用户空间映射的系统管理的物理内存不能修改其 CACHE Mode？ 针对这个问题进一步分析.





















