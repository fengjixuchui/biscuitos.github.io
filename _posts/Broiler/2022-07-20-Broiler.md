---
layout: post
title:  "Broiler Virtualization Technology"
date:   2022-07-20 12:00:00 +0800
categories: [MMU]
excerpt: Virtualization.
tags:
  - HypV
---

![](/assets/PDB/BiscuitOS/kernel/IND00000L0.PNG)

![](/assets/PDB/RPI/RPI100100.png)

#### 目录

> - [Broiler 项目简介](#A)
>
> - [Broiler 使用攻略](#B)
>
> - [Broiler 内存虚拟化](#C)
>
> - [Broiler PIO/MMIO 虚拟化](/blog/Broiler-vIO/)
> 
> - [Broiler BIOS](#D)
>
> - [Broiler 中断虚拟化](/blog/Broiler-vInterrupt/)
>
> - [Broiler 设备虚拟化](#K)
>
>   - [Broiler Virtio](#F2)

######  🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂 捐赠一下吧 🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂🙂

![BiscuitOS](/assets/PDB/BiscuitOS/kernel/HAB000036.jpg)

-------------------------------------------

<span id="A"></span>

![](/assets/PDB/HK/TH001699.JPEG)

#### Broiler 项目简介

![](/assets/PDB/HK/TH001699.png)

Broiler 是一个 X86 CPU 模拟器开源项目(魔改自 kvm-tools 项目)，皆在为开发者提供一套快速编译、架构简洁、运行高效的虚拟机实践方案。传统的 BiscuitOS 项目是基于 QEMU/KVM 组合构建的 Linux 发行版系统，新架构下的 Broiler 与 KVM 配合提供了 BiscuitOS 运行的虚拟化底座。Broiler 项目包含了一个精简的 BIOS、一套自定义的内存布局以及丰富的自定义外设，并通过 virtio 协议提供了系统使用的系统盘等。Broiler 项目还提供了多个富有实践意义的虚拟化专题文档，致力于为开发者提供一套极简但高效的实践环境.

> [Broiler Github Repositories 链接](https://github.com/BiscuitOS/Broiler) 

![](/assets/PDB/HK/TH001700.png)

上图是 Broiler 项目使用 Intel VT-x 虚拟化技术模拟出来的 Broiler 定制 CPU，其 vendor_id 为 BiscuitOS，Model name 为 "Broiler@16th Spe Intel(R) @ 5.50Hz". Broiler 可支持的带 virtio 协议的 Linux 版本，目前不支持 AMD 的 CPU。开发者可以参考[Broiler 使用攻略](#B)章节开始实践 Broiler.

![](/assets/PDB/HK/TH001701.png)

Broiler 支持的物理地址总线域由 PCI 总线域、 DDR 域以及外设映射区域构成，Broiler 还支持 X86 的 IO 空间。Broiler 支持最小 256MiB 的 DDR，DDR 在物理地址总线低端的映射范围为 \[0x00000000, 0xC0000000), 高端的映射范围从 0x100000000 开始。Broiler 支持一条 PCI 总线，PCI 总线域映射的 MMIO 从 \[0xC0000000, 0x100000000), 这段区域也包括一些设备的映射区域，例如 Local APIC 的映射区域，这里都作为 MMIO。Broiler 支持的 IO 空间如上图，包括了 PCI Config 的 DATA 和 ADDR PORT，以及一些常见的外设端口. 更多细节描述可以参考[Broiler 内存管理](#C)章节.

![](/assets/PDB/HK/TH001784.png)

Broiler 支持多种中断的注入以及虚拟化，通过与多种模拟设备在不同场景下进行中断虚拟化的实践，让开发者切身体会一个中断是如何虚拟化的。中断虚拟化专题从原理、虚拟化、使用和实践角度介绍了 vPIC、vIOAPIC、vLAPIC、MSI、MSIX 以及 IPI 中断，细节文档请参考[中断虚拟化](/blog/Broiler-vInterrupt/)章节.

![](/assets/PDB/HK/TH001481.png)

PCI 设备、普通外设在 X86 架构是不可缺少的部分，每个 X86 系统都会维护 PCI 总线，PCI 总线上连接的大量的 PCI 设备，另外南桥上通过低速总线连接了很多普通外设。CPU 通过与外设的寄存器进行交互实现了与外设通信，外设可以通过将其内部寄存器映射到 X86 的 IO Space 或者存储域里，以便 CPU 更加便捷访问外设。虚拟化层面也提供了对 MMIO 和 PIO 的支持，以实现虚拟机也可以访问模拟设备的寄存器，因此 PIO/MMIO 虚拟化是不可缺少的一部分，开发者可以参考[PIO/MMIO 虚拟化](/blog/Broiler-vIO/)章节.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="B"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000Q.jpg)

#### Broiler 使用攻略

![](/assets/PDB/HK/TH001702.png)

Broiler 项目已经适配多种场景，开发者可以在一台 Linux 发行版上直接部署使用 Broiler 项目，也可以基于 BiscuitOS 项目在 BiscuitOS 内部运行 Broiler, 甚至可以 Broiler 嵌套 Broiler 方式使用。开发者可以根据自身需求进行使用，默认推荐基于 BiscuitOS 进行 Broiler 的构建。具体参考如下:

> - [PlanA: Linux 发行版直接部署 Broiler 项目](#B1)
>
> - [PlanB: 基于 BiscuitOS 构建 Broiler 项目](#B2)
>
> - [PlanC: 构建基于 Broiler 项目的 BiscuitOS](#B3)
>
> - [PlanD: 部署 Broiler 嵌套 Broiler 项目(功能未开放)](#B4)

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="B1"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000F.jpg)

#### Linux 发行版直接部署 Broiler 项目

![](/assets/PDB/HK/TH001703.png)

Broiler 项目支持在 Linux 发行版上直接部署和运行 Linux 发行版，开发者可以自行准备带有 virtio 协议的 Linux 发行版镜像，也可以使用 Broiler 提供的 BiscuitOS 发行版. Broiler 项目基于 Intel 的硬件进行构建，因此开发者需要确保其处理器是 Intel 的处理器，另外需要开发者将处理器的 VT-x 和 VT-d 功能打开，具体打开方法可以参考网上，另外开发者需要准备 Linux 发行版，默认推荐 Ubuntu20.04 或者 Ubuntu 18.04，Linux 发行版上需要提前安装 kvm 模块。该方案开发者只能源码调试 Broiler，对于内核、KVM 无法进行源码调试。在本方案中，使用 Broiler 项目在 Linux 发行版上直接运行一个虚拟机。本节以 Ubuntu20.04 和 BiscuitOS 发行版为例进行说明，开发者可以参考如下步骤进行部署:

{% highlight bash %}
# 确认系统已经打开 VT-x
lscpu | grep Virtualization

# 安装 kvm 模块
sudo apt install -y qemu-kvm

# 下载 Broiler 源码
git clone https://github.com/BiscuitOS/Broiler.git
cd Broiler/
{% endhighlight %}

![](/assets/PDB/HK/TH001704.png)

Broiler 源码如上图，其中 bios 是 BIOS 相关的源码; broiler 目录下 Broiler 虚拟化技术的核心，用于虚拟 CPU 各种组件; hw 目录下提供了 Broiler 支持的设备模拟 Demo; include 目录下包括各类型的头文件; lib 目录下提供了 Broiler 使用的各类库函数; virtio 目录下包括了 virtio 协议的核心代码，以及一些 virtio 设备后端的实现. main.c 文件为 Broiler 的入口函数，Makefile 为编译项目的配置文件，RunBroiler.sh 用于启动 Broiler 的快速脚本，COPYING 为 Broiler 使用的 GPL 说明文件，README.md 为项目的简易说明文档. 接下来编译 Broiler 项目:

{% highlight bash %}
# 清除上次编译残留
make clean
# 编译 Broiler
make
{% endhighlight %}

编译完成之后会在当前目录下生成 BiscuitOS_Broiler 可执行文件，那么接下来对启动参数进行说明:

{% highlight bash %}
./BiscuitOS-Broiler --kernel linux-5.10-bzImage \
		    --rootfs BiscuitOS.img \
		    --memory 256 \
		    --cpu 2 \
		    --cmdline "noapic noacpi pci=conf1 reboot=k panic=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 root=/dev/vda rw rootfstype=ext4 console=ttyS0 loglevel=8"
{% endhighlight %}

"--kernel" 参数指向内核镜像所在的路径，"--rootfs" 参数指向了 ROOTFS 所在的路径，"--memory" 参数指明 Broiler 启动的虚拟机内存大小，单位为 MiB，"--cpu" 参数指明 Broiler 启动虚拟机的 CPU 数量, "--cmdline" 参数指明 Broiler 启动的虚拟机内核使用的 cmdline. 开发者可以自定义修改这些参数，默认使用的 kernel 和 rootfs 可以通过如下命令进行下载:

{% highlight bash %}
# Download rootfs and Image
git clone https://gitee.com/BiscuitOS_team/broiler-image-rep.git
cd broiler-image-rep/

# 使用镜像
cd Broiler/
./BiscuitOS-Broiler --kernel */broiler-image-rep/linux-5.10-bzImage \
                    --rootfs */broiler-image-rep/BiscuitOS.img \
                    --memory 256 \
                    --cpu 2 \
                    --cmdline "noapic noacpi pci=conf1 reboot=k panic=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 root=/dev/vda rw rootfstype=ext4 console=ttyS0 loglevel=8"
{% endhighlight %}

![](/assets/PDB/HK/TH001700.png)

上图便是成功使用 Broiler 启动了一个虚拟机，接下来开发者可以使用 Broiler 源码研究其实现，也可以将内核镜像和 ROOTFS 替换成自定义的镜像，更多的可能留给开发者自己.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="B2"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000T.jpg)

#### 基于 BiscuitOS 构建 Broiler 项目

![](/assets/PDB/HK/TH001705.png)

Broiler 项目支持在 BiscuitOS 上部署实践，其原理是现在 Linux 发行版上启动一个 BiscuitOS 虚拟机，然后再在内部启动 Broiler，这样做的优势是隔离性比较好，其次是这个方案可以对内核、KVM 和 Broiler 源码都进行修改调试。该方案同理也需要准备一台已经打开 VT-x/VT-d 的 Linux 发行版，并且已经安装 KVM 模块。接着需要部署 BiscuitOS，然后在 BiscuitOS 开发环境中部署 Broiler，看着过程很负责，其实推荐开发者使用这个方案，方案一定部署会因 BiscuitOS 带来很多开发效益的提升。开发者可以参考如下步骤进行部署使用, 首先部署一个 Linux 5.10 的环境:

> [BiscuitOS Linux 5.10 X86 开发环境部署](/blog/Linux-5.x-x86_64-Usermanual/)

部署完 BiscuitOS Linux 5.10 X86 开发环境之后，接下来部署 Broiler 源码，使用如下命令:

{% highlight bash %}
# 切换到 BiscuitOS 目录
cd BiscuitOS
make linux-5.10-x86_64_defconfig
make menuconfig

  [*] Package --->
      [*] KVM --->
          [*] BiscuitOS hyperv: Broiler

# 保存配置并使配置生效
make

# 进入 Broiler 目录
cd output/linux-5.10-x86_64/package/BiscuitOS-Broiler-default/
{% endhighlight %}

![](/assets/PDB/HK/TH001706.png)
![](/assets/PDB/HK/TH001707.png)
![](/assets/PDB/HK/TH001708.png)
![](/assets/PDB/HK/TH001704.png)

Broiler 源码如上图，其中 bios 是 BIOS 相关的源码; broiler 目录下 Broiler 虚拟化技术的核心，用于虚拟 CPU 各种组件; hw 目录下提供了 Broiler 支持的设备模拟 Demo; include 目录下包括各类型的头文件; lib 目录下提供了 Broiler 使用的各类库函数; virtio 目录下包括了 virtio 协议的核心代码，以及一些 virtio 设备后端的实现. main.c 文件为 Broiler 的入口函数，Makefile 为编译项目的配置文件，RunBroiler.sh 用于启动 Broiler 的快速脚本，COPYING 为 Broiler 使用的 GPL 说明文件，README.md 为项目的简易说明文档. 接下来编译 Broiler 项目:

{% highlight bash %}
# 清除上次编译残留
make clean
# 编译 Broiler
make
# 更新 ROOTFS
make install
make pack
{% endhighlight %}

编译完成之后会在当前目录下生成 BiscuitOS-Broiler-default 可执行文件，Broiler 运行的参数放在 RunBroiler.sh 文件里，其内容如下:

{% highlight bash %}
#!/bin/ash
# SPDX-License-Identifier: GPL-2.0-only

BiscuitOS-Broiler-default --kernel /mnt/Freeze/BiscuitOS-Broiler/bzImage \
                          --rootfs /mnt/Freeze/BiscuitOS-Broiler/BiscuitOS.img \
                          --memory 256 \
                          --cpu 2 \
                          --cmdline "noapic noacpi pci=conf1 reboot=k panic=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 root=/dev/vda rw rootfstype=ext4 console=ttyS0 loglevel=8"
{% endhighlight %}

"--kernel" 参数指向内核镜像所在的路径，"--rootfs" 参数指向了 ROOTFS 所在的路径，"--memory" 参数指明 Broiler 启动的虚拟机内存大小，单位为 MiB，"--cpu" 参数指明 Broiler 启动虚拟机的 CPU 数量, "--cmdline" 参数指明 Broiler 启动的虚拟机内核使用的 cmdline. 开发者可以自定义修改这些参数, 另外 Broiler 启动虚拟机使用的内核和 ROOTFS 已经打包到 BiscuitOS 的 ROOTFS 中，接下来直接使用如下命令进行实践:

{% highlight bash %}
# 运行 BiscuitOS
make run

# 进入 BiscuitOS 之后启动 Broiler
RunBroiler
{% endhighlight %}

![](/assets/PDB/HK/TH001700.png)

上图便是成功使用 Broiler 启动了一个虚拟机。开发者也可能遇到 Broiler 启动不了的问题，原因是找不到 /dev/kvm, 那么这个需要打开内核的宏: CONFIG_KVM 和 CONFIG_KVM_INTEL 宏。开发者可以修改内核源码和 KVM 源码，修改完毕之后，在 BiscuitOS-Broiler-default 目录下执行如下代码更新内核和 KVM:

{% highlight bash %}
# 编译内核和 KVM
make kernel

# 将内核和 KVM 更新到 ROOTFS
make install
make pack

# 运行 BiscuitOS
make run

# 进入 BiscuitOS 之后启动 Broiler
RunBroiler
{% endhighlight %}

通过上面的流程，开发者可以根据需求修改内核源码和 KVM 源码，并通过简单命令就可以更新到 ROOTFS 进行实践，那么接下来更多的可能留给开发者自己.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="B3"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000I.jpg)

#### 构建基于 Broiler 项目的 BiscuitOS

![](/assets/PDB/HK/TH001709.png)

传统的 BiscuitOS 项目是基于 KVM 与 QEMU 进行搭建的，其优点是虚拟化功能齐全，调试工具和手段丰富，但缺点就是 QEMU 过于复杂和冗余，而且移植难度大，编译慢和运行相对慢的特点。如果开发者只是纯粹的使用 QEMU 启动一个 BiscuitOS 来调试内核，那么可以考虑使用 Broiler 替换 QEMU，这样的优点是可以大大加快部署和编译的速度，加快内核启动的速度。同理该方案同理也需要准备一台已经>打开 VT-x/VT-d 的 Linux 发行版，并且已经安装 KVM 模块。那么接下来介绍部署一个 Broiler 的 Linux 5.10 X86 开发环境, 开发者参考如下步骤:

> [BiscuitOS Linux 5.10 X86 开发环境部署](/blog/Linux-5.x-x86_64-Usermanual/)

部署完 BiscuitOS Linux 5.10 X86 开发环境之后，接下来部署 Broiler 源码，使用如下命令:

{% highlight bash %}
# 切换到 BiscuitOS 目录
cd BiscuitOS
linux-5.10-x86_64-Broiler_defconfig
# 保存配置并使配置生效
make

# 进入 linux 5.10 目录
cd output/linux-5.10-x86_64/
{% endhighlight %}

![](/assets/PDB/HK/TH001710.png)

部署成功之后，可以看到 Broiler-system 目录，该目录即 BiscuitOS 使用的虚拟化底座，进入 Broiler-system 查看其源码:

![](/assets/PDB/HK/TH001704.png)

Broiler 源码如上图，其中 bios 是 BIOS 相关的源码; broiler 目录下 Broiler 虚拟化技术的核心，用于虚拟 CPU 各种组件; hw 目录下提供了 Broiler 支持的设备模拟 Demo; include 目录下包括各类型的头文件; lib 目录下提供了 Broiler 使用的各类库函数; virtio 目录下包括了 virtio 协议的核心代码，以及一些 virtio 设备后端的实现. main.c 文件为 Broiler 的入口函数，Makefile 为编译项目的配置文件，RunBroiler.sh 用于启动 Broiler 的快速脚本，COPYING 为 Broiler 使用的 GPL 说明文件，README.md 为项目的简易说明文档. 那么接下来查看 BiscuitOS 的启动参数，即 Broiler 的启动参数，其位于 linux 5.10 目录下的 RunBiscuitOS.sh:

{% highlight bash %}
# ROOTFS 镜像大小
ROOTFS_SIZE=150
# 数据盘大小
FREEZE_SIZE=512
# 内存大小
RAM_SIZE=512

...

do_running()
{
	....

        sudo ${BROILER} \
                --kernel ${LINUX_DIR}/x86/boot/bzImage \
                --rootfs ${ROOT}/BiscuitOS.img \
                --memory ${RAM_SIZE} \
                --cpu 2 \
                --cmdline "${CMDLINE}"
}
{% endhighlight %}

"--kernel" 参数指向内核镜像所在的路径，"--rootfs" 参数指向了 ROOTFS 所在的路径，"--memory" 参数指明 Broiler 启动的虚拟机内存大小，单位为 MiB，"--cpu" 参数指明 Broiler 启动虚拟机的 CPU 数量, "--cmdline" 参数指明 Broiler 启动的虚拟机内核使用的 cmdline. 接下来直接使用如下命令进行实践:

{% highlight bash %}
cd BiscuitOS/cd output/linux-5.10-x86_64/
./RunBiscuitOS.sh
{% endhighlight %}

![](/assets/PDB/HK/TH001700.png)

上图便是成功使用 Broiler 启动 BiscuitOS，开发者可以修改各种源码，然后在 BiscuitOS 进行实践，那么接下来更多的可能留给开发者自己.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="C"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000M.jpg)

#### Broielr 内存虚拟化

![](/assets/PDB/HK/TH001481.png)

在 Intel X86 架构中，系统物理地址总线构成的空间称为**存储域**，其主要由三部分组成，一部分是 DDR 控制器管理的 DDR 域，另外一部分是 PCI 总线域映射到存储域的 PCI 总线地址，最后一部分是设备内部存储空间映射到存储域的区域，这里统一将后者统称为 MMIO。Intel X86 架构也通过 Host PCI 主桥维护一颗 PCI 总线，该 PCI 总线构成的空间称为**PCI 总线域**。

![](/assets/PDB/HK/TH001695.png)

在 X86 架构中，存储域的空间长度和 PCI 总线域的长度是相同的，并且 DDR 域的地址可以映射到存储域，也可以映射到 PCI 总线域，同理 PCI 域的地址也可以映射到存储域，切该地址在存储域上称为 PCI 总线地址。

![](/assets/PDB/HK/TH001711.png)

在 Broiler 模拟的 X86 架构中，DDR 映射范围被划分做两段，第一段是 \[0x00000000, 0xC0000000), 大小为 2Gig 内存, 第二段从 0x100000000 开始. PCI 域映射到存储域的范围是 \[0xC0000000, 0x100000000), 该区域包括了 PCI 总线地址，同时也包括了其他外设存储空间映射的区域，例如 Local APIC 映射的 MMIO 区域.

![](/assets/PDB/HK/TH001712.png)

DDR 内存控制器可以看到完整的 DDR 域空间，DDR 域空间一般分作 3 类，第一类是直接映射到系统存储域的区域，第二类是重映射到系统存储域的区域，最后一类是系统进入 SMM 模式才可以访问的区域。在 Broiler 模拟的 X86 架构中，低于 2Gig 的 DDR 物理内存直接映射到系统存储域的低 2Gig 区域; 超过 4Gig 的 DDR 物理内存直接映射到系统存储域 4Gig 开始处; DDR 2Gig 到 3Gig 部分则通过重映射机制映射到了系统存储域 Reclaim Base 开始的地方，Reclaim Base 的地址正好是 DDR 4G 之后的长度。映射完毕之后，系统存储域 2Gig 到 4Gig 的部分预留给了 PCI 总线域映射的 MMIO，如果 PCI 总线域映射的范围超过了 2Gig，那么继续映射到 TOUUD BASE 之后，该地址是 DDR 重映射区域结束的地址.

![](/assets/PDB/HK/TH001713.png)

上图是一个 Broiler 运行的 4Gig 虚拟机，通过 "/proc/iomem" 接口可以看到系统存储域的布局，可以看到 System RAM 占用的范围是 \[0x00100000, 0xbfffffff], 以及 \[0x100000000, 0x13fffffff]. 另外可以看到 PCI 的 MMIO 从 0xC2000000 开始. 这个布局符合 Broiler 的规划.

![](/assets/PDB/HK/TH001714.png)

上图是一个 Broiler 运行的 2Gig 虚拟机，通过 "/proc/iomem" 接口可以看到系统存储域的布局，可以看到 System RAM 占用的范围是 \[0x00100000, 0x7fffffff]. 另外可以看到 PCI 的 MMIO 从 0xC2000000 开始，这个布局符合 Broiler 的规划.

![](/assets/PDB/HK/TH001715.png)

X86 架构中存在存储域和 IO 地址空间，IO 地址空间用于映射外设的寄存器，并通过 IN/OUT 指令进行访问，Broiler 同样模拟了一系列的 IO 端口，这些端口中比较常用的是 PCI conf1 的 ADDR 和 DATA 端口，系统通过两个端口访问 PCI 设备的配置空间。

![](/assets/PDB/HK/TH001717.png)

Broiler 使用 struct broiler_memory_region 数据结构描述一段 DDR 内存区域，以及这段 DDR 内存映射到 Guest OS 存储域的信息。其 guest_phys_addr 成员记录了这段 DDR 区域映射到 Guest OS 存储域的物理地址，这里可以称为 GPA; host_addr 成员记录了这段 DDR 区域在 Host 端的虚拟地址，也就是 HVA; size 成员指明了 DDR 区域映射到 Guest OS 存储域的长度.

![](/assets/PDB/HK/TH001716.png)

系统内可以同时存在多块 DDR 区域，那么就存在多个 struct broiler_memory_region 变量，Broiler 使用一颗区间树(由红黑树改造而来)，将所有的 DDR 区域按其在 Guest OS 存储域的地址 GPA 从小到大的插入到区间树里，那么 Broiler 可以使用一个 GPA 地址找到对应的 struct broiler_memory_region 数据结构，进而找到 HVA 地址。Broiler 提供了一系列接口辅助地址的转换:

{% highlight bash %}
# GPA(实施模式的物理地址) 转换为 HVA
static inline void *gpa_real_to_hva(struct broiler *broiler, u16 selector, u16 offset)
# GPA(保护模式的物理地址) 转换为 HVA
static inline void *gpa_flat_to_hva(struct broiler *broiler, u64 offset)
# GPA 转换成 HVA
void *gpa_to_hva(struct broiler *broiler, u64 offset)
{% endhighlight %}

![](/assets/PDB/HK/TH001718.png)

Broiler 使用 struct mmio_mapping 数据结构描述 Guest OS 存储域上的一段 MMIO，与内存管理类似，其使用一颗区间树 mmio_tree 管理所有 MMIO 区间，并使用 MMIO 区域的范围作为插入的条件，那么按先序遍历区间树的时候，MMIO 区间起始地址小的会被先遍历到。另外数据结构还包括了 mmio_fn 成员，该程序指向的函数用于处理虚拟机因为访问 MMIO 导致 VM_EXIT 时对 MMIO 区域进行读写模拟。Broiler 也使用 struct mmio_mapping 数据结构描述 Guest OS IO 空间的一段 IO 端口，其通过 pio_tree 区间树进行维护，其通过 IO 端口的区间范围插入到区间树，同理当 Guest OS 范围 IO 端口时，虚拟机也会 VM_EXIT 退出，退出之后 Broiler 就在 pio_tree 区间树中查找对应的 struct mmio_mapping, 然后找到其 mmio_fn 指向的函数，然后进行 IO 端口的读写模拟. Broiler 提供了一系列的接口辅助地址的转换:

{% highlight bash %}
# IO 端口的读操作 (Guest to Host)
static inline u8 ioport_read8(u8 *data)
static inline u16 ioport_read16(u16 *data)
static inline u32 ioport_read32(u32 *data)

# IO 端口的写操作 (Host to Guest)
static inline void ioport_write8(u8 *data, u8 value)
static inline void ioport_write16(u16 *data, u16 value)
static inline void ioport_write32(u32 *data, u32 value)
{% endhighlight %}

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="D"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000B.jpg)

#### Broiler BIOS

Broiler 提供了一个简易的 BIOS，何为简易? 这得从 BIOS 的作用说起，BIOS 的作用是初始化硬件，将硬件初始化到一个已知状态，然后提供中断向量表和 E820 表，然后部分 BDA 和 EBDA 数据，那么初始化阶段 BIOS 的使命就可以结束，那么可以不真正在虚拟机内运行 BIOS，而是直接将特定的数据直接写到内存的指定位置上即可，然后提供一些内核需要调用的 BIOS IVT 中断，最后提供一份 E820 表即可.

![](/assets/PDB/HK/TH001719.png)

在系统的物理地址总线空间，也就是存储域的前 1MiB 空间，原本是映射了 DDR 的物理内存，但这段物理内存预留给 Legacy BIOS 使用，那么操作系统不再直接使用这段物理内存。另外 BIOS 原先存储在 ROM 里，然后系统访问 ROM 的速度远比 RAM 的慢，那么为了加快速度就把 BIOS 的内容由 ROM 拷贝到 RAM，并放置在低 1MiB 物理内存。BIOS 正常运行时会将低 1MiB 区域分作不同的用途，其中 \[0x00000000, 0x00000400) 区域用于存放 BIOS 中断向量表; \[0x00000400, 0x00000500) 区域用于存储 BIOS 的 BDA 数据区, \[0x00005000, 0x0009FC00) 区域作为可动态支配的区域; \[0x0009FC00, 0x000A0000) 区域则存放 BIOS EBDA 数据. \[0x000A0000, 0x000C8000) 区域被 Legacy VIDEO RAM/ROM 占用，作为特殊用途使用. 值得注意的是 E820 位于 EBDA 开始处，并往下延伸.

![](/assets/PDB/HK/TH001720.png)

内核在启动过程中，主要使用了 BIOS 中断向量表提供的 Int15 中断和 Int10 中断，其中 Int15 中断用于获得 E820 表，其代码实现如上图. Int10 中断则是字符输出相关的中断，这里不做介绍，具体细节可以参看源码 bios/ 目录下内容.

-------------------------------------------

<span id="K"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000D.jpg)

#### Broiler 设备虚拟化

设备虚拟化是系统虚拟化软件使用软件的方式呈现给 Guest OS 硬件设备的逻辑，其先后经历了完全虚拟化、半虚拟化，以及硬件辅助虚拟化，包括将硬件直接透彻 (Passthrough) 给虚拟机，以及将一个硬件从硬件层面虚拟成多个硬件子系统，每个子系统分别透传给虚拟机等。Broiler 目前支持设备完全虚拟化和半虚拟化方案，其中半虚拟化方案就是标准 Virtio 协议。

![](/assets/PDB/HK/TH001722.png)

Broiler 最先支持的是设备全虚拟化方案，该方案按照硬件设备规范，完完整整地模拟硬件设备的逻辑. 当虚拟机运行的时候需要访问 I/O，那么会导致虚拟机停止运行并退出 (VM_EXIT), 内核的 KVM 模块最先捕获到虚拟机的退出，然后获知虚拟机退出的原因是访问 I/O, 那么 KVM 模块无法模拟这个硬件过程，那么将模拟的请求发送给用户空间的 Broiler，Broiler 在获知虚拟机退出的原因是访问 IO，那么 Broiler 进一步确认是访问哪个 IO，以及 Broiler 是否对该 IO 进行模拟，如果有那么 Broiler 运行相关的硬件模拟逻辑，如果需要返回数据，Broiler 将模拟产生的数据再次发送给内核 KVM，内核 KVM 组件收到硬件模拟完成的信息之后，通过 VM-Entry 一并数据发送给虚拟机，虚拟机恢复运行并获得所需的 IO 处理结果，那么虚拟机继续运行. 反之如果 Broiler 没有模拟相应的硬件，那么会直接通知内核 KVM 组件，KVM 组件将异常的结果一并返回给虚拟机，虚拟机再次运行时就会获得 IO 异常的信息.

![](/assets/PDB/HK/TH001723.png)

在全模拟方案中，Guest OS 只要访问 I/O 就会导致虚拟机退出，然后内核 KVM 处理不了就转到用户空间让 Broiler 进行处理，这大大降低了 I/O 性能。那为什么不直接在内核中完成设备的模拟动作呢? 在有的场景下，设备虚拟化更适合在内核空间进行，比如典型的中断虚拟化芯片的模拟，但有些设备模拟很复杂，如果放在内核中实现，除了增加内核的复杂度，也容易带来安全问题，于是提出了一种折中的 **Vhost 方案**，将设备模拟的数据处理面搬到内核，设备控制面还保留在用户空间。该方案从 Guest 到 KVM 的交互只有一次用户态的切换以及数据拷贝，并且 Guest 到 Host NIC 之间的通信是比较好的。

![](/assets/PDB/HK/TH001724.png)

对于软件方式的设备模拟来讲，完全没必要完全模拟硬件行为，而是制定一个更高效、简洁地适用于 Guest 驱动和设备交互的方式，于是提出了半虚拟化的概念。在虚拟机中，可以通过 Hypervisor 模拟 e1000 网卡，这个经典的网卡在各种客户 OS 都会提供 inbox 驱动，所以兼容性上来看是个不错的选择，但 e1000 包含了复杂的 IO 端口、寄存器和缓存配置，虚拟机每次收发包都会引起更多的 IO 和 MMIO 操作，使得虚拟机陷出，最终导致网络性能不佳。为了解决这个问题, IBM 在 2005 年提出了 virtio 协议，虚拟机中的半虚拟化前段驱动和主机上的后端设备简单使用 virtqueue 共享队列交换数据，大大减少了 e1000 模拟时复杂的 IO 操作，从而较大程度提升了虚拟网络性能.

![](/assets/PDB/HK/TH001725.png)

virtio 是一种 I/O 半虚拟化解决方案，是一套通用的 I/O 设备虚拟化的框架，也是对半虚拟化 Hypervisor 中的一组通用 I/O 设备的抽象。提供了一套上层应用程序与各 Hypervisor 虚拟化设备之间的通信框架，减少跨平台所带来的兼容性问题，大大提高了驱动程序的开发效率. Broiler 目前支持 virtio 协议的模拟设备包括: virtio-blk。

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-----------------------------------------------

###### <span id="K1">PIO 端口模拟</span>

![](/assets/PDB/HK/TH001745.png)

Linux 将地址空间分为了存储域空间和 IO 空间，CPU 可以直接通过指针引用来访问存储域空间，而 IO 空间则需要使用 IN/OUT 指令进行访问。IO 空间包含了很多的 IO 端口(Port), IO 端口是接口电路中 CPU 直接访问的寄存器地址。CPU 通过这些端口向接口电路中的寄存器发送命令，读取和传送数据. 上图是 Broiler 支持的 IO 端口, 比如常见的 PCI 配置空间的数据和地址寄存器、定时器寄存器、键盘控制器寄存器等。

![](/assets/PDB/HK/TH001746.png)

该源码为 Broiler 项目 hw/ 目录下的 BiscuitOS-pio-base.c 文件，该文件用于展示如何在 Broiler 模拟一个 PIO 端口。案例代码中模拟的端口起始地址为 0x6800, 长度为 16 个字节，这段区域中包含了 4 个寄存器，第一个是 SLOT_NUM 寄存器，用于维护 Slot 的数量; 第二个是 SLOT_SEL 寄存器，用于维护当前使用的 Slot 号; 第三个是 MIN_FREQ 寄存器，用于维护最小频率值; 最后一个是 MAX_FREQ 寄存器，用于维护最大频率值. 在 Broiler 初始化过程中需要进行一定的逻辑向系统模拟 PIO 硬件，例如案例 71 行调用 broiler_register_pio() 函数向 Broiler 平台模拟一个新的 PIO，其参数二和三指明了 PORT 的范围，以及 BiscuitOS_pio_callback() 函数用于模拟具体的 IO. 当 Broiler 需要移除 IO 端口模拟，可以使用 82 行的 broiler_deregister_pio() 函数，只需给出需要移除的端口号. 最后一起来看看 PIO 模拟 IN/OUT 指令的过程. BiscuitOS_pio_callback() 函数 37-47 行模拟了 OUT 指令，Broiler 使用 ioport_read32() 函数读取 Guest OS 向端口写入的内容; 函数 50-62 行模拟 IN 指令，Broiler 使用 ioport_write32() 函数向 Guest OS 的端口吐出需要读取的数据. 由于 Broiler 模拟 PIO 硬件还不够，需要 Guest OS 内部对 PIO 资源进行注册并使用，这里通过 BiscuitOS 项目提供的驱动介绍 Guest OS 内部如何使用 PIO，其可以通过 BiscuitOS 项目进行部署，部署逻辑如下:

{% highlight bash %}
# 切换到 BiscuitOS 目录
cd BiscuitOS
make linux-5.10-x86_64_defconfig
make menuconfig

  [*] Package --->
      [*] PIO: Programming Input/Output Model --->
          [*] PIO Driver for Broiler

# 保存配置并使配置生效
make

# 进入 Broiler 目录
cd output/linux-5.10-x86_64/package/BiscuitOS-Broiler-pio-default/
# 下载源码
make download
# 编译并运行源码
make
make install
make pack
make run
{% endhighlight %}

![](/assets/PDB/HK/TH001747.png)
![](/assets/PDB/HK/TH001748.png)
![](/assets/PDB/HK/TH001749.png)
![](/assets/PDB/HK/TH001750.png)

> [BiscuitOS-Broiler-pio-default Gitee @link](https://gitee.com/BiscuitOS_team/HardStack/tree/Gitee/Memory-Allocator/PIO/BiscuitOS-Broiler-pio)

BiscuitOS-Broiler-pio 驱动运行在 Guest OS 内部，其主要任务是将 Broiler 模拟的 PIO 注册到系统 IO 资源总线上，接着将 IO 端口映射到内存，并通过 Linux 提供的接口对四个寄存器进行访问。驱动在 24-29 行定义了新端口的资源描述，并将端口的名字设置为 "Broiler PIO", 驱动接着在 36 行调用 request_resource() 函数将 Broiler_pio_res 资源添加到系统 IO 空间，接着驱动在 40 行调用 ioport_map() 函数将 BISCUITOS_PIO_PORT 端口映射到存储域，并从 47-54 行调用 ioread32() 和 iowrite32() 函数对 IO 端口进行读写操作。接下来查看其基于 Broiler 在 BiscuitOS 中运行的情况:

![](/assets/PDB/HK/TH001751.png)

Broiler 版的 BiscuitOS 启动之后，加载驱动可以看到 log 显示了一些端口的操作，与 Broiler 模拟 PIO 行为一致，那么此时通过 /proc/ioports 查看 IO 空间的布局，此时看到 0x6800-0x6810 区域是 Broiler PIO 端口, 至此一次完整的 PIO 模拟实践完成，那么接下来分析 PIO 模拟的调用栈:

![](/assets/PDB/HK/TH001752.png)

Guest OS 对 PIO 发起的 IO 读操作，其会导致虚拟机 VM-EXIT, KVM 捕获到退出原因为 EXIT_REASON_IO_INSTRUCTION, 那么 KVM 通过 handle_io() 函数对 IO 读操作进行模拟，由于 PIO 的内容不是 In-kernel 的设备，那么 KVM 无法处理 IO 模拟，那么这个时候 KVM 将 IO 需要模拟的数据放到了 kvm_run 数据结构体中，其包含了 IO 端口号、读取数据的长度以及退出的原因为 KVM_EXIT_IO. KVM 将模拟 IO 任务转让给 Broiler 进行处理，Broiler 此时发现虚拟机因为 KVM_EXIT_IO 退出了，那么其在 pio_tree 红黑树中找到 IO 端口维护的模拟函数，对于 Broiler PIO 使用 BiscuitOS_pio_callback() 函数进行 IO 模拟。当 Broiler 模拟好 IO 数据之后，将其存储在 kvm_run 的 mmio_data 变量里，最后再次调用 KVM_RUN 进入到 KVM 中; KVM 收到 KVM_RUN IOCTL 之后，再次 VM_ENTRY 运行虚拟机，虚拟机运行之后继续执行对 PIO 的 IN 指令，此时 AX 寄存器里已经包含读到的 IO 模拟数据. 

![](/assets/PDB/HK/TH001753.png)

Guest OS 对 PIO 发起的 IO 写操作，其会导致虚拟机 VM-EXIT，KVM 捕获退出原因为 EXIT_REASON_IO_INSTRUCTION，那么 KVM 通过 handle_io() 函数对 IO 写操作进行模拟，由于 Broiler PIO 的内容不是 In-Kernel 的设备，那么 KVM 无法处理 IO 模拟，那么 KVM 将 IO 需要模拟的请求放到了 kvm_run 数据结构中，其包括了 IO 端口号、需要写入的数据、以及写入数据的长度，最后就是退出的原因填写为 KVM_EXIT_IO. KVM 将模拟 IO 任务转让给 Broiler 进行处理，Broiler 此时发现虚拟机因为 KVM_EXIT_IO 退出了，那么其在 pio_tree 红黑树中找到 IO 端口维护的模拟函数，Broiler PIO 使用 BiscuitOS_pio_callback() 函数模拟 IO 写操作。当 Broiler 模拟好 IO 之后调用 KVM_RUN 进入到 KVM 中; KVM 收到 KVM_RUN IOCTL 之后，再次 VM_ENTRY 运行虚拟机，此时虚拟机继续执行 Broiler PIO 写操作并完成.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-------------------------------------------

<span id="F2"></span>

![](/assets/PDB/BiscuitOS/kernel/IND00000L.jpg)

#### Virtio-blk 磁盘设备

virtio-blk 磁盘设备是使用 virtio 机制为 Guest OS 提供的一个高性能块设备 I/O 虚拟化的方法，其由 Guest OS 的前端驱动(Front-driver)和 Broiler 提供的后端驱动(Backed-driver) 组成，中间通过 vring 进行协商，在讲解 virtio-blk 之前先了解一下 Linux 块设备架构.

![](/assets/PDB/HK/TH001726.png)

在 Linux 中驱动对块设备的输入或输出 (I/O) 操作，都会向块设备发出一个请求，在驱动中使用 request 结构体进行描述。但对于一些磁盘设备而言请求速度很慢，这时内核就提供一种队列的机制把这些 IO 请求添加到队列中，这个队列就是请求队列，在驱动中使用 request_queue 结构体。在向块设备提交这些请求前内核会先执行请求的合并和排序预操作，以提高访问的效率，然后再由内核中的 I/O 调度子系统负责提交 I/O 请求，调度程序将资源分配给系统中的所有挂起的 I/O 请求，其工作是管理块设备的请求队列，决定队列中的请求的排列顺序以及什么时候发起请求到设备. 接着由通用块层 (Generia Block layer) 负责维持一个 I/O 请求在上层文件系统域底层物理磁盘之间的关系。在通用块层中，通常用一个 bio 结构体对应一个 I/O 请求. 块设备硬件对数据处理的基本单位称为扇区(Sectors), 一个扇区的大小为 512Bytes, Linux 制定对内核或文件系统数据处理的基本单位称之为块(Blocks),  通常一个块由一个或多个扇区构成.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-----------------------------------------------

###### <span id="F8">virtio-blk PCI 设备</span>

![](/assets/PDB/HK/TH001733.png)

virtio-blk 磁盘前端设备在 Guest OS 内存呈现为一个 PCI 设备, BDF 号为 00:00.0, 其包含三个 BAR，其中 BAR0 是一个 IO-BAR，其在 IO 空间的范围是 0x6200 到 0x62ff 的多个端口，BAR0 里包含了 virtio-blk 设备状态、VQ、Kick 、MSI 中断配置和磁盘相关的配置寄存器; BAR1 为一个 MM-BAR, 其映射在系统地址空间的范围是 \[0xc2000000, 0xc2000100), BAR 里的内容与 BAR0 一致; BAR2 为一个 MM-BAR，其映射在系统地址空间的范围是 \[0xc2000400, 0xc2000800), BAR 里的内容为 MSI 中断表相关的内容.

![](/assets/PDB/HK/TH001736.png)

Broiler 对 virtio-blk 对于的 PCI 设备配置空间进行模拟，其中包括为 IO-BAR 分配 IO 端口，为 MEM-BAR 分配 MMIO 地址，另外还包含了 PCI 设备 256 字节的配置空间内容的模拟。Broiler 还为每个 BAR 的地址访问进行模拟.

###### <span id="F3">virtio-blk BAR0</span>

![](/assets/PDB/HK/TH001731.png)

上图是 virtio-blk 磁盘对应前端设备看到的 PCI BAR0 bitmap，每当前端设备访问 BAR0 时会让虚拟机 VM-EXIT 退出到内核 KVM，KVM 处理不了让 Broiler 继续处理，Broiler 对BAR0 进行模拟，当 Broiler 模拟完毕之后将数据发送给 KVM，然后 KVM 在 VM-Entry 将数据传递给 Guest OS。那么回到 BAR0 本身，各字段的含义如下:

###### 0x00 - 0x03: 后端设备支持的磁盘特性
###### 0x04 - 0x07: 前端设备支持的磁盘特性

![](/assets/PDB/HK/TH001732.png)

前后端支持的特性如上图，前后端在交互过程中会确认各种所支持的磁盘特性，然后找出最小交集，最小交集就是该 virtio-blk 设备的磁盘特性。

###### 0x08 - 0x0B: Virtqueue 在 Guest 内的 GPA 地址

virtio-blk 磁盘前端设备负责在 Guest OS 内存申请一段内存，这段内存用于存储 Virtqueue 的 Desc-Ring、Avail-Ring 和 Used-Ring，然后将这段的物理内存 GPA 写入到该字段，后端设备通过该字段获得 Virtqueue 的 GPA 地址，进而获得 Virtqueue 对于的 HVA 地址，这样前后端就可以通过 Virtqueue 共享内存并交互数据.

###### 0x0C - 0x0D: Virtqueue Size

该字段用于描述 virtio-blk 使用 Virtqueue 的数量，该值由后端设备提供为只读。在 virtio-blk 磁盘中只使用一个 Virtqueue，因此该字段为 1.

###### 0x0E - 0x0F: Queue Select

该字段用于描述 virtio-blk 前端设备正在使用的 Virtqueue，在支持多 Virtqueue 设备的场景下，该字段用于描述前端设备正在使用哪个 Virtqueue。由于 virtio-blk 磁盘只使用一个 Virtqueu，那么该字段后端直接模拟为 0 即可，即第一个 Virtqueue.

###### 0x10 - 0x11: Kick

当 virtio-blk 磁盘前端设备准备好请求发我后端，再填充完 Avail-Ring 和 Desc-Ring 之后会向 Kick 字段进行写操作，该字段在 Broiler 里是一个特殊的区域，Broiler 会为该区域向 KVM 注册 ioeventfd，当因为 KVM_EXIT_IO 发生 VM-EXIT 之后，KVM 发现 IO 区域正好是 virtio-blk BAR0 的 kick，那么 KVM 会通过 ioveventfd 直接通知 Broiler，Broiler 在收到来自 KVM 的 ioeventfd 之后，Broiler 会触发处理磁盘请求的线程去处理请求，这个过程就实现了前端通知后端设备的逻辑链路.

###### 0x12: Device Status

该字段用于描述 virtio-blk 磁盘的状态，前后端设备都可以更新该字段。virtio-blk 磁盘可以根据 virtio 协议对该字段进行更新，目前支持的状态包括如下:

* ACKNOWLEDGE(Bit0): Guest OS 已经发现设备
* DRIVER(Bit1): Guest OS 加载驱动 Probe 驱动
* DRIVER_OK(Bit3): Guest OS 已经初始化好设备
* FAILED(BIT7): Guest OS 识别设备失败

###### 0x13: ISR Status

###### 0x14 - 0x15: Configuration Vector

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

---------------------------------------------------

###### <span id="F9">virtio-blk PIO/MMIO 模拟</span>

![](/assets/PDB/HK/TH001734.png)

virtio-blk 前端设备发起的 IO 读操作，其会导致虚拟机 VM-EXIT, KVM 捕获到退出原因为 EXIT_REASON_IO_INSTRUCTION, 那么 KVM 通过 handle_io() 函数对 IO 读操作进行模拟，由于 IO-BAR 的内容不是 In-kernel 的设备，那么 KVM 无法处理 IO 模拟，那么这个时候 KVM 将 IO 需要模拟的数据放到了 kvm_run 数据结构体中，其包含了 IO 端口号、读取数据的长度以及退出的原因为 KVM_EXIT_IO. KVM 将模拟 IO 任务转让给 Broiler 进行处理，Broiler 此时发现虚拟机因为 KVM_EXIT_IO 退出了，那么其在 pio_tree 红黑树中找到 IO 端口维护的模拟函数，对于 virtio-blk BAR0 的模拟由 virtio_pci_io_mmio_callback() 函数进行模拟，最后在 virtio_pci_data_in() 函数进行 IO 模拟。当 Broiler 模拟好 IO 数据之后，将其存储在 kvm_run 的 mmio_data 变量里，最后再次调用 KVM_RUN 进入到 KVM 中; KVM 收到 KVM_RUN IOCTL 之后，再次 VM_ENTRY 运行虚拟机，此时虚拟机继续将中断的 IO IN 指令继续执行，最后虚拟机从 IO-BAR 寄存器中读到值. 以上便是 KVM 与 Broiler 配合实现 virtio-blk IO-BAR IO 读模拟.

![](/assets/PDB/HK/TH001735.png)

virtio-blk 前端设备发起的 IO 写操作，其会导致虚拟机 VM-EXIT，KVM 捕获退出原因为 EXIT_REASON_IO_INSTRUCTION，那么 KVM 通过 handle_io() 函数对 IO 写操作进行模拟，由于 IO-BAR 的内容不是 In-Kernel 的设备，那么 KVM 无法处理 IO 模拟，那么 KVM 将 IO 需要模拟的请求放到了 kvm_run 数据结构中，其包括了 IO 端口号、需要写入的数据、以及写入数据的长度，最后就是退出的原因填写为 KVM_EXIT_IO. KVM 将模拟 IO 任务转让给 Broiler 进行处理，Broiler 此时发现虚拟机因为 KVM_EXIT_IO 退出了，那么其在 pio_tree 红黑树中找到 IO 端口维护的模拟函数，对于 virtio-blk BAR0 的模拟由 virtio_pci_io_mmio_callback() 函数进行模拟，最后在 virtio_pci_data_out() 函数进行 IO 模拟。当 Broiler 模拟好 IO 之后调用 KVM_RUN 进入到 KVM 中; KVM 收到 KVM_RUN IOCTL 之后，再次 VM_ENTRY 运行虚拟机，此时虚拟机完成了对 IO-BAR 的写操作. 以上便是 KVM 与 Broiler 配合实现 virtio-blk IO-BAR IO 写模拟.

![](/assets/PDB/HK/TH001737.png)

virtio-blk 前端设备有新的 IO 请求时，首先准备所需的数据，然后更新 Avail-Ring 的 avai_idx 的值，然后通过 BAR0 的 kick 寄存器写操作出发主动通知后端设备，其流程与通常的写流程不同。Broiler 在 virtio-blk 初始化过程中，通过 KVM_IOEVENTFD IOCTL 向 KVM 维护的 ioevetfd 链表里注册一个监听函数，当前端设备对 kick 寄存器写操作会导致虚拟机会 VM-EXIT，此时 KVM 捕获到的退出原因是 EXIT_REASON_IO_INSTRUCTION, 那么 KVM 调用 handle_io() 函数进行 IO 模拟，与普通的写 IO 模拟不同的是 kernel_pio() 函数会被调用，该函数在 KVM 的 PIO Bus 上查找 Kick 寄存器的监听函数，当找到监听函数，那么 KVM 通过 IOEVENT 机制通知 Broiler 的 virtio_pci_ioevent_callback() 函数，该函数调用 notify_vq() 函数会唤醒 virtio-blk 的线程处理磁盘的 IO 请求，在处理的过程中后端设备就会从 virtio-blk 的 Virtqueue 中读取 IO 请求的描述信息，并更新 last_avail_idx，然后通过 Desc-Ring 的描述处理 IO 请求。当 IO 请求模拟完之后 Broiler 调用 virtio_blk_complete() 函数更新 virtio-queue Used-Ring 的 used_idx 值，并向虚拟机注入中断，虚拟机在次运行并收到中断之后，virtio-blk 前端设备消费该中断，然后读取 Used-ring 并更新 last_used_idx 和 used_idx 的值，最终确认 virtio-blk 的请求完成.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)

-----------------------------------------------------

###### <span id="F4">virtio-blk Virtqueue</span>

![](/assets/PDB/HK/TH001728.png)

Virtqueue 是 virtio-blk 的前端设备和后端设备交互的核心组件，其由三个环形队列构成，分别为 Desc-Ring、Avail-Ring 和 Used-Ring, Desc-Ring 用于描述前端 IO 请求相关，其中包含了 IO 请求数据在 Guest OS 内部的地址、长度和 IO flags; Avail-Ring 用于描述前端设备新的 IO 请求，每当前端设备准备好 IO 请求之后更新完 Desc-Ring，然后将对于的 Desc-idx 填入到 Avail-Ring\[avail_idx] 内，接着更新 avail_idx, 一切处理完毕之后向 Virtqueue PCI BAR0 的 Kick 寄存器执行写操作，以此告诉后端 IO 请求准备完毕。Kick 寄存器的写会导致 VM-EXIT，执行权回到了 KVM，KVM 利用 ioeventfd 机制将请求直接发送给 Broiler，Broiler 获得执行权之后唤醒后端设备进行 IO 模拟。后端设备发现 Avail-Ring 中由新的 IO 请求，于是从 Avail-Ring 中获得最新请求在 Desc-Ring 中的索引，然后解析 Desc-Ring 中指定的 Entry，并获得 IO 请求相关的 GPA 地址、长度以及请求标志，后端设备利用这些数据将 GPA 地址转换成 HPA 地址，然后进行 IO 模拟，IO 模拟处理完毕之后，后端设备将 Desc-Ring 处理完的 IO 请求对应的 Desc-idx 写入到 Used-Ring\[used_idx] 项里并更新 used_idx 值，后端设备最后在虚拟机重新 VM_ENTRY 之前注入中断。虚拟机重新运行之后，virtio-blk 收到了中断，然后发现 Used-Ring 中有更新，于是读取 Used-Ring[used_idx] 的内容，并在 Desc-Ring 队列中找到 IO 请求的描述，那么对应的 IO 请求处理完毕，以上便是基于 virtqueue 的前后端设备交互流程.

![](/assets/PDB/HK/TH001738.png)

Virtqueue 的内存布局如上，其由前端设备在 Guest OS 内分配一块内存，然后根据 virtio 协议前后端达成一致的内存布局，前端设备在初始化 virtio-blk 阶段通过 BAR0 相关寄存器与后端设备进行协商 virtqueue 的信息，其中包括 virtqueue 的 GPA 地址、长度以及设备目前使用那个 virtqueue. 在 Virtqueue 的内存布局中，Desc-Ring 占用了最前端部分，其包含了 VIRTIO_BLK_QUEUE_SIZE 个成员，每个成员用于记录一次前端设备 IO 请求描述，包括 IO 请求数据的地址、长度以及标志; Virtqueue 的第二大部分是紧随其后的 Avail-Ring，其也包含了 VIRTIO_BLK_QUEUE_SIZE 个成员，每个成员描述一个新的 IO 请求在 Desc-Ring 中的位置. 在 Desc-Ring 与 Avail-Ring 之间存在两个变量，分别是 avail_flags 和 avail_idx; Virtqueue 的第三部分是 Used-Ring，Used-Ring 并没有紧随 Avail-Ring 之后，而是在下一个 PAGE_ALIGN 的地方开始，那么 Avail-Ring 和 Used-Ring 之间存在一段空洞 pad\[]. Used-Ring 同样包含了 VIRTIO_BLK_QUEUE_SIZE 个成员，并且 Used-Ring 前面紧贴着两个变量，分别是 used_idx 和 used_flags, Used-Ring 之后也紧贴着 avail_event_idx.

![](/assets/PDB/HK/TH001739.png)

Virtio-blk 前端设备的 BAR0 的多个域包含了 virtqueue 相对配置，其中 \[0x4, 0x7] 域描述了 virtqueue 在 Guest OS 内的地址，即 GPA 地址; \[0x0C, 0x0D] 域描述了 virtqueue 中每个 Ring 中成员的数量, Broiler virtqueue 支持 VIRTIO_BLK_QUEUE_SIZE 个成员; \[0x0E, 0x0F] 域用于描述当前使用第几个 virtqueue，在支持多 virtqueue 的设备，该域可以设置设备当前使用哪个 virtqueue，对于单 virtqueue 的设备，那么该域为 0; \[0x014, 0x15] 域用于描述 virtqueu 使用的中断信息.

![](/assets/PDB/HK/TH001741.png)

Virtqueue 的 Avail-Ring 是一个无锁的 FIFO 环形队列，avail_idx 变量对前后端可见，但只有前端可以对其进行读写操作，后端只能对其进行读操作，avail_idx 用于描述最新的 IO 请求在 Avail-Ring 中的索引。后端设备维护 last_avail_idx 变量，用于描述上一次 IO 请求的索引。当前端没有新的 IO 请求，avail_idx 和 last_avail_idx 是相等的; 当前端设备有新的 IO 请求，那么前端设备会更新 avail_idx 并通过 kick 通知后端去处理新的 IO 请求; 后端收到通知之后，检查 last_avail_idx 和 avail_idx 是否相等，如果发现 avail_idx 更大，那么表示 Avail-Ring 中有新的 IO 请求需要处理，那么后端设备开始处理 IO 请求并更新 last_avail_idx，使其与 avail_idx 相同. 由于前后端设备各自维护一个变量，因此不需要锁进行处理，所以可以做到无锁交互.

![](/assets/PDB/HK/TH001742.png)

Virtqueue 的 Used-Ring 也是一个无锁 FIFO 环形队列, used_idx 变量对前后端设备可见，但只有后端设备可以对其读写操作，前端设备只能对其读操作，used_idx 用于描述已经完成的 IO 请求在 Used-Ring 中的索引。前端设备维护 last_used_idx 变量，用于描述上一次完成的 IO 请求的索引。当后端设备没有新完成 IO 请求，那么 used_idx 与 last_used_idx 是相等的; 当后端设备处理完一个 IO 请求之后，那么后端设备会更新 used_idx 并在虚拟机再次运行的时候注入中断，以此告诉前端设备有新的 IO 请求处理完成; 当前端设备收到指定中断之后，前端设备检查 last_used_idx 与 used_idx 是否相同，如果 used_idx 更大，那么说明有新的 IO 请求完成，前端设备就更新 last_used_idx 的值，使其与 used_idx 相同，最后 IO 请求处理完毕。由于前后端设备维护各自的一个变量，因此不需要进行锁操作，所以可以做到 Used-Ring 的无锁.

![](/assets/PDB/HK/TH001729.png)

当 Guest OS 内部前端设备 FE 需要与后端设备写数据时，那么前端设备会将数据准备好，然后调用 virtqueue_push() 函数将数据所在的 GPA 地址和长度信息填充到 Desc-Ring, 这个时候获得填充内容在 Desc-Ring 中的索引 desc-idx，紧接着前端设备先更新 Avail-Ring 的 avai_idx, 然后将 desc-idx 更新到 Avail-Ring\[avail_idx] 对应的 Entry 里，最后通过向 virtio-blk 对应的 BAR0 的 kick 寄存器写操作，那么这个时候 Guest 会 VM_EXIT。KVM 收到 VM_EXIT 的 reason 为 EXIT_MMIO, 那么通过 eventfd 机制将这个消息通知给 Broiler，Broiler 收到这个信息之后将消息通过 EPOLL 广播出去，这个时候后端设备模拟 BAR0 的 kick 寄存器的处理函数收到这个信息之后，通过 eventfd 告诉后端设备前端有数据到达。后端设备首先从 Avail-Ring 中获得 avail-idx 的值，然后与后端设备维护的 last_avail_idx 进行比较，如果发现 last_avail_idx 小于 avail-idx, 那么表示 Avail-Ring 里有前端设备新下发的数据需要读取，那么后端设备通过 Get_buf() 函数获得 Avail-Ring 中 Desc-idx 的值，然后从 Desc_Ring\[Desc-idx] 中获得数据所在的 GPA 和长度信息，接下来后端设备将 GPA 转换成 HVA，并搬运数据, 搬运完毕之后后端设备更新 last_avail_ring.

![](/assets/PDB/HK/TH001730.png)

当后端设备更新完 Avail-Ring 之后与前端的交互并还没有完成，这个时候需要告诉前端设备后端已经更新完毕，那么后端设备更新 Used-Ring 的 used_idx，并将前端设备写请求在 Desc-Ring 中的 Desc-idx 写入到 Used-Ring\[used_idx] 里，这个时候后端设备向 virtio-blk 对应的 PCI 设备注入中断，虚拟机运行之后收到 virtio-blk PCI 设备的中断，那么前端设备会将自己维护的 last_used_idx 和 Used-Ring 的 used_idx 进行比较，如果 last_used_idx 小于 used_idx, 那么表示 Used-Ring 里有未处理的数据，那么前端设备更新 last_used_idx, 然后读出 Used-Ring\[used_idx] 的值，该值是 Desc-Ring 里面的一个 Desc-idx 索引，然后读出 Desc-Ring\[Desc-idx] 的值，这个时候前端设备发现是之前 Request，那么前端设备认为前一次 Request 完成. 至此一次完整的前后端数据交互完成。另外值得注意的是在 virtio-blk 磁盘设备里，只有前端设备可以写 Avail-Ring，而后端设备只能写 Used-Ring，那么请求只能前端设备发出，后端设备只能处理请求.

![](/assets/PDB/HK/TH001740.png)

virtio-blk 磁盘前后端 IO 请求调用栈如上图，当前端设备有新的 IO 请求，那么前端设备调用 virtio_add_req() 函数向 virtqueue 的 Desc-Ring 和 Avail-Ring 中更新 IO 请求，并更新 avail_idx，接着调用 virtqueue_notify() 函数向 virtio-blk BAR0 的 kick 寄存器执行写操作，以此告诉后端设备有新的 IO 请求, 该操作会导致 VM-EXIT。KVM 获得控制权之后发现 VM 退出的原因是 EXIT_REASON_IO_INSTRUCTION, 并且是因为 OUT 指令退出，那么 KVM 经过函数调用进入 kernel_pio() 函数，该函数会在 KVM 维护的 PIO_BUS 上查找 BAR0 kick 寄存器注册的 IOEVENTFD 监听函数，当查找到 KVM 会调用 kvm_iodevice_write() 函数通过 IOEVENTFD 机制直接通知用户空间的 Broiler，Broiler 收到通知之后获得执行权，并直接调用 virtio_pci_ioevent_callback() 函数，该函数会直接唤醒磁盘线程的处理函数 virtio_blk_do_io(), 后端设备此时会检查 Avail-Ring 是否有新的 IO 请求，此时会拿后端设备维护的 last_avail_idx 和 avail_idx 进行比较，如果 avail_idx 大于 last_avail_idx，那么表示有新的 IO 请求，那么后端设备开始处理 IO 请求，并更新 last_avail_idx 和 avail_idx。当 IO 请求处理完毕之后，后端设备会向 Used-Ring 中写入一个新的项，新项指向与 Avail-Ring 指向同一个 Desc-Ring Entry，接着更新 used_idx，最后后端设备会通知 KVM 在虚拟机 VM_ENTRY 时注入一个 MSI 中断告诉前端设备 IO 请求已经处理完毕，KVM 收到通知注入中断并再次运行虚拟机，此时前端收到 MSI 中断，然后调用 virtblk_done() 函数，并在其子函数中检查 last_used_idx 是否与 used_idx 相等，如果 used_idx 更大那么说明有 IO 请求已经处理完毕，那么这个时候从 Used-Ring 处理完成的 IO 请求，那么一个 IO 请求处理完毕.

![](/assets/PDB/BiscuitOS/kernel/IND000100.png)
